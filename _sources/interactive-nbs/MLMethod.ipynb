{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Example - ML Method: Mean of a Gaussian\n",
    "(inspired by [this](https://matthewfeickert.github.io/Statistics-Notes/notebooks/Introductory/Likelihood-Function.html) tutorial)\n",
    "\n",
    "In this section we have seen how the likelihood function is defined:\n",
    "\n",
    "$$L(\\theta)=f(x_1|\\theta)\\cdot f(x_2|\\theta)\\cdots f(x_n|\\theta)=\\prod_{i = 1}^{Nevts} f(x_i|\\theta).$$\n",
    "\n",
    "We then noticed how, for convenience, in HEP we use the negative log-likelihood:\n",
    "\n",
    "$$NLL(\\theta) = - \\sum_{i = 1}^{Nevts} \\text{ln} f(x_i|\\theta)$$\n",
    "\n",
    "for which we find the minimum.\n",
    "\n",
    "We then saw how the uncertainty of a ML estimator at $\\pm 1 \\sigma$ can be found by checking where the the NLL increases from the maximum by 0.5.\n",
    "\n",
    "Here we will apply these easy concepts to a simple case: the estimation of the mean of a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood and NLL can be written like follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Likelihood:\n",
    "    function: callable\n",
    "    data: np.ndarray\n",
    "\n",
    "    def __call__(self, *params):\n",
    "        return np.prod(self.function(self.data, *params))\n",
    "\n",
    "\n",
    "class NLL(Likelihood):\n",
    "    def __call__(self, *params):\n",
    "        return -np.sum([np.log(self.function(self.data, *params))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where in the signature of ```__call__``` every reference to data has been purposely avoided to emphasize the fact that the likelihood is a function of parameters only.\n",
    "\n",
    "Our gaussian function can be written as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    return (\n",
    "        1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = np.random.uniform(-0.1, 0.1)\n",
    "true_sigma = np.random.uniform(0.1, 1.0)\n",
    "\n",
    "print(f\"true mean: {true_mu:.3f}\")\n",
    "print(f\"true standard deviation: {true_sigma:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncs = []\n",
    "def draw(n_samples):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "    axes[0, 0].clear()\n",
    "    axes[1, 0].clear()\n",
    "\n",
    "    lkl_label = r\"$L\\left(\\mu | \\vec{x}\\right)$\"\n",
    "    nll_label = r\"$NLL\\left(\\mu | \\vec{x}\\right)$\"\n",
    "\n",
    "    fig.suptitle(f\"{n_samples} samples\")\n",
    "\n",
    "    samples = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "    lkl = Likelihood(gaussian, samples)\n",
    "    nll = NLL(gaussian, samples)\n",
    "\n",
    "    n_mu  = 1000\n",
    "    mus = np.linspace(-0.5, 0.5, n_mu)\n",
    "    sigma = true_sigma\n",
    "\n",
    "    lkl_scan = np.array([lkl(mu, sigma) for mu in mus])\n",
    "    nll_scan = np.array([nll(mu, sigma) for mu in mus])\n",
    "    idx_min_nll_scan = np.argmin(nll_scan)\n",
    "    mu_best = mus[idx_min_nll_scan]\n",
    "    nll_scan -= nll_scan[idx_min_nll_scan] # move minimum to 0\n",
    "    arr_min_nll_ = np.ones(n_mu)*0.5\n",
    "    low_idx, high_idx = np.argwhere(np.diff(np.sign(nll_scan - arr_min_nll_))).flatten()\n",
    "    unc = np.abs(mus[high_idx] - mus[low_idx])\n",
    "    mu_low = abs(mu_best - mus[low_idx])\n",
    "    mu_high = abs(mu_best - mus[high_idx])\n",
    "    uncs.append((n_samples, unc))\n",
    "\n",
    "    axes[0, 0].plot(mus, lkl_scan, label=lkl_label)\n",
    "    axes[0, 0].set_xlim(-0.5, 0.5)\n",
    "    axes[0, 0].plot(samples, np.zeros_like(samples), \"k|\", label=r\"$\\vec{x}$\")\n",
    "    axes[0, 0].set_ylabel(lkl_label)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].plot(mus, nll_scan, label=nll_label)\n",
    "    axes[1, 0].set_xlim(-0.5, 0.5)\n",
    "    axes[1, 0].set_ylim(0, 3)\n",
    "    axes[1, 0].set_ylabel(nll_label)\n",
    "    axes[1, 0].set_xlabel(r\"$\\mu$\")\n",
    "    axes[1, 0].axvline(mu_best, color=\"k\", linestyle=\"--\", label=r\"$\\mu_{best}$\")\n",
    "    axes[1, 0].axvline(true_mu, color=\"r\", linestyle=\"--\", label=r\"$\\mu_{true}$\")\n",
    "    axes[1, 0].axhline(0.5, color=\"k\", linestyle=\"--\", linewidth=0.5)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    axes[1, 1].plot([u[0] for u in uncs], [u[1] for u in uncs], 'ro')\n",
    "    axes[1, 1].set_xlim(0, 200)\n",
    "    axes[1, 1].set_ylim(0, 0.5)\n",
    "    axes[1, 1].set_ylabel(\"Uncertainty\")\n",
    "    axes[1, 1].set_xlabel(r\"$N_{samples}$\")\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_plot = interactive(draw, n_samples=widgets.IntSlider(min=1, max=200, step=1, value=50))\n",
    "display(slider_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot on the right (and by looking at how the NLL width shrinks) you can notice how the uncertainty on the estimated parameter decreases as a function of the number of samples (i.e., more statistics implies more precise estimations)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69ca9f6dd25aad5fc7b31625bcfbde310e390cf4c052d14668182af17b0fee34"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
