
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Measurements uncertainties &#8212; Statistical Methods and Data Analysis Techniques</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'errors';</script>
    <link rel="icon" href="_static/favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Interactive Example - Error Matrix" href="interactive-nbs/ErrorMatrix.html" />
    <link rel="prev" title="Random Walk" href="interactive-nbs/randomWalk.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="preface.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Statistical Methods and Data Analysis Techniques - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Statistical Methods and Data Analysis Techniques - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="probability.html">Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/ConditionalProbability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/BayesTheorem.html">Bayes Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/exponentialGrowth.html">Example of exponential growth</a></li>



<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/harmonicMean.html">Harmonic mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/covarianceCorrelation.html">Covariance and correlation</a></li>


</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="probabilityDistributions.html">Probability Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/randomWalk.html">Random Walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Measurements uncertainties</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/ErrorMatrix.html">Interactive Example - Error Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/slidingMean.html">Sliding Mean</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="monteCarlo.html">Monte Carlo methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/randomNumbers.html">Random numbers generators with “numpy”</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Statistical inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="likelihood.html">Parameter Estimation - Likelihood</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/StraighLineFit_LSM_vs_MLM.html">From LSM to MLM (…a.k.a. from Physics 1 laboratory to the Likelihood)</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/MLMethod.html">Interactive Example - ML Method: Mean of a Gaussian</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="leastSquares.html">Parameter Estimation - Least Squares</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="hypothesisTesting.html">Hypotheses Testing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="confidenceIntervals.html">Confidence Intervals</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/Poisson_CI.html">Poisson Confidence Intervals</a></li>


<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/Gaussian_CI.html">Gaussian Confidence Intervals</a></li>

<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/LimitNearBoundariesBayesianApproach.html">Compute the bayesian upper limit for a gaussian near the physical boundary</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/LimitNearBoundariesBayesianApproachPoisson.html">Compute the bayesian upper limit for a Poisson near the physical boundary</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/FC_PoissonMeanWithKnownBackground.html">Feldman-Cousins confidence cnterval construction for a single Poisson, with known background and unknown signal</a></li>

<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/UpperLimit.html">Interactive Example - Upper Limit</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="mva.html">Multivariate Analysis Methods</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="appendix.html">Appendices</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="appendices/Histograms.html">Histograms</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="exercises.html">Exercises</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Histograms.html">Exercises on Histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Probability.html">Exercises on Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_ProbabilityDensityFunctions.html">Exercises on Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Covariance.html">Exercises on Covariance and Correlation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/mdonega/hep-datanalysis-jb/main?urlpath=tree/book/errors.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/errors.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Measurements uncertainties</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainties-and-the-clt">Uncertainties and the CLT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation">Error propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-of-one-variable">Function of one variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-of-several-variables">Function of several variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#several-functions-of-several-variables">Several functions of several variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-on-efficiencies">Uncertainty on efficiencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-on-the-mean">Uncertainty on the mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-mean">Weighted mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-closer-look-at-the-error-matrix">A closer look at the error matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-vs-systematic-uncertainties">Statistical vs. Systematic Uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#work-with-systematic-uncertainties">Work with systematic uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-systematic-uncertainties">Evaluating Systematic Uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-many-digits">How many digits?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="measurements-uncertainties">
<h1>Measurements uncertainties<a class="headerlink" href="#measurements-uncertainties" title="Link to this heading">#</a></h1>
<p>In this chapter we describe how to treat and combine the statistical and
systematic uncertainties associated to measurements.</p>
<section id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h2>
<p><strong>Precision:</strong> how reproducible is the measurement under identical
conditions.</p>
<p><strong>Accuracy:</strong> how close the measured value is to the nominal/reference
value.</p>
<p>One can be very precise, but not accurate (always measuring exactly the
same, but wrong value). More measurements may increase the precision,
but not the accuracy.</p>
<figure class="align-center" id="fig-precisionaccuracy">
<a class="reference internal image-reference" href="_images/precisionAccuracy.png"><img alt="_images/precisionAccuracy.png" src="_images/precisionAccuracy.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Precision and acuracy. <a class="reference external" href="https://en.wikipedia.org/wiki/Accuracy_and_precision">Wikipedia</a></span><a class="headerlink" href="#fig-precisionaccuracy" title="Link to this image">#</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Many particle detector systems are based
on statistical processes (like energy loss by multiple scattering, shower development
in calorimeters etc..). The core of their resolution comes from the
addition of several contributions which are typically described by a
“Gaussian” distribution. When moving away from the core, the distribution often
shows non gaussian tails which depends on the details of the detector.</p>
</aside>
<p><strong>(Intrinsic) Resolution:</strong> the smallest change in a measured value that
the instrument can detect. We will use the names resolution for instruments and sensitivity for
measurements.</p>
<p><strong>Measurement range:</strong> the difference between the largest and the smallest input
value that an instrument is capable of measuring/reading</p>
<p><strong>Dynamic range:</strong> the ratio between measurement range and the
resolution (quoted usually as log value “decibel=dB” in base-10)</p>
<p><strong>Bandwidth:</strong> the difference between the upper and lower frequencies
(in electronics) that an instrument is capable of measuring. Or the
maximal throughput for data transfer (computing)</p>
<p><strong>Calibration:</strong> is the process of comparing the response of a device with unknown accuracy to a reference device with a known accuracy and precision. Or more formally “Operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties (of the calibrated instrument or secondary standard) and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication.” - BIPM: Bureau International des Poids et Mesures <a class="reference external" href="http://www.bipm.org/">BIPM</a></p>
</section>
<section id="uncertainties-and-the-clt">
<h2>Uncertainties and the CLT<a class="headerlink" href="#uncertainties-and-the-clt" title="Link to this heading">#</a></h2>
<p>Any measurement we perform is affected by several uncertainties
generated by several different sources. Let’s say you measure your
weight on a scale. The number indicated by the needle will be affected
by several sources of uncertainties: parallax, rounding, your movements
etc… That’s the reason why when presenting a measurement we don’t
just give the central value <span class="math notranslate nohighlight">\(x\)</span>, but we <span class="math notranslate nohighlight">\(\textit{must}\)</span> quote its
uncertainty <span class="math notranslate nohighlight">\(\pm \sigma_x\)</span>. This number is usually represented by a
Gaussian standard deviation. The reason why we can use the Gaussian
assumption as a way to present the uncertainty (i.e. why we implicitly
make the assumption that the measurements are Gaussian distributed)
comes from the central limit theorem (see <a class="reference internal" href="#./probabilityDistributions.html#the-central-limit-theorem"><span class="xref myst">CLT</span></a>)
A measurement affected by the effect of many independent additive
effects will be “approximately” Gaussian distributed. “Approximately”
means that the <em>core</em> of the distribution is well described by a
Gaussian distribution, while the <em>tails</em> typically will show deviations.</p>
</section>
<section id="error-propagation">
<h2>Error propagation<a class="headerlink" href="#error-propagation" title="Link to this heading">#</a></h2>
<p>Typically the measurement of an observable is extracted from the
combination of several different quantities measured directly. In this
section we will analyse how the uncertainty on those quantities can be
combined/propagated to the final measurement.</p>
<section id="function-of-one-variable">
<h3>Function of one variable<a class="headerlink" href="#function-of-one-variable" title="Link to this heading">#</a></h3>
<p>We start the discussion about error propagation from the most simple
case. Let <span class="math notranslate nohighlight">\(f\)</span> be a function of only one variable <span class="math notranslate nohighlight">\(x\)</span>. The basic idea is
to see how much the function changes when the values of <span class="math notranslate nohighlight">\(x\)</span> moves within
its uncertainty. For this we make a Taylor expansion of <span class="math notranslate nohighlight">\(f\)</span> around
<span class="math notranslate nohighlight">\(x_{0}\)</span> to the first order:</p>
<div class="math notranslate nohighlight">
\[
f(x)\approx f(x_0)+(x-x_0)\left(\frac{df}{dx}\right)_{x=x_0}
\]</div>
<p>Using <span class="math notranslate nohighlight">\(V(f)=&lt;f^2&gt;-&lt;f&gt;^2\)</span> yields</p>
<div class="math notranslate nohighlight">
\[
V(f)=\sigma_f^2\approx \left(\frac{df}{dx}\right)^2\sigma_x^2.
\]</div>
<p>This approximation is only true if the uncertainties are small, i.e. the
first derivative must not vary too much within the neighborhood of a few
<span class="math notranslate nohighlight">\(\sigma\)</span>. The derivative should be estimated at the true value of <span class="math notranslate nohighlight">\(x\)</span>
and when that is unknown its measured value is used.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>You can read the product/quotient formula as: the percentage of fraction error
adds in quadrature (i.e. if <span class="math notranslate nohighlight">\(x\pm3\%\)</span>, <span class="math notranslate nohighlight">\(y\pm4\%\)</span> then the uncertainty
on <span class="math notranslate nohighlight">\(x\cdot y\)</span> and <span class="math notranslate nohighlight">\(x/y\)</span> is <span class="math notranslate nohighlight">\(\pm 5\%\)</span>); same is true for the
reciprocal, the percentage error on a quantity and its reciprocal are
the same.</p>
</aside>
<p>Some examples of error propagation are shown in the following table:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(z = f(x,y)\)</span></p></th>
<th class="head"><p>Uncertainty</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(z = x\pm y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma_z = \sqrt{\sigma_x^2 + \sigma_y^2}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(z = x\cdot k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma_z = k \cdot \sigma_x\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(z = x\cdot y\;\;\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\sigma_z}{z} = \sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(z = x / y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\sigma_z}{z} = \sqrt{\left(\frac{\sigma_x}{x}\right)^2 + \left(\frac{\sigma_y}{y}\right)^2}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(z = x^n\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\sigma_z}{z} = n \frac{\sigma_x}{x}\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">uncertainties</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">ufloat</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x = </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">ufloat</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y = </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># sum</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x + y = </span><span class="si">{</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># multiplication by a number</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x * 2 = </span><span class="si">{</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># product</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x * y = </span><span class="si">{</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># division</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x / y = </span><span class="si">{</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># power</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x ** 2 = </span><span class="si">{</span><span class="n">x</span><span class="w"> </span><span class="o">**</span><span class="w"> </span><span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x = 2.00+/-0.10
y = 3.00+/-0.20
x + y = 5.00+/-0.22
x * 2 = 4.00+/-0.20
x * y = 6.0+/-0.5
x / y = 0.67+/-0.06
x ** 2 = 4.0+/-0.4
</pre></div>
</div>
</div>
</div>
</section>
<section id="function-of-several-variables">
<h3>Function of several variables<a class="headerlink" href="#function-of-several-variables" title="Link to this heading">#</a></h3>
<p>In the case of a function <span class="math notranslate nohighlight">\(f(x,y)\)</span> of two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, we
repeat the Taylor expansion to the first order:</p>
<div class="math notranslate nohighlight">
\[
f(x,y)\approx f(x_0,y_0)+\left(\frac{\partial f}{\partial x}\right)_{x_0,y_0}\cdot(x-x_0)+\left(\frac{\partial f}{\partial y}\right)_{x_0,y_0}\cdot(y-y_0)
\]</div>
<p>Again we assume that the uncertainties are small, which allows us to
drop the higher-order terms of the Taylor expansion. We thus get the
result:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
V(f) &amp;=&amp;\left(\frac{\partial f}{\partial x}\right)^2V(x)+\left(\frac{\partial f}{\partial y}\right)^2V(y)+2\frac{\partial f}{\partial x}\frac{\partial f}{\partial y}\cdot \mbox{cov}(x,y)\\
\sigma_{f}^2&amp;=&amp;\left(\frac{\partial f}{\partial x}\right)^2\sigma_x^2+\left(\frac{\partial f}{\partial y}\right)^2\sigma_y^2+2\frac{\partial f}{\partial x}\frac{\partial f}{\partial y}\cdot \mbox{cov}(x,y)\\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mbox{cov}(x,y) = \left&lt;(x-&lt;x&gt;)\cdot (y-&lt;y&gt;)\right&gt;\)</span> is the
covariance, defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{ij} = \mbox{cov}(x_{i},x_{j}) = 
\left(\begin{array}{cc}
    \sigma_x^2 &amp; \mbox{cov}(x,y) \\
    \mbox{cov}(x,y)   &amp; \sigma_y^2
  \end{array}\right) =
\left(\begin{array}{cc}
    \sigma_x^2 &amp; \rho\sigma_x \sigma_y \\
    \rho\sigma_x \sigma_y &amp; \sigma_y^2
  \end{array}\right)
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho\)</span> is the correlation coefficient defined in Eq.<a class="reference internal" href="probability.html#equation-rho">(1)</a>.</p>
<p>The covariance matrix is a <em>symmetric</em> <span class="math notranslate nohighlight">\(n \times n\)</span>
matrix, which can be generalized for a function <span class="math notranslate nohighlight">\(f\)</span> of <span class="math notranslate nohighlight">\(n\)</span> variables
<span class="math notranslate nohighlight">\(x_1,x_2,\ldots x_n\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
\sigma_f^2=\sum_j\left(\frac{\partial f}{\partial x_j}\right)^2\cdot \sigma^2_{x_j}+\sum_{j}\sum_{k\ne j}\left(\frac{\partial f}{\partial x_j}\right)\left(\frac{\partial f}{\partial x_k}\right)\cdot \mbox{cov}(x_j,x_k)
\]</div>
<p>For continuous variables the diagonal elements <span class="math notranslate nohighlight">\(V_{ij}\)</span> are the
variances</p>
<div class="math notranslate nohighlight">
\[
\sigma_{x_i}^2=\int (x_i-&lt;x_i&gt;)^2 f(x_1,\ldots x_n)dx_1\ldots dx_n
\]</div>
<p>and they are always positive. The off-diagonal elements can be positive
or negative, and they represent the covariances:</p>
<div class="math notranslate nohighlight">
\[
V_{ij}=\int (x_i-&lt;x_i&gt;)(x_j-&lt;x_j&gt;) f(x_1,\ldots x_n)dx_1\ldots dx_n.
\]</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Let <span class="math notranslate nohighlight">\(A = \frac{F-B}{F+B}\)</span> be the measured
forward/backward-asymmetry of an angular distribution, where <span class="math notranslate nohighlight">\(F(B)\)</span>
is the forward (backward) hemisphere of a detector. Be <span class="math notranslate nohighlight">\(N = F+B\)</span> the
total number of measured events. If the uncertainties <span class="math notranslate nohighlight">\(\sigma_{F}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{B}\)</span> for <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are uncorrelated (this is the case if <span class="math notranslate nohighlight">\(N\)</span>
is not fixed), we have</p>
<div class="math notranslate nohighlight">
\[
\sigma_A=\frac{2FB}{N^2}\sqrt{\left(\frac{\sigma_F}{F}\right)^2
+\left(\frac{\sigma_B}{B}\right)^2}.
\]</div>
<p>In the case of Poisson distributed events (<span class="math notranslate nohighlight">\(\sigma_{F}^{2} = F\)</span> and <span class="math notranslate nohighlight">\(\sigma_{B}^{2}=B\)</span>) we
have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sigma_A&amp;=&amp;\frac{2FB}{N^2}\sqrt{\frac{1}{F}
+\frac{1}{B}} \\
\sigma_A&amp;=&amp;\frac{1-A^2}{2}\sqrt{\left(\frac{1}{F}+\frac{1}{B}\right)}\end{aligned}
\end{split}\]</div>
<p>From that we can distinguish two limiting cases:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F\sim B\sim N/2\)</span> and the asymmetry <span class="math notranslate nohighlight">\(A\sim 0\)</span>:
Thus the uncertainty is <span class="math notranslate nohighlight">\(\sigma_A\sim \frac{\delta N}{N}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(F\gg B\)</span> and hence <span class="math notranslate nohighlight">\(A\sim +1\)</span>:
<span class="math notranslate nohighlight">\(\sigma_A\sim \frac{2\delta B}{N}\)</span>, i.e. the uncertainty is
dominated by the uncertainty of the smaller number of events.</p></li>
</ul>
<p>Alternatively we can also fix the total number of events <span class="math notranslate nohighlight">\(N\)</span> and
consider the events <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(B\)</span> as binomially distributed: let <span class="math notranslate nohighlight">\(p\)</span> is
the probability that a particle is registered in the forward hemisphere
of the detector. From this it follows that:
<span class="math notranslate nohighlight">\(\sigma_F^2=\sigma_B^2=Np(1-p)\sim FB/N\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(F\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are fully anti-correlated (which means that cov(F,B) = <span class="math notranslate nohighlight">\(- \sigma^2_{F}\)</span>),
it follows</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sigma_A^2=\left(\frac{\partial A}{\partial F}\, \, \frac{\partial A}{\partial B} \right)\left(\begin{array}{cc}\sigma_F^2 &amp; -\sigma_F^2 \\
-\sigma_F^2 &amp; \sigma_F^2
\end{array}\right)
{\frac{\partial A}{\partial F}\choose \frac{\partial A}{\partial B}}.
\end{split}\]</div>
<p>Finally with <span class="math notranslate nohighlight">\(\partial A/ \partial F= - \partial A/ \partial B=1/N\)</span> we
get:</p>
<div class="math notranslate nohighlight">
\[
\sigma_A=\frac{2}{N}\sqrt{\frac{FB}{N}}
\]</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">ufloat</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x = </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">ufloat</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;y = </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f(x, y) = </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># covariance matrix</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;covariance matrix:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># correlation matrix</span>
<span class="n">corr</span> <span class="o">=</span> <span class="n">uncertainties</span><span class="o">.</span><span class="n">correlation_matrix</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;correlation matrix:</span><span class="se">\n</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x = 2.00+/-0.10
y = 3.00+/-0.20
f(x, y) = 8.0+/-0.4
covariance matrix:
[[0.01 0.   0.01]
 [0.   0.04 0.08]
 [0.01 0.08 0.17]]
correlation matrix:
[[1.         0.         0.24253563]
 [0.         1.         0.9701425 ]
 [0.24253563 0.9701425  1.        ]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="several-functions-of-several-variables">
<h3>Several functions of several variables<a class="headerlink" href="#several-functions-of-several-variables" title="Link to this heading">#</a></h3>
<p>Finally we look at the most general case, in which we have a set of
random variables <span class="math notranslate nohighlight">\({x} = (x_{1}, \ldots, x_{n})\)</span> with expectation
values <span class="math notranslate nohighlight">\({\mu}= (\mu_{1}, \ldots, \mu_{n})\)</span> belonging to the set of
probability density functions <span class="math notranslate nohighlight">\({F}({x})={f_1,f_2,\ldots,f_n}\)</span>.
The covariance matrix <span class="math notranslate nohighlight">\(U_{kl}\)</span> is then given by:</p>
<div class="math notranslate nohighlight">
\[
U_{kl}=cov(f_k,f_l)= \sum_{i,j}\left(\frac{\partial f_k}{\partial x_i}\frac{\partial f_l}{\partial x_j}\right)_{\mbox{x=${\mu}$}} cov(x_i,x_j).
\]</div>
<p>This can be written in a shortened way as <span class="math notranslate nohighlight">\(U = A\, V\, A^{T}\)</span> where the
matrix <span class="math notranslate nohighlight">\(A\)</span> of the derivatives is given by</p>
<div class="math notranslate nohighlight">
\[
A_{ij}=\left(\frac{\partial f_i}{\partial x_j}\right)_{\mbox{ x=$\mu$}}
\]</div>
<p>and <span class="math notranslate nohighlight">\(A^{T}\)</span> is its transpose. The matrices <span class="math notranslate nohighlight">\(U=cov(f_{i},f_{j})\)</span> and
<span class="math notranslate nohighlight">\(V=cov(x_{i},x_{j})\)</span> contain the covariance for <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. Both are
symmetric with dimension <span class="math notranslate nohighlight">\(n \times n\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Transformation to polar coordinates in 2D. Assume we have
measured a point in cartesian coordinates <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> with the
uncertainties <span class="math notranslate nohighlight">\(\sigma_{x}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y}\)</span>. The measurements in <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(y\)</span> shall be independent such that we can write <span class="math notranslate nohighlight">\(V_{11}=\sigma_{x}^{2}\)</span>,
<span class="math notranslate nohighlight">\(V_{22}=\sigma_{y}^{2}\)</span> and <span class="math notranslate nohighlight">\(V_{i \ne j}=0\)</span>. Now we want to get the
covariance matrix in polar coordinates. The transformation equations are
<span class="math notranslate nohighlight">\(f_1:\,r^2=x^2+y^2\)</span> and <span class="math notranslate nohighlight">\(f_2:\, \theta=arctan (y/x)\)</span>. It follows for
<span class="math notranslate nohighlight">\(A=\partial f_i/\partial x_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left( \begin{array}{cc}
 \frac{\partial r}{\partial x} &amp; \frac{\partial r}{\partial y}  \\
 \frac{\partial \theta}{\partial x}&amp; \frac{\partial \theta}{\partial y} \end{array} \right)
 =\left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{y}{r}  \\
 \frac{-y}{r^2}&amp; \frac{x}{r^2} \end{array} \right)
\end{split}\]</div>
<p>And with this we can compute <span class="math notranslate nohighlight">\(U=A\,V\,A^{T}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
U &amp;=&amp; \left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{y}{r}  \\
 \frac{-y}{r^2}&amp; \frac{x}{r^2} \end{array} \right)
 \cdot \left( \begin{array}{cc}
 \sigma_x^2 &amp; 0  \\
 0&amp; \sigma_y^2 \end{array} \right) \cdot
\left( \begin{array}{cc}
 \frac{x}{r} &amp; \frac{-y}{r^2}  \\
 \frac{y}{r}&amp; \frac{x}{r^2} \end{array} \right)\\
U&amp;=&amp;\left( \begin{array}{cc}
 \frac{1}{r^2}(x^2\sigma^2_x+y^2\sigma^2_y) &amp; \frac{xy}{r^3}(-\sigma_x^2+\sigma_y^2)  \\
 \frac{xy}{r^3}(-\sigma_x^2+\sigma_y^2)&amp; \frac{1}{r^4}(y^2\sigma_x^2+x^2\sigma_y^2) \end{array} \right)
=\left( \begin{array}{cc}
\sigma_r^2 &amp; \sigma^2_{r\theta} \\
\sigma^2_{r\theta} &amp; \sigma_{\theta}^2
\end{array} \right)
\end{aligned}
\end{split}\]</div>
</div>
</section>
</section>
<section id="uncertainty-on-efficiencies">
<h2>Uncertainty on efficiencies<a class="headerlink" href="#uncertainty-on-efficiencies" title="Link to this heading">#</a></h2>
<p>Suppose you are performing a “counting experiment” and you decide to
accept events only if they pass a set of selection criteria (e.g. <span class="math notranslate nohighlight">\(p_T&gt;\)</span>
20 GeV; <span class="math notranslate nohighlight">\(|\eta|&lt;2.5\)</span>). Call <span class="math notranslate nohighlight">\(N_0\)</span> the total number of events and <span class="math notranslate nohighlight">\(N_p\)</span>
the subset passing the selection. The efficiency of the selection is:</p>
<div class="math notranslate nohighlight">
\[
\epsilon = \frac{N_p}{N_0}
\]</div>
<p>You cannot apply a straightforward error
propagation on uncorrelated Poisson uncertainties because <span class="math notranslate nohighlight">\(N_p\)</span> and
<span class="math notranslate nohighlight">\(N_0\)</span> are correlated.</p>
<p>The correct way to compute the uncertainty is to look at the equivalent
binomial problem with total number of events <span class="math notranslate nohighlight">\(N_0\)</span> and probability
<span class="math notranslate nohighlight">\(\epsilon\)</span> to pass:</p>
<div class="math notranslate nohighlight">
\[
\left( \Delta \epsilon \right)^2 = \frac{\epsilon(1-\epsilon)}{N_0}
\]</div>
<p>Another way to get to the same result is to work with the uncorrelated
variables pass <span class="math notranslate nohighlight">\(N_p\)</span> and fail <span class="math notranslate nohighlight">\(N_f\)</span>, such that the total number <span class="math notranslate nohighlight">\(N_0\)</span> is
not a fixed number: $<span class="math notranslate nohighlight">\(\epsilon = \frac{N_p}{N_p + N_f}\)</span>$ and then apply
error propagation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\left( \Delta \epsilon \right)^2 &amp;=&amp; \left( \frac{\partial \epsilon}{\partial N_p}\right)^2(\Delta N_p)^2 + \left(  \frac{\partial \epsilon}{\partial N_f}\right)^2 (\Delta N_f)^2 \\
 &amp;=&amp; \left( \frac{N_f}{N_0^2}\right)^2(\Delta N_p)^2 + \left(-\frac{N_p}{N_0^2}\right)^2 (\Delta N_f)^2 \\
 &amp;=&amp; \frac{(1-\epsilon)^2N_p + \epsilon^2 N_f}{N_0}\\
 &amp;=&amp; \frac{\epsilon(1-\epsilon)}{N_0}\\
\end{aligned}
\end{split}\]</div>
</section>
<section id="uncertainty-on-the-mean">
<h2>Uncertainty on the mean<a class="headerlink" href="#uncertainty-on-the-mean" title="Link to this heading">#</a></h2>
<p>Suppose we measure <span class="math notranslate nohighlight">\(n\)</span> times the quantity <span class="math notranslate nohighlight">\(x\)</span>. The measured mean value
of <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\(\bar{x} = \sum_i x_i / n\)</span>. As all the single measurements,
also the mean will be affected by statistical fluctuations. The
difference between the measured mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span> (the
true-unknown value of the quantity) is described by a Gaussian
distribution (because of the CLT) with variance
<span class="math notranslate nohighlight">\(V(\bar{x}) = \sigma^2 / n\)</span>. For <span class="math notranslate nohighlight">\(n\to \infty\)</span> (if the measurement is
not biased, see <a class="reference internal" href="#likelihood.html#properties-of-the-estimators"><span class="xref myst">properties of estimators</span></a>
it will converge to the “true” value:
<span class="math notranslate nohighlight">\(\langle \bar{x} \rangle = \mu\)</span>. The variance of <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the
variance of <span class="math notranslate nohighlight">\(x\)</span> divided by <span class="math notranslate nohighlight">\(n\)</span>: <span class="math notranslate nohighlight">\(V(\bar{x})= \sigma^2 /n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
  Var(\bar{x}) &amp; = &amp;  \langle (\bar{x} -\mu )^2 \rangle \\
               &amp; = &amp;  \langle \left( \frac{1}{n} \sum_i x_i - \mu  \right) ^2\rangle\\
               &amp; = &amp;  \frac{1}{n^2} n \langle x^2 \rangle + \frac{n(n-1)}{n^2} \langle x_i x_j\rangle_{i\neq j} - 2\mu\langle \bar{x} \rangle + \mu^2\\
               &amp; = &amp;  \frac{\langle x^2 \rangle}{n} + \frac{n-1}{n} \mu^2 -\mu^2\\
               &amp; = &amp;  \frac{\langle x^2 -\mu^2\rangle}{n} = \frac{\sigma^2}{n} \\
\end{aligned}
\end{split}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>A challeging question often arises in experiments with long datataking periods: shall I keep collecting data and improve my measurements as <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> or shall I upgrade my experiment ?</p>
</aside>
<p>The <em>standard deviation of the mean</em> falls like <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>. To improve
the resolution of your measurement by a factor of 2 you need to get 4
times more measurements (<em>slang:</em> 4 times more statistics).</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Take a photo-detector with an energy resolution of 50 keV.
If a mono-energetic photon (coming e.g. from a certain nuclear decay) is
registered, its energy is only known to a precision of 50 keV. If 100
(mono-energetic) photons are measured (all coming from the same nuclear
decay), then the uncertainty of the mean energy is only
<span class="math notranslate nohighlight">\(50/\sqrt{100} = 5\)</span> keV. For a resolution of 1 keV we need 2500 events.
So, to double the precision, you need four times more photons.</p>
</div>
</section>
<section id="weighted-mean">
<h2>Weighted mean<a class="headerlink" href="#weighted-mean" title="Link to this heading">#</a></h2>
<p>Suppose we want to compute the average of a set of measurements <span class="math notranslate nohighlight">\(x_{i}\)</span>
with different uncertainties <span class="math notranslate nohighlight">\(\sigma_{i}\)</span>. Intuitively the measurements
with large uncertainties will “matter” less than measurements with small
uncertainties.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>You have two measurements of <span class="math notranslate nohighlight">\(x\)</span>: <span class="math notranslate nohighlight">\(10\pm0.1\)</span> and <span class="math notranslate nohighlight">\(8\pm 5\)</span>.
In this case the second measurement will have basically no weight in
your knowledge about <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div>
<p>The correct way to obtain the mean in this case is to take into account
explicitly the uncertainty of the measurements:</p>
<div class="math notranslate nohighlight" id="equation-wmean">
<span class="eqno">(4)<a class="headerlink" href="#equation-wmean" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\bar{x}&amp;=&amp;\frac{\sum x_i/\sigma_i^2}{\sum 1/\sigma_i^2} \\
\sigma_{\bar{x}}^2&amp;=&amp;\frac{1}{\sum 1/\sigma_i^2}
\end{aligned}\end{split}\]</div>
<p>In this
case the individual results are weighted such that the values with small
uncertainties contribute more to the average.
Some comments:</p>
<ul class="simple">
<li><p>The weighted mean collapses to the arithmetic mean when fixing all
the uncertainty’s to be equal</p></li>
<li><p>“Few measurements with small uncertainties are better than many
measurements with large uncertainties”. Let the uncertainty of a
first set of <span class="math notranslate nohighlight">\(n_1\)</span> measurements of the quantity <span class="math notranslate nohighlight">\(x\)</span> be <span class="math notranslate nohighlight">\(\sigma_{1}\)</span>.
The uncertainty on the mean is
<span class="math notranslate nohighlight">\(\sigma_{\bar{x}} = \sigma_1/\sqrt{n_1}\)</span>. If we have a second set of
<span class="math notranslate nohighlight">\(n_2\)</span> measurements with uncertainty <span class="math notranslate nohighlight">\(\sigma_{2}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{2} &gt; \sigma_{1}\)</span> then to get to the same precision you need
to collect more data as:
<span class="math notranslate nohighlight">\(n_{2} = n_{1} \left( \frac{\sigma_{2}}{\sigma_{1}} \right) ^{2}\)</span></p></li>
<li><p>Care must be taken if the individual results and their uncertainty’s
deviate too much from each other. Consider the following example: An
experiment measures in one hour <span class="math notranslate nohighlight">\(100 \pm 10\)</span> events, and another
experiment measures in one hour only <span class="math notranslate nohighlight">\(1 \pm 1\)</span> events. The
Eq.<a class="reference internal" href="#equation-wmean">(4)</a> would then tell us that we have <span class="math notranslate nohighlight">\(2 \pm 1\)</span>
events. But the (unweighted) mean would give <span class="math notranslate nohighlight">\(50.5 \pm 5\)</span>. In this
case instead of blindly quote the mean or the weighted mean you
should go back and understand <em>why</em> you get such different outcomes
(it might be a problem of some parameters of the data taking, some
faulty equipment, some trivial mistake etc…). In case you can’t
find any reason for that, it would be wise to give the full
information at hand and preset both results</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Compute the best estimate of the Higgs mass from the ATLAS (
<span class="math notranslate nohighlight">\(m_H= 125.36 \pm 0.41\)</span> GeV) <a class="reference external" href="https://arxiv.org/pdf/1406.3827v1.pdf">ATLAS</a> and CMS
(<span class="math notranslate nohighlight">\(m_H = 125.02 \pm 0.30\)</span> GeV) <a class="reference external" href="https://arxiv.org/pdf/1412.8662v1.pdf">CMS</a>. Applying the formula for the
weighted average we get: <span class="math notranslate nohighlight">\(m_H = 125.14 \pm 0.24\)</span> GeV. Compare it with
the official LHC <a class="reference external" href="http://arxiv.org/abs/1503.07589">combination</a>.</p>
</div>
</section>
<section id="a-closer-look-at-the-error-matrix">
<h2>A closer look at the error matrix<a class="headerlink" href="#a-closer-look-at-the-error-matrix" title="Link to this heading">#</a></h2>
<p>We have encountered in the previous sections the error matrix (also
called covariance matrix). Here we will take a closer look at it,
focusing on the importance of the off-diagonal terms describing the
correlations.<br />
Let’s start from the case of a 2D probability density function built
from two <em>uncorrelated</em> Gaussian distributions in <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The two
p.d.f.’s are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
P(x) &amp;=&amp; \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma_x} e^{-\frac{1}{2}\frac{x^2}{\sigma_x^2}}\\
P(y) &amp;=&amp; \frac{1}{\sqrt{2\pi}} \frac{1}{\sigma_y} e^{-\frac{1}{2}\frac{y^2}{\sigma_y^2}}
\end{aligned}
\end{split}\]</div>
<p>(for simplicity we take the two distributions to be centred at 0) and
the combined 2D uncorrelated distribution is just the product of the
two:</p>
<div class="math notranslate nohighlight">
\[
P(x,y) = \frac{1}{2\pi} \frac{1}{\sigma_x\sigma_y} e^{-\frac{1}{2} \left( \frac{x^2}{\sigma_x^2} + \frac{y^2}{\sigma_y^2}\right)}
\]</div>
<p>In one dimension the Gaussian probability is reduced by <span class="math notranslate nohighlight">\(1/\sqrt{e}\)</span>
when moving away from the maximum by 1<span class="math notranslate nohighlight">\(\sigma\)</span>. In 2D this point becomes
a curve and in this particular example an ellipse with equation:
$<span class="math notranslate nohighlight">\(\frac{x^2}{\sigma_x^2} +\frac{y^2}{\sigma_y^2} = 1\)</span>$ We can rewrite
the same equation in matrix form (in the case of no correlation is an
overkill but this notation will become useful in the following):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(x,y \right)
\left( \begin{array}{cc}
    \frac{1}{\sigma_x^2} &amp; 0 \\
    0 &amp; \frac{1}{\sigma_y^2}  \\
  \end{array} \right)
\left( \begin{array}{c}
  x\\
  y\\
\end{array} \right) = 1
\end{split}\]</div>
<p>The matrix in the previous equation is called
the <em>inverse of the error matrix</em> and its inverse is called <strong>error matrix for <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left( \begin{array}{cc}
    \sigma_x^2 &amp; 0 \\
    0 &amp; \sigma_y^2  \\
  \end{array} 
  \right)
\end{split}\]</div>
<p>The general element of the error(covariance)
matrix for <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(x_1,\ldots,x_n\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\langle (x_i-\bar{x_i})(x_j-\bar{x_j}) \rangle.
\]</div>
<p>The notation above allows to treat in a simple way the case of
<em>correlated variables</em>.</p>
<p>Take the previous uncorrelated case and rotate the <span class="math notranslate nohighlight">\((x,y)\)</span> axes as :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
x' &amp;=&amp; x\cos\theta - y\sin\theta\\
y' &amp;=&amp; x\sin\theta + y\cos\theta\end{aligned}
\end{split}\]</div>
<p>Let’s use a numerical example: be <span class="math notranslate nohighlight">\(\sigma_x = 1/4\)</span> and <span class="math notranslate nohighlight">\(\sigma_y = 1/2\)</span> (see <a class="reference internal" href="#fig-errorellipse"><span class="std std-numref">Fig. 6</span></a>). An <a class="reference internal" href="interactive-nbs/ErrorMatrix.html"><span class="std std-doc">interactive version</span></a> of it is also available.</p>
<figure class="align-center" id="fig-errorellipse">
<a class="reference internal image-reference" href="_images/errorEllipse.png"><img alt="_images/errorEllipse.png" src="_images/errorEllipse.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Interpretation of the uncertainty for two variables: the error
ellipse.</span><a class="headerlink" href="#fig-errorellipse" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Then the uncorrelated case reads:</p>
<div class="math notranslate nohighlight">
\[
16 x^2 + 4 y^2 =1.
\]</div>
<p>Applying the rotation (i.e. correlating the two measurements (see
<a class="reference internal" href="#fig-errorellipse"><span class="std std-numref">Fig. 6</span></a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left( x~y \right)'
\left( \begin{array}{cc}
    \cos\theta   &amp; \sin\theta \\
    -\sin \theta &amp; \cos \theta  \\
  \end{array} \right)
\left( \begin{array}{cc}
    1/\sigma_x^2 &amp; 0 \\
    0 &amp; 1/\sigma_y^2  \\
  \end{array} \right)
\left( \begin{array}{cc}
    \cos\theta   &amp; -\sin\theta \\
    \sin \theta &amp; \cos \theta  \\
  \end{array} \right)
\left( \begin{array}{c}
    x \\
    y \\
  \end{array} \right)'
  
\end{split}\]</div>
<p>we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}\label{eq:ellipse}
\left( x~y \right)'
\left( \begin{array}{cc}
    13 &amp;  3\sqrt{3} \\
    3\sqrt{3} &amp;  7 \\
  \end{array} \right)
\left( \begin{array}{c}
    x \\
    y \\
  \end{array} \right)'.
\end{split}\]</div>
<p>The matrix in the centre is the “inverse error
matrix” and its inverse is the “error matrix”:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{1}{64}
\left( \begin{array}{cc}
    7 &amp;  -3\sqrt{3} \\
    -3\sqrt{3} &amp;  13 \\
  \end{array} \right)
\end{split}\]</div>
<p>Given the error matrix is trivial to extract
uncertainties on the variables and their correlation coefficients:</p>
<ul class="simple">
<li><p>the uncertainty on <span class="math notranslate nohighlight">\(x'\)</span> is given by intersection of the rectangle
inscribing the ellipse with the x-axis: <span class="math notranslate nohighlight">\(\sigma_{x'}^2 = 7/64\)</span> the
square root of the first diagonal element of the error matrix</p></li>
<li><p>the uncertainty on <span class="math notranslate nohighlight">\(y'\)</span> is given by intersection of the rectangle
inscribing the ellipse with the y-axis: <span class="math notranslate nohighlight">\(\sigma_{y'}^2 = 13/64\)</span> the
square root of the second diagonal element of the error matrix</p></li>
<li><p>the intersection of the ellipse with the x-axis is
<span class="math notranslate nohighlight">\(\sqrt{1/13} = 0.277\)</span> the inverse of the square root of the first
diagonal element of the inverse error matrix</p></li>
<li><p>the intersection of the ellipse with the y-axis is
<span class="math notranslate nohighlight">\(\sqrt{1/7} = 0.378\)</span> the inverse of the square root of the second
diagonal element of the inverse error matrix</p></li>
<li><p>the off-diagonal elements of the error-matrix are
<span class="math notranslate nohighlight">\(\rho \sigma_{x'} \sigma_{y'}\)</span>; knowing <span class="math notranslate nohighlight">\(\sigma_{x'}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{y'}\)</span> from the diagonal elements we obtain a correlation
coefficient <span class="math notranslate nohighlight">\(\rho = 0.54\)</span>.</p></li>
<li><p>the semi-axes of the ellipse are the square roots of the eigenvalues
of the error matrix (here we know the diagonalized matrix, i.e.
before rotation, and we can just read them off: 0.25, 0.5)</p></li>
</ul>
</section>
<section id="statistical-vs-systematic-uncertainties">
<h2>Statistical vs. Systematic Uncertainties<a class="headerlink" href="#statistical-vs-systematic-uncertainties" title="Link to this heading">#</a></h2>
<p>When repeating measurements (for example to reduce the uncertainty by
averaging over many results), the usual assumption is that the
experiments can be repeated under identical conditions, being
independent of each other and thus giving identical, independent
results. Unfortunately, this <em>ideal world</em> does not exist. Repeated
measurements will give slightly different results, due to diverse
sources such as changing experimental conditions (mostly unknown),
imprecise measurement (resolution), thermal or quantum fluctuations and
others. The differences in the results are “randomly” varying, giving
the so-called <em>statistical uncertainty</em>. For these kind of
uncertainties, as in previous section, repeating the measurement
increases the precision.</p>
<p>A different kind of uncertainty is represented by the <em>systematic
uncertainty</em>:</p>
<ul class="simple">
<li><p><strong>Systematic effect</strong>= background, selection bias, efficiencies,
energy resolutions, angular resolution, theory
renormalizarion/factorization scales, etc…</p></li>
<li><p><strong>Systematic uncertainty</strong> = the uncertainty in estimating a
systematic effect</p></li>
<li><p><strong>Systematic mistake</strong> = result of negleting such effects</p></li>
</ul>
<p>Systematic uncertainties are usually independent from the statistical
uncertainties. It is therefore important to always quote both
uncertainties separately in the results:</p>
<div class="math notranslate nohighlight">
\[
x = 10.2 \pm 0.2 \text{ (stat)} \pm 0.3 \text{ (syst)} [units]%  \pm 0.3 \text{ (theory)} [units].
\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Some systematic uncertainties do get reduced with larger data
samples. Take the case of a systematics uncertainty associated to a
calibration. If the calibration is performed on a sample of
available data, the larger the calibration sample the smaller will
be the uncertainty.</p>
</aside>
<p>The systematic uncertainty is a statement made by the experimenters
about their understanding of their own equipment, and in general it will
not decrease with larger data samples (like the statistical
uncertainty). An interesting situation is reached when the
systematic uncertainty is larger than the statistical one. In this case
the precision of the result will not be improved by taking more data; it
will only improve by better understanding the experimental setup.</p>
</section>
<section id="work-with-systematic-uncertainties">
<h2>Work with systematic uncertainties<a class="headerlink" href="#work-with-systematic-uncertainties" title="Link to this heading">#</a></h2>
<p>Once the systematic uncertainties are singled out, they can be treated
with the same covariance matrices techniques developed above for the
statistical uncertainties.<br />
Suppose you have two measurements <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> with statistical
uncertainties <span class="math notranslate nohighlight">\(\sigma_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\)</span> respectively and a common
systematic uncertainty <span class="math notranslate nohighlight">\(S\)</span>. Putting together the components in a matrix
we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{i,j}^{tot} = \left( \begin{array}{cc}
\sigma_1^2+S^2 &amp; S^2  \\
 S^2&amp; \sigma_2^2+S^2\end{array} \right)
\end{split}\]</div>
<p>If, instead of a constant systematic uncertainty, the uncertainty is
given as a percentage <span class="math notranslate nohighlight">\(T= \epsilon x_{i}\)</span> (e.g. <span class="math notranslate nohighlight">\(\epsilon = 0.01\)</span> for a
1%), then the covariance matrix is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{i,j}^{tot} = \left( \begin{array}{cc}
 \sigma_1^2+\epsilon^2 x_1^2 &amp; \epsilon^2x_1x_2  \\
 \epsilon^2x_1x_2 &amp; \sigma^2+\epsilon^2x_2^2 \end{array} \right)
 \end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Consider two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> with two sources of
uncertainties: a statistical (<span class="math notranslate nohighlight">\(s_{x}, s_{y}\)</span>) with <em>no</em> correlation and
a systematic (<span class="math notranslate nohighlight">\(c_{x}, c_{y}\)</span>) with <em>full</em> correlation (e.g. luminosity):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
x &amp;=&amp; x_{0} \pm s_{x} \text{ (stat)} \pm c_{x} \text{ (syst)} \\
y &amp;=&amp; y_{0} \pm s_{y} \text{ (stat)} \pm c_{y} \text{ (syst)}
\end{aligned}
\end{split}\]</div>
<p>Because the uncertainties are already separated into a correlated and
uncorrelated category, they can be summed up in quadrature at the matrix
level, yielding:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{ij}^{tot} = \left( \begin{array}{cc}
s_{x}^{2} &amp; 0 \\
0 &amp; s_{y}^{2} \end{array} \right) + \left( \begin{array}{cc}
c_{x}^{2} &amp; c_{xy} \\
c_{yx} &amp; c_{y}^{2} \end{array} \right) = \left( \begin{array}{cc}
\sigma_{x}^{2} &amp; \rho \sigma_{x} \sigma_{y} \\
\rho \sigma_{x} \sigma_{y} &amp; \sigma_{y}^{2} \end{array} 
\right),
\end{split}\]</div>
<p>where
<span class="math notranslate nohighlight">\(\rho\)</span> is the correlation coefficient
<span class="math notranslate nohighlight">\(\rho = \frac{c_{xy}}{\sigma_{x} \sigma_{y}}\)</span> and
<span class="math notranslate nohighlight">\(\sigma_{i}^{2} = s_{i}^{2} + c_{i}^{2}\)</span> is the sum of the squared
individual uncertainties for <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Take three variables <span class="math notranslate nohighlight">\(x_{1}, x_{2}, x_{3}\)</span> with statistical
uncertainties <span class="math notranslate nohighlight">\(\sigma_{1},\sigma_{2},\sigma_{3}\)</span>, a common systematic
uncertainty <span class="math notranslate nohighlight">\(S\)</span> and a second systematic uncertainty <span class="math notranslate nohighlight">\(T\)</span> shared by only
<span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>. In this case the covariance matrix reads:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{i,j}^{tot} = \left( \begin{array}{ccc}
 \sigma_1^2+S^2+T^2 &amp; S^2+T^2  &amp; S^2 \\
 S^2+T^2 &amp; \sigma_2^2+S^2+T^2 &amp; S^2 \\
S^2 &amp; S^2 &amp; \sigma_3^2+S^2
 \end{array} \right)
\end{split}\]</div>
</div>
</section>
<section id="evaluating-systematic-uncertainties">
<h2>Evaluating Systematic Uncertainties<a class="headerlink" href="#evaluating-systematic-uncertainties" title="Link to this heading">#</a></h2>
<p>To deal with systematic uncertainties one has to distinguish between
<em>known</em> and <em>unknown</em> (or <em>unsuspected</em>) sources of uncertainties.</p>
<p>“Known” sources can be:</p>
<ul class="simple">
<li><p>uncertainties on factors in the analysis: calibration, efficiencies,
corrections, etc…</p></li>
<li><p>theoretical uncertainties on branching ratios, masses, fragmentation
etc…</p></li>
</ul>
<p>To evaluate the impact of systematic uncertainties from known sources
<span class="math notranslate nohighlight">\(s_{i}\)</span> on a correction factor <span class="math notranslate nohighlight">\(F\)</span>, there are several possibilities.
Either one can take several (the more the better) typical assumptions
for <span class="math notranslate nohighlight">\(s_{i}\)</span> and repeat the calculation of <span class="math notranslate nohighlight">\(F\)</span> and then calculate the
standard deviation of <span class="math notranslate nohighlight">\(F\)</span>. Or an experimental parameter (for example the
energy resolution) can be varied up and down by one sigma and check the
change in the variable. Or again another possibility is to take two
extreme assumptions as values for the source <span class="math notranslate nohighlight">\(s_{i}\)</span> and argue that the
true value has to be in between them and use the difference divided by
<span class="math notranslate nohighlight">\(\sqrt{12}\)</span>. The factor <span class="math notranslate nohighlight">\(\sqrt{12}\)</span> is due to the standard deviation of
the uniform distribution, which can be used to model the total ignorance
about the parameter value.</p>
<p>Uncertainties from “unsuspected” sources can be studied by repeating the
analysis in different ways such as:</p>
<ul class="simple">
<li><p>vary the range of data used for extraction of the result</p></li>
<li><p>use only a subset of the data (e.g. split the data into two
categories)</p></li>
<li><p>change cuts, binning, borders of the histogram.</p></li>
<li><p>change the parameterization or the fit technique</p></li>
</ul>
<p>Finding trends as a result of the above checks usually points to an
“unsuspected” systematic effect.<br />
<br />
However you <em>should not</em> go through the previous list blindly and sum up
in quadrature all resulting deviations. This will simply increase the
systematic uncertainty the more effects the experimenters will conceive!
It is wrong to state that “the more careful you are the bigger should
your systematic uncertainty be”. Remember that out of 20 checks one is
expected to be off more than <span class="math notranslate nohighlight">\(2\sigma\)</span> and every third is off by
<span class="math notranslate nohighlight">\(1\sigma\)</span> or more. If no systematic effect is expected a priori, and if
the deviation from the expected result is not significant, no additional
systematic uncertainty must be added. On the other hand, if you do see a
deviation, try to understand where it comes from and eventually fix it.
Only as a last resort include a discrepancy in systematic uncertainties.</p>
</section>
<section id="how-many-digits">
<h2>How many digits?<a class="headerlink" href="#how-many-digits" title="Link to this heading">#</a></h2>
<p>We report here the recommendation from the PDG on how to round the
numbers representing your results. The basic rule states that if the
three highest order digits of the uncertainty lie between 100 and 354,
we round to two significant digits. If they lie between 355 and 949, we
round to one significant digit. Finally, if they lie between 950 and
999, we round up to 1000 and keep two significant digits. In all cases,
the central value is given with a precision that matches that of the
uncertainty. So, for example, the result (coming from an average) 0.827
<span class="math notranslate nohighlight">\(\pm\)</span> 0.119 would appear as 0.83 <span class="math notranslate nohighlight">\(\pm\)</span> 0.12, while 0.827 <span class="math notranslate nohighlight">\(\pm\)</span> 0.367
would turn into 0.8 <span class="math notranslate nohighlight">\(\pm\)</span> 0.4.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>Most of the material of this section is taken from:</p>
<ul class="simple">
<li><p>L. Lyons, <span id="id1">[<a class="reference internal" href="bibliography.html#id4" title="Lyons Louis. Statistics for Nuclear and Particle Physicists. Cambridge University Press, 1986. URL: https://www.cambridge.org/highereducation/books/statistics-for-nuclear-and-particle-physicists/9544B39F3244D9457BEC324CD34F1571#overview.">Lou86</a>]</span>, “Statistics for Nuclear and Particle Physicist”: Ch. 1 and 3</p></li>
<li><p>R. Barlow, <span id="id2">[<a class="reference internal" href="bibliography.html#id8" title="Roger Barlow. Statistics - A guide to the use of statistical methods in the physical sciences. Wiley, 1989. URL: https://www.wiley.com/en-us/Statistics:+A+Guide+to+the+Use+of+Statistical+Methods+in+the+Physical+Sciences-p-9780471922957.">Bar89</a>]</span>, “ A guide to the use of statistical methods in the physical sciences”. Ch. 4</p></li>
<li><p>J.R. Taylor, <span id="id3">[<a class="reference internal" href="bibliography.html#id9" title="John Taylor. An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements. University Science Books, 1982. URL: https://uscibooks.aip.org/books/an-introduction-to-error-analysis-the-study-of-uncertainties-in-physical-measurements-third-edition/.">Tay82</a>]</span>, “Introduction to error analysis”</p></li>
<li><p>R. Barlow, <span id="id4">[<a class="reference internal" href="bibliography.html#id10" title="Roger Barlow. Systematic errors: facts and fictions. 2002. URL: https://arxiv.org/abs/hep-ex/0207026, doi:10.48550/ARXIV.HEP-EX/0207026.">Bar02</a>]</span>, “Systematic Errors: facts and fictions”</p></li>
<li><p>PDG, <span id="id5">[<a class="reference internal" href="bibliography.html#id7" title="Particle Data Group. Passage of Particle Through Matter. Lawrence Berkeley National Laboratory, 2022. URL: https://pdg.lbl.gov/2022/reviews/rpp2022-rev-rpp-intro.pdf.">Gro22b</a>]</span>, Introduction, 5.3 Rounding</p></li>
</ul>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="interactive-nbs/randomWalk.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Random Walk</p>
      </div>
    </a>
    <a class="right-next"
       href="interactive-nbs/ErrorMatrix.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Interactive Example - Error Matrix</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainties-and-the-clt">Uncertainties and the CLT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation">Error propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-of-one-variable">Function of one variable</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-of-several-variables">Function of several variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#several-functions-of-several-variables">Several functions of several variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-on-efficiencies">Uncertainty on efficiencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-on-the-mean">Uncertainty on the mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-mean">Weighted mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-closer-look-at-the-error-matrix">A closer look at the error matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-vs-systematic-uncertainties">Statistical vs. Systematic Uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#work-with-systematic-uncertainties">Work with systematic uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-systematic-uncertainties">Evaluating Systematic Uncertainties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-many-digits">How many digits?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mauro Donega
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>