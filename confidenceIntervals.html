
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Confidence Intervals &#8212; Statistical Methods and Data Analysis Techniques</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'confidenceIntervals';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Poisson Confidence Intervals" href="interactive-nbs/Poisson_CI.html" />
    <link rel="prev" title="Hypotheses Testing" href="hypothesisTesting.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="preface.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Statistical Methods and Data Analysis Techniques - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Statistical Methods and Data Analysis Techniques - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="preface.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="probability.html">Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/ConditionalProbability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/BayesTheorem.html">Bayes Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/exponentialGrowth.html">Example of exponential growth</a></li>



<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/harmonicMean.html">Harmonic mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/covarianceCorrelation.html">Covariance and correlation</a></li>


</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="probabilityDistributions.html">Probability Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/randomWalk.html">Random Walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="errors.html">Measurements uncertainties</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/ErrorMatrix.html">Interactive Example - Error Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/slidingMean.html">Sliding Mean</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="monteCarlo.html">Monte Carlo methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/randomNumbers.html">Random numbers generators with “numpy”</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="inference.html">Statistical inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="likelihood.html">Parameter Estimation - Likelihood</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/StraighLineFit_LSM_vs_MLM.html">From LSM to MLM (…a.k.a. from Physics 1 laboratory to the Likelihood)</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/MLMethod.html">Interactive Example - ML Method: Mean of a Gaussian</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="leastSquares.html">Parameter Estimation - Least Squares</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="hypothesisTesting.html">Hypotheses Testing</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Confidence Intervals</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/Poisson_CI.html">Poisson Confidence Intervals</a></li>


<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/Gaussian_CI.html">Gaussian Confidence Intervals</a></li>

<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/LimitNearBoundariesBayesianApproach.html">Compute the bayesian upper limit for a gaussian near the physical boundary</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/LimitNearBoundariesBayesianApproachPoisson.html">Compute the bayesian upper limit for a Poisson near the physical boundary</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/FC_PoissonMeanWithKnownBackground.html">Feldman-Cousins confidence cnterval construction for a single Poisson, with known background and unknown signal</a></li>

<li class="toctree-l2"><a class="reference internal" href="interactive-nbs/UpperLimit.html">Interactive Example - Upper Limit</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="mva.html">Multivariate Analysis Methods</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="appendix.html">Appendices</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="appendices/Histograms.html">Histograms</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="exercises.html">Exercises</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Histograms.html">Exercises on Histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Probability.html">Exercises on Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_ProbabilityDensityFunctions.html">Exercises on Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercises/Ex_Covariance.html">Exercises on Covariance and Correlation</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/mdonega/hep-datanalysis-jb/main?urlpath=tree/book/confidenceIntervals.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/confidenceIntervals.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Confidence Intervals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-belt-neyman-frequentist-construction">Confidence belt - Neyman/Frequentist construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-likelihood-or-the-chi-2-to-set-confidence-intervals">Use the likelihood or the <span class="math notranslate nohighlight">\(\chi^2\)</span> to set confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-near-boundaries">Limits near boundaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feldman-cousins">Feldman-Cousins</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-flip-flop-problem">The flip-flop problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-with-background">Poisson with background</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-with-boundary-at-the-origin">Gaussian with boundary at the origin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neutrino-oscillations">Neutrino oscillations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-examples">Other examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underfluctuations-and-significance">Underfluctuations and significance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lep-test-statistic-l-s-b-l-b">LEP test statistic: <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nuisance-parameters">Nuisance parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-issue-of-sensitivity-and-the-cls-procedure">The issue of sensitivity and the CLs procedure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lhc-test-statistics-2-ln-lambda-mu">LHC test statistics: <span class="math notranslate nohighlight">\(-2\ln(\lambda(\mu))\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-likelihood-ratio">Profile likelihood ratio</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discovery-test-statistics">Discovery test statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-formulas">Asymptotic Formulas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asimov-dataset">Asimov dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-limits-test-statistic">Upper limits test statistic</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-measurements">Combining measurements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discovery-significance-s-sqrt-b">Discovery significance: <span class="math notranslate nohighlight">\(S/\sqrt{B}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-from-the-search-of-the-higgs-at-the-lhc">Examples from the search of the Higgs at the LHC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-fit-signal-strength">Best fit signal strength</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-other-parameters">Extracting other parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approach-to-upper-limits">Bayesian approach to upper limits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#look-elsewhere-effect">Look-Elsewhere Effect</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="confidence-intervals">
<h1>Confidence Intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading">#</a></h1>
<p>We have seen in previous chapters how to estimate the parameters of a
p.d.f. fitting them from data (“point estimation”) and how to get their
uncertainties as the covariance matrix. If the estimator is gaussian
distributed then the uncertainty is just given in terms of the “standard
deviation”.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>A certain manufacturer produces silicon wafers with
thickness of <span class="math notranslate nohighlight">\(500~\mu m \pm 5 \mu m\)</span>. Assuming that the production
process gives gaussian distributed thicknesses, we can read the
uncertainty as a way to communicate that 68% of the wafers will have a
thickness between <span class="math notranslate nohighlight">\(495 \mu m\)</span> and <span class="math notranslate nohighlight">\(505 \mu m\)</span>. If you say that the
thickness of the sensor is between <span class="math notranslate nohighlight">\(495 \mu m\)</span> and <span class="math notranslate nohighlight">\(505 \mu m\)</span> you are
correct 68% of the times: you’re making a 68% confidence level (CL)
statement. The larger the CL you choose (95%, 99%,…), the wider the
interval is going to be(490-510<span class="math notranslate nohighlight">\(\mu m\)</span>, 485-515<span class="math notranslate nohighlight">\(\mu m\)</span>,…).</p>
</div>
<p>In the general case, when the distribution of the estimator is not
gaussian, the statistical uncertainty is reported as <strong>confidence
intervals</strong> at a given <strong>confidence level</strong>.</p>
<p>The choice of the interval you quote is matter of choice:</p>
<ul class="simple">
<li><p>symmetric intervals <span class="math notranslate nohighlight">\(\mu - x_- = \mu + x_+\)</span></p></li>
<li><p>shortest interval <span class="math notranslate nohighlight">\(min_{|x_- - x_+|} (x_-,x_+)\)</span></p></li>
<li><p>central <span class="math notranslate nohighlight">\(\int_{-\infty}^{x_-} P(x)dx = \int_{x_+}^{+\infty} P(x)dx\)</span></p></li>
<li><p>…</p></li>
</ul>
<p>In the gaussian case all the intervals above coincide; for a generic
distribution, that is not necessarily the case (see  <a class="reference internal" href="#fig-asymintervals"><span class="std std-numref">Fig. 39</span></a>)</p>
<figure class="align-center" id="fig-asymintervals">
<a class="reference internal image-reference" href="_images/asymIntervals.png"><img alt="_images/asymIntervals.png" src="_images/asymIntervals.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">Example of intervals on an asymmetric
p.d.f.</span><a class="headerlink" href="#fig-asymintervals" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="confidence-belt-neyman-frequentist-construction">
<h2>Confidence belt - Neyman/Frequentist construction<a class="headerlink" href="#confidence-belt-neyman-frequentist-construction" title="Link to this heading">#</a></h2>
<p>The general way to communicate the statistical uncertainty on a
measurement is to give a confidence interval. In this section we will
describe the frequentist/classical construction given by Neyman in
1937.</p>
<p>Before going to the formal construction let’s take a look at an example
to show a common pitfall.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>The weight of an empty dish is measured to be
<span class="math notranslate nohighlight">\(25.30 \pm 0.14\)</span> g. A sample of powder is placed on the dish, and the
combined weight measured as <span class="math notranslate nohighlight">\(25.50 \pm 0.14\)</span> g. By subtraction and error
propagation the weight of the powder is <span class="math notranslate nohighlight">\(0.20 \pm 0.20\)</span> g. This is a
perfectly sensible results. However, look at what happens to the
probabilities. The naive statement now says that there is a 32% chance
of the weight being more than <span class="math notranslate nohighlight">\(1~\sigma\)</span> away from the mean, which is
evenly split, making a 16% chance that the weight is negative
! [&#64;Barlow]</p>
</div>
<p>The general issue addressed by the construction of the confidence belt
is how to turn the knowledge from a measurement <span class="math notranslate nohighlight">\(x\pm \sigma\)</span> into a
statement about the value X of the random variable.</p>
<p>Suppose you have a set of measurements <span class="math notranslate nohighlight">\(\{x_1,\ldots,x_n\}\)</span> and you use
them to compute the observed value of an estimator
<span class="math notranslate nohighlight">\(\hat{\theta}(x_1,\ldots,x_n) = \hat{\theta}_{obs}\)</span>. Suppose also that
given any “true value” of <span class="math notranslate nohighlight">\(\theta\)</span> you are able to compute the pdf of
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span>: <span class="math notranslate nohighlight">\(g(\hat{\theta},\theta)\)</span>. From the p.d.f.
<span class="math notranslate nohighlight">\(g(\hat{\theta},\theta)\)</span> (see <a class="reference internal" href="#fig-pdf"><span class="std std-numref">Fig. 40</span></a>) we can determine two values <span class="math notranslate nohighlight">\(u_\alpha\)</span> and
<span class="math notranslate nohighlight">\(v_\beta\)</span> such that there is a probability <span class="math notranslate nohighlight">\(\alpha\)</span> to observe
<span class="math notranslate nohighlight">\(\hat{\theta} \geq u_\alpha\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta} \leq v_\beta\)</span>.</p>
<figure class="align-center" id="fig-pdf">
<a class="reference internal image-reference" href="_images/pdf.png"><img alt="_images/pdf.png" src="_images/pdf.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 40 </span><span class="caption-text">Example of <span class="math notranslate nohighlight">\(g(\hat{\theta},\theta)\)</span>.</span><a class="headerlink" href="#fig-pdf" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The values of <span class="math notranslate nohighlight">\(u_\alpha\)</span>, <span class="math notranslate nohighlight">\(v_\beta\)</span> depends on the true value of
<span class="math notranslate nohighlight">\(\theta\)</span> and, because we know the p.d.f. <span class="math notranslate nohighlight">\(g(\hat{\theta},\theta)\)</span>, we
can compute them inverting: $<span class="math notranslate nohighlight">\(\label{eq:alpha}
\alpha = P(\hat{\theta} \geq u_\alpha(\theta)) = \int_{u_\alpha(\theta)}^\infty g(\hat{\theta},\theta) d\hat{\theta}\)</span>$</p>
<div class="math notranslate nohighlight">
\[\beta = P(\hat{\theta} \leq v_\beta(\theta)) = \int_{-\infty}^{v_\beta(\theta)}g(\hat{\theta},\theta) d\hat{\theta}\]</div>
<p>We can now “build horizontally” as in in <a class="reference internal" href="#fig-belt"><span class="std std-numref">Fig. 41</span></a>.</p>
<figure class="align-center" id="fig-belt">
<a class="reference internal image-reference" href="_images/belt.png"><img alt="_images/belt.png" src="_images/belt.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">Example of how to built “horizontally” a confidence belt.</span><a class="headerlink" href="#fig-belt" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For each value of <span class="math notranslate nohighlight">\(\theta\)</span> on the y-axis you compute the boundaries
<span class="math notranslate nohighlight">\(u_\alpha\)</span>, <span class="math notranslate nohighlight">\(v_\beta\)</span>. By scanning the whole y-axis you obtain two
curves <span class="math notranslate nohighlight">\(u_\alpha(\theta)\)</span> and <span class="math notranslate nohighlight">\(v_\beta(\theta)\)</span>. The region in between
the two curves is called <strong>confidence belt</strong>. By construction, for any
value of <span class="math notranslate nohighlight">\(\theta\)</span> the probability for the estimator to be inside the
belt is</p>
<div class="math notranslate nohighlight">
\[
P(v_\beta(\theta) \leq \hat{\theta} \leq u_\alpha(\theta)) &lt; 1 -\alpha - \beta.
\]</div>
<p>Now we can “read the plot vertically” (see
<a class="reference internal" href="#fig-readthebelt"><span class="std std-numref">Fig. 42</span></a>: take your data and compute the observed
value <span class="math notranslate nohighlight">\(\hat{\theta}_{obs}\)</span>. Now take that value and read off the plot on
the y-axis <span class="math notranslate nohighlight">\(a(\hat{\theta})\)</span> and <span class="math notranslate nohighlight">\(b(\hat{\theta})\)</span>. The interval <span class="math notranslate nohighlight">\([a,b]\)</span>
is the <strong>confidence interval</strong> at a <strong>confidence level</strong>
<span class="math notranslate nohighlight">\(1-\alpha - \beta\)</span>. Note that you’re making a statement about <em>the
interval you choose, not on the value of <span class="math notranslate nohighlight">\(\theta\)</span></em>.</p>
<figure class="align-center" id="fig-readthebelt">
<a class="reference internal image-reference" href="_images/readthebelt.png"><img alt="_images/readthebelt.png" src="_images/readthebelt.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">Example of how to read “vertically” a confidence belt.</span><a class="headerlink" href="#fig-readthebelt" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Formally you need to require that the two functions <span class="math notranslate nohighlight">\(u_\alpha(\theta)\)</span>
and <span class="math notranslate nohighlight">\(v_\beta(\theta)\)</span> are monotonically increasing (which is generally
the case for well behaved estimators) and so you can invert them:
$<span class="math notranslate nohighlight">\(a(\hat{\theta}) = u^{-1}_\alpha(\hat{\theta}) \qquad ; \qquad  b(\hat{\theta}) = v^{-1}_\beta(\hat{\theta})\)</span>$
If that is the case then you can translate the inequalities</p>
<div class="math notranslate nohighlight" id="equation-eq-alpha">
<span class="eqno">(12)<a class="headerlink" href="#equation-eq-alpha" title="Link to this equation">#</a></span>\[\hat{\theta} \geq u_\alpha(\theta) \qquad ; \qquad \hat{\theta} \leq v_\beta(\theta)\]</div>
<p>into</p>
<div class="math notranslate nohighlight" id="equation-eq-beta">
<span class="eqno">(13)<a class="headerlink" href="#equation-eq-beta" title="Link to this equation">#</a></span>\[a(\hat{\theta}) \geq \theta \qquad ; \qquad b(\hat{\theta}) \leq \theta .\]</div>
<p>So the Eq.<a class="reference internal" href="#equation-eq-alpha">(12)</a> and Eq.<a class="reference internal" href="#equation-eq-beta">(13)</a> become</p>
<div class="math notranslate nohighlight">
\[
P(a(\hat{\theta}) \geq \theta) = \alpha \qquad ; \qquad P(b(\hat{\theta})\leq \theta) = \beta
\]</div>
<p>which proves that <span class="math notranslate nohighlight">\(P(a(\hat{\theta}) \leq \theta \leq b(\hat{\theta})) = 1-\alpha-\beta\)</span>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Here and in the following, we will use the convention used by
Cowan also keeping the term “significance level” for the goodness of
fit and using “confidence level” for the coverage probability of a
confidence interval.</p>
</aside>
<p>The confidence level is also called <strong>coverage probability</strong>
because, from a frequentist point of view, if the experiment were to be
repeated many times, the interval <span class="math notranslate nohighlight">\([a,b]\)</span> would cover the true (unknown)
value <span class="math notranslate nohighlight">\(\theta_{true}\)</span>, <span class="math notranslate nohighlight">\(1-\alpha-\beta\)</span> of the times.</p>
<p>The confidence interval <span class="math notranslate nohighlight">\([a,b]\)</span> is typically expressed by reporting the
result of a measurement as <span class="math notranslate nohighlight">\(\hat{\theta}_{-c}^{+d}\)</span> where
<span class="math notranslate nohighlight">\(c = \hat{\theta}-a\)</span> and <span class="math notranslate nohighlight">\(d = b-\hat{\theta}\)</span>. These values for a
central confidence level of 68% are also the one used to represent
graphically the error bars.</p>
<p>The case we have just treated is the so called “two-sided” confidence
interval. There are however cases where we might be interested in giving
only a one sided interval:</p>
<ul class="simple">
<li><p>lower limit <span class="math notranslate nohighlight">\(a \leq \theta\)</span> with coverage probability <span class="math notranslate nohighlight">\(1-\alpha\)</span></p></li>
<li><p>upper limit <span class="math notranslate nohighlight">\(\theta \leq b\)</span> with coverage probability <span class="math notranslate nohighlight">\(1-\beta\)</span></p></li>
</ul>
<p>The value <span class="math notranslate nohighlight">\(a\)</span> is the value of <span class="math notranslate nohighlight">\(\theta\)</span> for which a fraction <span class="math notranslate nohighlight">\(\alpha\)</span> of
the measurements would be higher than the observed one (and similarly
for <span class="math notranslate nohighlight">\(b\)</span>). To get the value of <span class="math notranslate nohighlight">\(a\)</span> (<span class="math notranslate nohighlight">\(b\)</span>) we have to solve (typically
numerically) the equation for a (<span class="math notranslate nohighlight">\(b\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\alpha  = \int_{\hat{\theta}_{obs}}^\infty g(\hat{\theta},a) d\hat{\theta}
\]</div>
<div class="math notranslate nohighlight">
\[
\beta  = \int_{-\infty}^{\hat{\theta}_{obs}}g(\hat{\theta},b) d\hat{\theta}
\]</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Let’s take as an example a gaussian distributed estimator
with mean <span class="math notranslate nohighlight">\(\theta\)</span> (true value unknown) and standard deviation
<span class="math notranslate nohighlight">\(\sigma_{\hat{\theta}}\)</span> (known; if unknown you can estimate it from data
getting <span class="math notranslate nohighlight">\(\hat{\sigma}_\theta\)</span> and move from the gaussian to the
Student’s <span class="math notranslate nohighlight">\(t-\)</span>distribution). The gaussian estimator is very common
thanks to the central limit theorem (where any estimator that is a sum
of random variables is approximately gaussian in the large sample
limit). The integrals in
Eq.<a class="reference internal" href="#equation-eq-alpha">(12)</a> and Eq.<a class="reference internal" href="#equation-eq-beta">(13)</a>  
become the well known cumulative functions of the gaussian:</p>
<div class="math notranslate nohighlight">
\[
G(\hat{\theta},\theta,\sigma_{\hat{\theta}}) = \int_{-\infty}^{\hat{\theta}} \frac{1}{\sqrt{2 \pi \sigma_{\hat{\theta}}^2}} exp\left( \frac{-(\hat{\theta}' - \theta)^2}{2\sigma_{\hat{\theta}^2}} \right) d\hat{\theta}'
\]</div>
<p>and the general case exposed above, simplifies considerably. We need to
solve for <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> the equations:</p>
<div class="math notranslate nohighlight">
\[
\alpha = 1-G(\hat{\theta}_{obs},a,\sigma_{\hat{\theta}}) = 1 - \Phi\left( \frac{\hat{\theta}_{obs} - a}{\sigma_{\hat{\theta}}}\right)
\]</div>
<div class="math notranslate nohighlight">
\[
\beta = G(\hat{\theta}_{obs},b,\sigma_{\hat{\theta}}) = \Phi\left( \frac{\hat{\theta}_{obs} - b}{\sigma_{\hat{\theta}}}\right)
\]</div>
<p>that can be easily done by using the quantile of the gaussian
<span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> (the inverse function of <span class="math notranslate nohighlight">\(\Phi\)</span>):</p>
<div class="math notranslate nohighlight">
\[
a = \hat{\theta}_{obs} - \sigma_{\hat{\theta}} \Phi^{-1}(1-\alpha)
\]</div>
<div class="math notranslate nohighlight">
\[
b = \hat{\theta}_{obs} + \sigma_{\hat{\theta}} \Phi^{-1}(1-\beta)
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\Phi^{-1}(\beta) = -\Phi^{-1}(1-\beta)\)</span>. Thus, the
general curve for <span class="math notranslate nohighlight">\(u_\alpha(\theta)\)</span> and <span class="math notranslate nohighlight">\(v_\beta(\theta)\)</span> become linear
function.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Another very frequent case is when the estimator is a
Poisson variable <span class="math notranslate nohighlight">\(n\)</span>. Suppose you have performed a measurement and you
found <span class="math notranslate nohighlight">\(\hat{\nu}_{obs} = n_{obs}\)</span> and from this you want to build the
confidence interval for the mean <span class="math notranslate nohighlight">\(\nu\)</span> (see
<a class="reference internal" href="#fig-poisson9evts"><span class="std std-numref">Fig. 43</span></a>. The procedure is the same as for the
general case shown above, but here we are dealing with a discrete
variable. This means that in general, once <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are
fixed, because <span class="math notranslate nohighlight">\(\hat{\nu}\)</span> can only take discrete values, the
Eq.<a class="reference internal" href="#equation-eq-alpha">(12)</a>, Eq.<a class="reference internal" href="#equation-eq-beta">(13)</a> only holds for particular values of <span class="math notranslate nohighlight">\(\nu\)</span> (and in
general because of rounding we will give conservative intervals). Using
the Poisson p.d.f. we have these equations to solve numerically:</p>
<div class="math notranslate nohighlight">
\[
\alpha = \sum_{n=n_{obs}}^\infty f(n;a) = 1 - \sum_{n=0}^{n_{obs}-1}f(n;a) = 1 - \sum_{n=0}^{n_{obs}-1}\frac{a^n}{n!}e^{-a}
\]</div>
<div class="math notranslate nohighlight">
\[
\beta  = \sum_{n=0}^{n=n_{obs}}f(n;b) = \sum_{n=0}^{n_{obs}} \frac{b^n}{n!}e^{-b}
\]</div>
</div>
<figure class="align-center" id="fig-poisson9evts">
<a class="reference internal image-reference" href="_images/poisson9evts.png"><img alt="_images/poisson9evts.png" src="_images/poisson9evts.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">Here
we observed 9 radioactive decays and we constructed the 90% symmetric CL
<span class="math notranslate nohighlight">\(\lambda \in [\lambda_{low}-9, \lambda_{up}-9]\)</span>, i.e.
<span class="math notranslate nohighlight">\(\lambda = 9^{+6.7}_{-4.3}.\)</span></span><a class="headerlink" href="#fig-poisson9evts" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Upper limit when <span class="math notranslate nohighlight">\(n_{obs} = 0\)</span>:</strong> Again from the formulas in the
previous example, you can compute what is the upper limit on the
frequency of a rare event in case you don’t observe any. At
<span class="math notranslate nohighlight">\(1-\beta = 95\)</span>% CL you get <span class="math notranslate nohighlight">\(b=2.996\)</span>, typically rounded to 3. This is
why anytime you see a paper with zero observed events the quoted upper
limit is 3.</p>
<p><strong>Error bars on empty bins:</strong> From the formulas in the example above we
can see that the upper limit <span class="math notranslate nohighlight">\(b\)</span> when <span class="math notranslate nohighlight">\(n_{obs} = 0\)</span> becomes
<span class="math notranslate nohighlight">\(\beta=e^{-b}\)</span>. If we set the central confidence level at
<span class="math notranslate nohighlight">\(1-\beta=0.6827\)</span> we find <span class="math notranslate nohighlight">\(b=-\log(0.3173/2) \sim 1.8\)</span>. This is the
reason of the error bar at 1.8 when we observe zero events in a counting
experiment. (In FIXME <code class="docutils literal notranslate"><span class="pre">ROOT</span></code> you can get the correct Poisson error bars using
the method <code class="docutils literal notranslate"><span class="pre">TH1::SetBinErrorOption(TH1::kPoisson)</span></code>).</p>
<p>When the p.d.f. of the estimator is not a gaussian, and approximating it
to a gaussian would lead to biases or under/over coverage of the
confidence interval, one can always extract the p.d.f. of the estimator
tossing toy experiments and compute the confidence belt numerically. In
some specific cases one can also try to transform the estimator such
that the transformed variable is gaussian distributed.</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Consider, as an example for which the p.d.f. of the
estimator is not gaussian, the correlation coefficient <span class="math notranslate nohighlight">\(\rho\)</span> of a
two-dimensional gaussian and the estimator <span class="math notranslate nohighlight">\(r\)</span> as</p>
<div class="math notranslate nohighlight">
\[
r=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\left( \sum_{j=1}^n(x_j-\bar{x})^2\cdot\sum_{k=1}^{n}(y_k-\bar{y})  \right)^{1/2}}
\]</div>
<p>The distribution for this estimator is shown in
<a class="reference internal" href="#fig-correst"><span class="std std-numref">Fig. 44</span></a> and it only approaches a gaussian distribution in the large sample
limit.</p>
<p>For this specific case, Fisher showed that the transformed estimator</p>
<div class="math notranslate nohighlight">
\[
z=\tanh^{-1} r = \frac{1}{2} \log\frac{1+r}{1-r}
\]</div>
<p>reaches the gaussian limit much more quickly. You can use this transformation for an
estimator <span class="math notranslate nohighlight">\(\zeta\)</span></p>
<div class="math notranslate nohighlight">
\[
\zeta = \tanh^{-1} \rho = \frac{1}{2} \log\frac{1+\rho}{1-\rho}
\]</div>
<p>The expectation value and the variance for <span class="math notranslate nohighlight">\(z\)</span> are approximately given by:</p>
<div class="math notranslate nohighlight">
\[
&lt;z&gt; = \frac{1}{2}\log\frac{1+\rho}{1-\rho}   + \frac{\rho}{2(n-1)} \qquad ; \qquad V[z] =  \frac{1}{n-3}
\]</div>
<p>Assuming that the sample is large enough such that you can neglect the
bias <span class="math notranslate nohighlight">\(\frac{\rho}{2(n-1)}\)</span>, you can use these to determine the
confidence interval <span class="math notranslate nohighlight">\([a,b] = [z-\hat{\sigma}_z,z+\hat{\sigma}_z]\)</span> such
that the lower limit <span class="math notranslate nohighlight">\(a\)</span> for <span class="math notranslate nohighlight">\(\zeta\)</span> is at confidence level <span class="math notranslate nohighlight">\(1-\alpha\)</span>
and the upper limit <span class="math notranslate nohighlight">\(b\)</span> is at <span class="math notranslate nohighlight">\(1-\beta\)</span> confidence level. From <span class="math notranslate nohighlight">\([a,b]\)</span>
on <span class="math notranslate nohighlight">\(\zeta\)</span> you can go back to the interval <span class="math notranslate nohighlight">\([A,B]\)</span> on <span class="math notranslate nohighlight">\(\rho\)</span> simply
inverting <span class="math notranslate nohighlight">\(\zeta = \tanh^{-1} \rho\)</span>
(<span class="math notranslate nohighlight">\([A = \tanh \alpha, B = \tanh\beta]\)</span>).</p>
</div>
<figure class="align-center" id="fig-correst">
<a class="reference internal image-reference" href="_images/corrEst.png"><img alt="_images/corrEst.png" src="_images/corrEst.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Distribution of the
correlation estimator for a sample of size n=20 and different values of
<span class="math notranslate nohighlight">\(\rho\)</span>.</span><a class="headerlink" href="#fig-correst" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="use-the-likelihood-or-the-chi-2-to-set-confidence-intervals">
<h2>Use the likelihood or the <span class="math notranslate nohighlight">\(\chi^2\)</span> to set confidence intervals<a class="headerlink" href="#use-the-likelihood-or-the-chi-2-to-set-confidence-intervals" title="Link to this heading">#</a></h2>
<p>In the large sample limit approximation it is easy to extract confidence
intervals using the likelihood method or equivalently the <span class="math notranslate nohighlight">\(\chi^2\)</span>
method (<span class="math notranslate nohighlight">\(L=\exp(-\chi^2/2)\)</span>).</p>
<p>In the gaussian case, as we have already seen when estimating the
uncertainty on the likelihood estimator, we can get the estimated
<span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\theta}}\)</span> for <span class="math notranslate nohighlight">\(\sigma_{\hat{\theta}}\)</span> from:</p>
<div class="math notranslate nohighlight">
\[
\log ~L(\hat{\theta} \pm N \sigma_{\hat{\theta}}) = \log L_{max} - \frac{N^2}{2}
\]</div>
<p>and again as we have already seen, this amounts to move by 0.5 from the
maximum value. Even if the likelihood is not gaussian the central
confidence interval <span class="math notranslate nohighlight">\([a,b] = [\hat{\theta}-c, \hat{\theta}+d]\)</span> can still
be approximated by:</p>
<div class="math notranslate nohighlight">
\[
{\color{blue}{LIKELIHOOD}}~:~\log ~L(\hat{\theta}^{+d}_{-c}) = \log L_{max} - \frac{N^2}{2} \qquad \; \qquad {\color{blue}{\chi^2}}~:~\chi^2(\hat{\theta}^{+d}_{-c}) = \chi^2_{min} + N^2
\]</div>
<p>for the likelihood and the <span class="math notranslate nohighlight">\(\chi^2\)</span> methods respectively, where
<span class="math notranslate nohighlight">\(N=\Phi^{-1}(1-\gamma/2)\)</span> is the quantile of the standard gaussian
distribution corresponding to the desired confidence level. The <span class="math notranslate nohighlight">\(\chi^2\)</span>
prescription is just the likelihood one where we use <span class="math notranslate nohighlight">\(\log L=-\chi^2/2\)</span>.
This is by far the most used method to report the statistical
uncertainties, but it has to be remembered that it exact <em>only</em> for
gaussian likelihood or in the large sample limit and for all other cases
it is only an approximation !</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Consider the lifetime measurement of an unstable particle.
We can extract the value of the lifetime by fitting the exponential
distribution of the proper decay time of a large number of particles.</p>
<p>The ML estimator comes out to
be just the average <span class="math notranslate nohighlight">\(\hat{\tau}=\frac{1}{n}\sum_{i=1}^{n}t_i\)</span>. For a
small statistics as in <a class="reference internal" href="#fig-lifetime"><span class="std std-numref">Fig. 45</span></a>, where we only have 5 measurements, the
likelihood is non parabolic and it is preferable to report the
asymmetric values given by <span class="math notranslate nohighlight">\(\log L_{max} - \frac{1}{2}\)</span>: i.e.
<span class="math notranslate nohighlight">\([\hat{\tau} - \Delta\hat{\tau}_-, \hat{\tau} + \Delta\hat{\tau}_+]\)</span>.</p>
</div>
<figure class="align-center" id="fig-lifetime">
<a class="reference internal image-reference" href="_images/lifetime.png"><img alt="_images/lifetime.png" src="_images/lifetime.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Non-parabolic log-Likelihood scan of a lifetime measurement done on a small statistics
sample.</span><a class="headerlink" href="#fig-lifetime" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The same method can be applied to find confidence intervals in several
dimensions (a.k.a. confidence regions). What in one dimension was an
interval in n-dimensions becomes a region bounded by an hyper-ellipsoid
defined by constant values of <span class="math notranslate nohighlight">\(Q(\vec{\hat{\theta}},\vec{\theta})\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\vec{\hat{\theta}}\)</span> is described by a n-dimensional gausssian p.d.f.
<span class="math notranslate nohighlight">\(g(\vec{\hat{\theta}}, \vec{\theta})\)</span>
,then <span class="math notranslate nohighlight">\(Q(\vec{\hat{\theta}}, \vec{\theta})\)</span> is distributed according to
a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with n degrees of freedom. We can then compute
the probability to have the estimate <span class="math notranslate nohighlight">\(\vec{\hat{\theta}}\)</span> near the true
value (and viceversa) as</p>
<div class="math notranslate nohighlight">
\[
P(Q(\vec{\theta},\vec{\hat{\theta}}) &lt; Q_\gamma) = \int_0^{Q_\gamma} f(z;n)dz = 1-\gamma
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(z;n)\)</span> is the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with n degrees of freedom.
Inverting using the quantile of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution we get:</p>
<div class="math notranslate nohighlight">
\[
Q_\gamma = F^{-1}(1-\gamma;n)
\]</div>
<p>The region defined by the
<span class="math notranslate nohighlight">\(Q(\vec{\theta},\vec{\hat{\theta}}) &lt; Q_\gamma\)</span> is called <em>confidence
region</em> with confidence level <span class="math notranslate nohighlight">\(1-\gamma\)</span>. In the case of a gaussian
likelihood, the confidence region can be constructed by finding the
region of the space such that:</p>
<div class="math notranslate nohighlight">
\[
\log L(\vec{\theta}) = \log L_{max} - \frac{Q_\gamma}{2}
\]</div>
<p>Typical values of the quantiles of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution are listed in
<a class="reference internal" href="#fig-quantiles"><span class="std std-numref">Fig. 46</span></a>. Note that for increasing number of
dimensions the confidence level, at a fixed quantile <span class="math notranslate nohighlight">\(Q_\gamma\)</span>
decreases.</p>
<figure class="align-center" id="fig-quantiles">
<a class="reference internal image-reference" href="_images/quantiles.png"><img alt="_images/quantiles.png" src="_images/quantiles.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 46 </span><span class="caption-text">Quantiles and confidence levels in n-Dimensions.</span><a class="headerlink" href="#fig-quantiles" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="limits-near-boundaries">
<h2>Limits near boundaries<a class="headerlink" href="#limits-near-boundaries" title="Link to this heading">#</a></h2>
<p>When performing searches, very often we are faced with the problem of
looking for a new effect at the boundaries of the phase space. Suppose
we want to measure the neutrino mass; because of the oscillation
phenomenon we know neutrinos are not (all of them) massless. Still their
mass is very small, i.e. near the boundary m = 0.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Deciding how to quote the result of a measurement after seeing the
results is called “flip-flop” and will be addressed using the
Feldman-Cousins construction in Sec. <a class="reference internal" href="#confidenceIntervals.html#the-flip-flop-problem"><span class="xref myst">Flip-Flop</span></a>.</p>
</aside>
<p>Depending on the resolution of the measurement, we typically face two
situations:</p>
<ul class="simple">
<li><p>the data yields a value significantly different from zero; we will
then quote the measured value with an uncertainty</p></li>
<li><p>the data yields a value compatible with zero; we will then proceed
to quote an upper limit on the value of the parameter at a given CL.</p></li>
</ul>
<p>The problem gets trickier when the estimator takes values in a
physically forbiddend region. To get the idea: suppose you measure a
mass as <span class="math notranslate nohighlight">\(m^2 = E^2 - p^2\)</span>. Depending on the uncertainties on energy and
momentum you can obtain negative values for the mass. From the purely
statistical point of view you can proceed as before and set the upper limit
<span class="math notranslate nohighlight">\(\theta_{up}\)</span> at <span class="math notranslate nohighlight">\(1-\beta\)</span> CL is given by:</p>
<div class="math notranslate nohighlight">
\[
\theta_{up} = \hat{\theta}_{obs} + \sigma_{\hat{\theta}}\Phi^{-1} (1-\beta).
\]</div>
<p>The interval <span class="math notranslate nohighlight">\((-\infty, \theta_{up})\)</span> will contain the true value of
<span class="math notranslate nohighlight">\(\theta\)</span> with a probability of 95%.</p>
<p>Now, take as an example a measurement giving <span class="math notranslate nohighlight">\(\theta_{obs} = -2.0\)</span> with
a standard deviation <span class="math notranslate nohighlight">\(\hat{\sigma}=1\)</span>: the upper limit is
<span class="math notranslate nohighlight">\(\theta_{up} = -0.355\)</span> at 95% CL. We are setting a negative upper limit
on a mass! Having a negative upper limit has to happen 5% of the times
when using a 95%CL. We just got one of the 5%. The annoying point is
that, because we knew from the beginning that the mass has to be
positive, the experiment does not bring any new piece of information. In
such a case we should still quote the value we found such that, when
combined with more experiments, we will contribute to the measurement of
the correct value. If you encounter a very asymmetric likelihood, you
should in general publish the scan of the likelihood on the parameter
you are estimating.</p>
<p>A naive approach to avoid the negative upper limit is to shift the
negative estimate to the boundary (<span class="math notranslate nohighlight">\(\hat{\theta}=0\)</span> in this case):</p>
<div class="math notranslate nohighlight">
\[
\theta_{up}=max(\hat{\theta}_{obs},0) + \sigma_{\hat{\theta}}\Phi^{-1} (1-\beta).
\]</div>
<p>In this case, if the limit is positive you will quote the same value
given by the classical construction, while for negative values you will
end up with an unwanted coverage larger than the quoted <span class="math notranslate nohighlight">\(1-\beta\)</span>.</p>
<p>The most natural way to include boundaries in the computation of limits
is to use the Bayes theorem:</p>
<div class="math notranslate nohighlight">
\[
p(\theta|x) = \frac{L(x|\theta)\pi(\theta)}{\int L(x|\theta')\pi(\theta')d\theta'}
\]</div>
<p>where <span class="math notranslate nohighlight">\(L(x|\theta)\)</span> is the likelihood to observe a value <span class="math notranslate nohighlight">\(x\)</span> given the
parameter <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> is the prior on <span class="math notranslate nohighlight">\(\theta\)</span>. The most
probable value of the posterior <span class="math notranslate nohighlight">\(p(\theta|x)\)</span> coincides with the ML
estimator if the prior is flat (the denominator only contributes as a
normalization constant). To set limits (without considering boundaries
for the moment), we can use the expression for the posterior and find
numerically the interval <span class="math notranslate nohighlight">\([a,b]\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
\alpha = \int_{-\infty}^{a}p(\theta|x)d\theta \qquad \beta =\int_{b}^{+\infty}p(\theta|x)d\theta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> define the CL.</p>
<p>Using the Bayes theorem, the inclusion of boundaries is trivial: we just
need to write them in the prior. In the example of a mass limited to
positive values, we can write the prior to be zero for negative masses
and flat above:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi(\theta) = \left\{
            \begin{array}{rll}
                0 &amp; \mbox{if} &amp; \theta \le 0 \\
                1 &amp; \mbox{if} &amp; \theta &gt;  0
            \end{array}\right.
\end{split}\]</div>
<p>The upper limit on the mass will be
found as before by solving numerically for <span class="math notranslate nohighlight">\(\theta_{up}\)</span> the integral on
the posterior:</p>
<div class="math notranslate nohighlight">
\[
1-\beta = \int_{-\infty}^{\theta_{up}} p(\theta|x) = \frac{\int_{-\infty}^{\theta_{up}}L(x|\theta)\pi(\theta)d\theta }{\int_{-\infty}^{+\infty}L(x|\theta)\pi(\theta)d\theta }{}
\]</div>
<p>This method is clearly affected by the issues on the encoding of
ignorance in the prior (i.e. the choice of a flat prior):</p>
<ul class="simple">
<li><p>using a flat prior assumes that the probability for the parameter
<span class="math notranslate nohighlight">\(\theta\)</span> is the same everywhere in the physically allowed phase
space; applied to the neutrino mass example, this would mean that
the probability is the same in the range <span class="math notranslate nohighlight">\([0,1]\)</span>eV as in
<span class="math notranslate nohighlight">\([10^{10}, 10^{10}+1]\)</span>eV.</p></li>
<li><p>a flat prior won’t remain flat under a non-trivial transformation of
the parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
<p>With the Bayes approach, you don’t need anymore to go for the “build
horizontally / read vertically” classical construction. You just need to
compute the integrals above. For the example of the neutrino mass: the
denominator is an integral on the full phase space <span class="math notranslate nohighlight">\((0,+\infty)\)</span>, while
the numerator runs on <span class="math notranslate nohighlight">\((0, \theta_{up})\)</span>. For large values of
<span class="math notranslate nohighlight">\(x = \theta_{obs}\)</span> the bayesian limit approaches the classical one: the
likelihood you are integrating is far from the boundary and the tail
below zero is negligible. The closer you get to the boundary the larger
is the area of the likelihood leaking in the unphysical region. Still,
the upper limit will never cross zero because it is extracted from the
ratio of two integrals where the numerator is by construction always a
fraction of the denominator.</p>
<p><a class="reference internal" href="#fig-comparison"><span class="std std-numref">Fig. 47</span></a> shows the comparison of the different
methods discussed so far: the classical construction; the “shifted”
classical construction where the upper limit if forced to be flat for
<span class="math notranslate nohighlight">\(\hat{\theta}&lt;0\)</span>; and the bayesian approach where the upper limit on
<span class="math notranslate nohighlight">\(\theta\)</span> never falls below zero.</p>
<figure class="align-center" id="fig-comparison">
<a class="reference internal image-reference" href="_images/comparison.png"><img alt="_images/comparison.png" src="_images/comparison.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 47 </span><span class="caption-text">Comparison of the upper limits obtained with the classical construction, the
“shifted” classical construction and the bayesian approach.</span><a class="headerlink" href="#fig-comparison" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Another frequent case is when we try to set an upper limit on a rare
signal with a counting experiment. Here the boundary is given by the
number of events that has to be greater or equal than zero. Let’s start
from the simpler situation where we don’t set any constraint (or
equivalently we are far away from the boundary) and compute the upper
limit following the classical construction. We observe <span class="math notranslate nohighlight">\(n\)</span> events some
of which come from signal <span class="math notranslate nohighlight">\(n_s\)</span> and some others from background <span class="math notranslate nohighlight">\(n_b\)</span>:
<span class="math notranslate nohighlight">\(n = n_s+n_b\)</span>. The number of events follow a Poisson distribution with
an expected number of events <span class="math notranslate nohighlight">\(\nu_s\)</span> for signal and <span class="math notranslate nohighlight">\(\nu_b\)</span> for
background (suppose we know the expected number of background events
with negligible uncertainty). We want to get the upper limit on the
number of signal events in the sample. The ML estimator for the number
of signal events is simply <span class="math notranslate nohighlight">\(\hat{\nu}_s = n - \nu_b\)</span>. The confidence
interval can be extracted as usual by solving (numerically) the
equation:</p>
<div class="math notranslate nohighlight">
\[
\beta = P(\hat{\nu}_s\leq\hat{\nu}_s^{obs}| \nu^{up}) = \sum_{n\leq \hat{\nu}_{obs}}\frac{(\nu_s^{up} + \nu_b)^n e^{(\nu_s^{up} + \nu_b)}}{n!}
\]</div>
<p>The results for some values of observed events and expected backgrounds
are shown in <a class="reference internal" href="#fig-poissonlim"><span class="std std-numref">Fig. 48</span></a>.</p>
<figure class="align-center" id="fig-poissonlim">
<a class="reference internal image-reference" href="_images/poissonLim.png"><img alt="_images/poissonLim.png" src="_images/poissonLim.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 48 </span><span class="caption-text">Upper
limit on the number of signal events as a function of the expected
number of background events <span class="math notranslate nohighlight">\(\nu_b\)</span> for different observed <span class="math notranslate nohighlight">\(n\)</span>.</span><a class="headerlink" href="#fig-poissonlim" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Before moving to the Bayesian solution to setting the upper limit, let’s
see where the classical one ends into troubles. Suppose you have a small
number of expected signal events and large background. In this situation
the number of observed events will be dominated by the background and we
can find cases where the number of observed events is smaller than the
expected number of background events (e.g. <span class="math notranslate nohighlight">\(\nu_b\)</span> = 8, <span class="math notranslate nohighlight">\(n\)</span> = 1). By
reading off the limit from
<a class="reference internal" href="#fig-poissonlim"><span class="std std-numref">Fig. 48</span></a> we find a negative number of signal events.
As we have already seen for the example of the neutrino mass measurement
this is not wrong. We are setting an upper limit at 95% CL and we got
one large downward fluctuations which should happen in 5% of the cases.
Nevertheless from a physics content the result is not particularly
interesting (we knew before setting up the experiment that the number of
events has to be greater than or equal to zero).</p>
<p>Using the bayesian approach the posterior can be written as:</p>
<div class="math notranslate nohighlight">
\[
p(\nu_s|n_{obs}) = \frac{L(n_{obs}|\nu_s)\pi(\nu_s)}{\int L(n_{obs}|\nu_s')\pi(\nu_s')d\nu_s'}
\]</div>
<p>To include the boundary <span class="math notranslate nohighlight">\(\nu_s&gt;0\)</span> we can simply define the prior as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi(\nu_s) = \left\{
            \begin{array}{rll}
                0 &amp; \mbox{if} &amp; \nu_s \le 0 \\
                1 &amp; \mbox{if} &amp; \nu_s &gt;  0
            \end{array}\right.
\end{split}\]</div>
<p>To compute the upper limit we just need to plug this prior in the posterior expression
$<span class="math notranslate nohighlight">\(
1-\beta = \frac{\int_{0}^{\nu_s^{up}}L(n_{obs}|\nu_s)d\nu_s }{\int_{0}^{+\infty}L(n_{obs}|\nu_s)d\nu_s}
\)</span>$</p>
<p>and solve numerically for <span class="math notranslate nohighlight">\(\nu_s^{up}\)</span>. The equivalent of
<a class="reference internal" href="#fig-poissonlim"><span class="std std-numref">Fig. 48</span></a> using a bayesian approach is shown in
<a class="reference internal" href="#fig-poissonlimbayes"><span class="std std-numref">Fig. 49</span></a>.</p>
<figure class="align-center" id="fig-poissonlimbayes">
<a class="reference internal image-reference" href="_images/poissonLimBayes.png"><img alt="_images/poissonLimBayes.png" src="_images/poissonLimBayes.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 49 </span><span class="caption-text">Bayesian upper limit on the number of signal
events as a function of the expected number of background events <span class="math notranslate nohighlight">\(\nu_b\)</span>
for different observed <span class="math notranslate nohighlight">\(n\)</span>.</span><a class="headerlink" href="#fig-poissonlimbayes" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The bayesian limit is always greater or equal than the classical one. As
expected the bayesian approaches the classical limit when we move to the
region where the number of observed events is much larger than the
number of expected background events. The fact that the two limits
precisely coincide for <span class="math notranslate nohighlight">\(n_{bkg}=0\)</span> is a pure coincidence since the
Bayesian limit depends on the particular choice of the prior.</p>
</section>
<section id="feldman-cousins">
<h2>Feldman-Cousins<a class="headerlink" href="#feldman-cousins" title="Link to this heading">#</a></h2>
<p>The Feldman-Cousins (FC) prescription of the confidence intervals solves
two complications related to the classical Neyman construction: the so
called “flip-flop” problem and the problem of obtaining unphysical (or
empty set) interval using the Neyman construction. Their solution
provides, in a purely frequentist manner, intervals which are never
unphysical, nor empty and unifies the set of classical confidence
intervals for setting upper limits and quoting two-sided confidence
intervals. However, it still suffers from an apparent paradox when the
number of observed events is lower than the background only expectations
(background underfluctuations).</p>
<p>*In this section we will follow closely the paper in Ref. <span id="id1">[<a class="reference internal" href="bibliography.html#id15" title="Gary J. Feldman and Robert D. Cousins. Unified approach to the classical statistical analysis of small signals. Physical Review D, 57(7):3873–3889, apr 1998. URL: https://doi.org/10.1103%2Fphysrevd.57.3873, doi:10.1103/physrevd.57.3873.">FC98</a>]</span>.</p>
<section id="the-flip-flop-problem">
<h3>The flip-flop problem<a class="headerlink" href="#the-flip-flop-problem" title="Link to this heading">#</a></h3>
<p>The flip-flop problem arises when one decides, <em>based on the results of
the experiment</em>, whether to publish an upper limit or a central
confidence interval. This is a very common (and sensible) attitude.
Typically the reasoning goes as: if I see that a result is below
3<span class="math notranslate nohighlight">\(\sigma\)</span> I publish an upper limit, if the result has a significance of
more than 3<span class="math notranslate nohighlight">\(\sigma\)</span> then I publish a central confidence interval. The
typical sentence in papers is: “the data are compatible with the
background expectations (or no excess is observed above the expected
background) <em>hence</em> we set an upper limit on…”. As seen in the
previous section, one can even decide that in case of a boundary, e.g. a
mass which has to be positive, to use the maximum between the measured
value and zero <span class="math notranslate nohighlight">\(\max(m,0)\)</span> and quote the CL corresponding to zero. These
choices has an impact on the coverage of the reported confidence
intervals.</p>
<p>To better understand the flip-flop problem we start by looking at the
confidence belt at 90% CL of <a class="reference internal" href="#fig-fc1"><span class="std std-numref">Fig. 50</span></a>(left). Here we are considering a gaussian
observable with <span class="math notranslate nohighlight">\(\sigma=1\)</span>; the <span class="math notranslate nohighlight">\(x\)</span>-axis is the measured mean <span class="math notranslate nohighlight">\(x\)</span>, while
the <span class="math notranslate nohighlight">\(y\)</span>-axis is the unknown mean <span class="math notranslate nohighlight">\(\mu\)</span>. By construction the coverage is
90%, i.e. reading the plot vertically for a measured <span class="math notranslate nohighlight">\(x\)</span>, we have
<span class="math notranslate nohighlight">\(P(\mu \in [\mu_1,\mu_2]) =90\%\)</span>. The corresponding plot for the upper
limit at 90%, again using the classical construction, is shown in the
<a class="reference internal" href="#fig-fc1"><span class="std std-numref">Fig. 50</span></a>(right).</p>
<figure class="align-center" id="fig-fc1">
<a class="reference internal image-reference" href="_images/FC1.png"><img alt="_images/FC1.png" src="_images/FC1.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 50 </span><span class="caption-text">(left) 90% CL confidence belt; (right) 90% CM upper limit.</span><a class="headerlink" href="#fig-fc1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>If you decide decide a posteriori (i.e. based on the result of the
measurement) whether to publish a central confidence interval, an upper
limit or the truncated <span class="math notranslate nohighlight">\(\max(x,0)\)</span> limit, you will get a confidence
region such as the one reported in <a class="reference internal" href="#fig-fc2"><span class="std std-numref">Fig. 51</span></a>.</p>
<figure class="align-center" id="fig-fc2">
<a class="reference internal image-reference" href="_images/FC2.png"><img alt="_images/FC2.png" src="_images/FC2.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 51 </span><span class="caption-text">“A posteriori” confidence interval: if
the result x is more than <span class="math notranslate nohighlight">\(3\sigma\)</span> (i.e. x <span class="math notranslate nohighlight">\(&gt;\)</span> 3) publish the central
confidence interval; if the result x is less than <span class="math notranslate nohighlight">\(3\sigma\)</span> (i.e. x <span class="math notranslate nohighlight">\(&lt;\)</span>
3) publish an upper limit; if the results is negative use max(meas,0)
and quote the CL corresponding to 0.</span><a class="headerlink" href="#fig-fc2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-flipflop">
<a class="reference internal image-reference" href="_images/flipflop.png"><img alt="_images/flipflop.png" src="_images/flipflop.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 52 </span><span class="caption-text">The interval doesn’t have the correct coverage.</span><a class="headerlink" href="#fig-flipflop" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This region does not guarantee the 90% coverage. It’s not “built
horizontally” following the classical construction; it’s a patchwork of
confidence intervals. Take for example the case of <span class="math notranslate nohighlight">\(\mu=2\)</span> (see
<a class="reference internal" href="#fig-flipflop"><span class="std std-numref">Fig. 52</span></a>. The interval starts with the lower
limit at <span class="math notranslate nohighlight">\(2-1.28\)</span> on the boundary of the 90% upper limit and ends at
<span class="math notranslate nohighlight">\(2+1.64\)</span> on the <span class="math notranslate nohighlight">\(90\%\)</span> central interval, giving a coverage of only
<span class="math notranslate nohighlight">\(1-0.1-0.05 = 85\%\)</span>. In other cases, like at <span class="math notranslate nohighlight">\(\mu=1\)</span> the region
over-covers as shown in the same figure, the confidence region
overcovers. We will see in the following how the FC prescription allows
to unify these intervals and avoid the problem.</p>
<p>Typically the flip-flop problem only comes if you publish a confidence
interval for a well established signal, but with more data the signal
disappears and you are forced to set a limit. While potentially this
could be a very serious issue, in reality it only happens very rarely
because the experimental sensitivity typically grows with additional
data.</p>
</section>
<section id="poisson-with-background">
<h3>Poisson with background<a class="headerlink" href="#poisson-with-background" title="Link to this heading">#</a></h3>
<p>To understand the FC prescription we will use the example of a counting
experiment. The measured number of events is <span class="math notranslate nohighlight">\(n\)</span>, the number of expected
background events is <span class="math notranslate nohighlight">\(b\)</span> (set to <span class="math notranslate nohighlight">\(b=3\)</span> in this example) and we want to
build the confidence interval for the mean <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(n|\mu) = \frac{(\mu+b)^n}{n!}e^{-(\mu+b)}
\]</div>
<p>Let’s start from the
classical construction of the upper limit (UL) as seen in</p>
<p>Sec.<a class="reference internal" href="#confidenceIntervals.html#confidence-belt-neyman-frequentist-construction"><span class="xref myst">Confidence Intervals</span></a>
(see <a class="reference internal" href="#fig-singledouble"><span class="std std-numref">Fig. 53</span></a>).</p>
<p>We build horizontally the UL curve scanning the <span class="math notranslate nohighlight">\(y\)</span>-axis (the unknown signal mean) <span class="math notranslate nohighlight">\(\mu\)</span> and for each value
of <span class="math notranslate nohighlight">\(\mu\)</span> we find the value of the limit solving numerically for <span class="math notranslate nohighlight">\(\mu\)</span>
the equation <span class="math notranslate nohighlight">\(0.1=\sum_0^{n_{obs-1}}P(n|\mu+b)\)</span>. See
<a class="reference internal" href="#fig-poisson9evts"><span class="std std-numref">Fig. 43</span></a> for a graphical reminder of the procedure.
Following the same procedure we can built the confidence interval for
the central interval at <span class="math notranslate nohighlight">\(CL=90\%\)</span>. In this case the lower and upper
limits are computed solving numerically for <span class="math notranslate nohighlight">\(c\)</span> in
<span class="math notranslate nohighlight">\(0.05=\sum_0^{n_{obs-1}}P(n|c+b)\)</span> and for <span class="math notranslate nohighlight">\(a\)</span> in
<span class="math notranslate nohighlight">\(0.05=\sum_{n_{obs}}^\infty P(n|a+b)\)</span>. When <span class="math notranslate nohighlight">\(n=0\)</span> we get an empty set.
For both the upper and central limits, n = 0 has no solution (in the
example b = 3). This is counter-intuitive: if one measures no events,
clearly the most likely value of <span class="math notranslate nohighlight">\(\mu\)</span> is zero. Why should one rule out
the most likely scenario? Let’s see how this issue is addressed using
the FC prescription.</p>
<figure class="align-center" id="fig-singledouble">
<a class="reference internal image-reference" href="_images/singleDouble.png"><img alt="_images/singleDouble.png" src="_images/singleDouble.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text">90%
CL upper limit (left) and confidence interval (right) following the
classical construction. for the case with expected background
<span class="math notranslate nohighlight">\(b=3\)</span>.</span><a class="headerlink" href="#fig-singledouble" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The FC uses the classical Neyman construction, but using a different
<em>ordering principle</em>. Every time a confidence region is defined, we use
(often without even noticing) the concept of ordering principle. An
ordering principle is needed to specify which values of the measured <span class="math notranslate nohighlight">\(x\)</span>
to include in the acceptance region. When building an upper limit we
start from the smallest values of <span class="math notranslate nohighlight">\(x\)</span>, for a central limit we start from
the value of <span class="math notranslate nohighlight">\(x\)</span> closest to the central value, etc… There are
infinite ordering principles. The FC ordering uses an ordering principle
based on a likelihood ratio.</p>
<p>To understand the idea let’s take the example where we want to build
horizontally the interval for <span class="math notranslate nohighlight">\(\mu=0.5\)</span> (remember we set the number of
background events <span class="math notranslate nohighlight">\(b=3\)</span>). The Poisson probability to obtain <span class="math notranslate nohighlight">\(0\)</span> events,
when the expected number of events is <span class="math notranslate nohighlight">\(\mu = 0.5\)</span> is 0.03. Let’s compare
this low value with the one we obtain using as <span class="math notranslate nohighlight">\(\mu\)</span> the most probable
<span class="math notranslate nohighlight">\(\mu_{best}\)</span>. <span class="math notranslate nohighlight">\(\mu_{best}\)</span> is defined as the value of the mean signal
<span class="math notranslate nohighlight">\(\mu\)</span> which maximizes <span class="math notranslate nohighlight">\(P(n|\mu)\)</span> when we require <span class="math notranslate nohighlight">\(\mu_{best}\)</span> to be
physically allowed (i.e. <span class="math notranslate nohighlight">\(\mu_{best}&gt;0\)</span>). This simply means (remembering
the ML estimator for a signal in presence of background)
<span class="math notranslate nohighlight">\(\mu_{best} = max(0,n-b)\)</span>. The probability to obtain <span class="math notranslate nohighlight">\(0\)</span> events when the
expected number of events is <span class="math notranslate nohighlight">\(\mu_{best} = 0\)</span> is 0.05. So, while
<span class="math notranslate nohighlight">\(P(n,\mu=0.05)=0.03\)</span> is quite low on an absolute scale, it is very much
comparable with <span class="math notranslate nohighlight">\(P(n,\mu_{best}=0)=0.05\)</span> of the most probable value
<span class="math notranslate nohighlight">\(\mu_{best}\)</span>.</p>
<p>Following this observation the ordering principle is defined on the
likelihood ratio:</p>
<div class="math notranslate nohighlight">
\[
R(\mu) = \frac{P(n|\mu)}{P(n|\mu_{best})}
\]</div>
<p>where <span class="math notranslate nohighlight">\(R\)</span> is called <em>rank</em>. The effect of this ordering principle is to
increase the rank of the values with low probability if they are close
to <span class="math notranslate nohighlight">\(\mu_{best}\)</span>. The interval is then built by including values of <span class="math notranslate nohighlight">\(n\)</span>
in decreasing order of <span class="math notranslate nohighlight">\(R\)</span> until the sum of <span class="math notranslate nohighlight">\(P(n|\mu)=1-CL\)</span> i.e. matches
the CL we want. Due to the discreteness of <span class="math notranslate nohighlight">\(n\)</span>, the acceptance region
can contain more summed probability than required by the CL, i.e. we
could end up over-covering. Notice that while in the typical orderings
different values of <span class="math notranslate nohighlight">\(\mu\)</span> do not talk to each other, here <span class="math notranslate nohighlight">\(\mu_{best}\)</span>
influence the “weight” of all the other values (their order).</p>
<p>Let’s compute the interval for the numerical example used above
(expected number of background events is <span class="math notranslate nohighlight">\(b=3\)</span>, we measure <span class="math notranslate nohighlight">\(n\)</span> events
and we want to build the confidence interval for <span class="math notranslate nohighlight">\(\mu=0.5\)</span>).</p>
<figure class="align-center" id="fig-tablefc">
<a class="reference internal image-reference" href="_images/tableFC.png"><img alt="_images/tableFC.png" src="_images/tableFC.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 54 </span><span class="caption-text">Confidence interval construction for <span class="math notranslate nohighlight">\(\mu=0.5\)</span> and background <span class="math notranslate nohighlight">\(b=3\)</span>.</span><a class="headerlink" href="#fig-tablefc" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For each value of <span class="math notranslate nohighlight">\(n\)</span> we first compute <span class="math notranslate nohighlight">\(\mu_{best}\)</span> the best (physical)
value of the estimator given n (see table in <a class="reference internal" href="#fig-tablefc"><span class="std std-numref">Fig. 54</span></a>.
We get <span class="math notranslate nohighlight">\(\mu_{best} = max(0,n-b)\)</span>: <span class="math notranslate nohighlight">\(n=0 \to \mu_{best}=0\)</span>,
<span class="math notranslate nohighlight">\(n=1 \to \mu_{best}=0\)</span>, <span class="math notranslate nohighlight">\(n=2 \to \mu_{best}=0\)</span>, <span class="math notranslate nohighlight">\(n=3 \to \mu_{best}=0\)</span>,
<span class="math notranslate nohighlight">\(n=4 \to \mu_{best}=1\)</span>, <span class="math notranslate nohighlight">\(n=5 \to \mu_{best}=2\)</span>,…. For each <span class="math notranslate nohighlight">\(n\)</span> we
also compute <span class="math notranslate nohighlight">\(P(n|\mu=0.05)\)</span> and <span class="math notranslate nohighlight">\(P(n|\mu_{best})\)</span> and the rank
<span class="math notranslate nohighlight">\(R=P(n|\mu=0.05)/P(n|\mu_{best})\)</span>. Now we can complete the confidence
interval. We add the probabilities <span class="math notranslate nohighlight">\(P(n|\mu=0.05)\)</span> in decreasing order
of the rank <span class="math notranslate nohighlight">\(R\)</span> until we match the desired CL:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
P(4|\mu=0.05) &amp; + P(3|\mu=0.05)  + P(2|\mu=0.05)  + P(5|\mu=0.05)  + P(1|\mu=0.05)  + \\
              &amp; P(0|\mu=0.05)  + P(6|\mu=0.05) = \\
0.189         &amp; + 0.216          + 0.185         + 0.132          + 0.106          + 0.030          + 0.077          = 0.935
\end{aligned}
\end{split}\]</div>
<p>We intentionally stay on the conservative side and add passed 0.9. The
alternative would be to stop at <span class="math notranslate nohighlight">\(R=6\)</span> which is <span class="math notranslate nohighlight">\(0.858 &lt; 0.9\)</span>, hence
under-covering. Repeating the procedure scanning the values of <span class="math notranslate nohighlight">\(\mu\)</span> on
the <span class="math notranslate nohighlight">\(y\)</span>-axis we obtain the complete confidence belt.</p>
<p>The resulting FC confidence interval is compared with the classical one
in <a class="reference internal" href="#fig-compfc"><span class="std std-numref">Fig. 55</span></a>.</p>
<figure class="align-center" id="fig-compfc">
<a class="reference internal image-reference" href="_images/compFC.png"><img alt="_images/compFC.png" src="_images/compFC.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 55 </span><span class="caption-text">Comparison of the FC confidence interval with the classical one.</span><a class="headerlink" href="#fig-compfc" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>At large <span class="math notranslate nohighlight">\(n\)</span> the FC and the classical construction gives approximately
the same confidence interval: we are far away from the physical boundary
and so the background is effectively subtracted without constraint. For
small values of <span class="math notranslate nohighlight">\(n\)</span> , the confidence interval automatically becomes
upper limits on <span class="math notranslate nohighlight">\(\mu\)</span>; i.e. the lower endpoint is 0 for <span class="math notranslate nohighlight">\(n \leq 4\)</span> in
this case. Thus, flip-flopping between the plots in
<a class="reference internal" href="#fig-singledouble"><span class="std std-numref">Fig. 53</span></a> is replaced by one coherent set of
confidence interval, (and no interval is the empty set).</p>
</section>
<section id="gaussian-with-boundary-at-the-origin">
<h3>Gaussian with boundary at the origin<a class="headerlink" href="#gaussian-with-boundary-at-the-origin" title="Link to this heading">#</a></h3>
<p>As another example we can build the FC confidence interval for a
gaussian distribution with a boundary <span class="math notranslate nohighlight">\(\mu&gt;0\)</span> (e.g. the physical boundary imposed on the mass being positive). and compare it with what we have already encountered in the example in Sec.<a class="reference internal" href="#confidenceIntervals.html#confidence-belt-neyman-frequentist-construction"><span class="xref myst">Classical contruction</span></a></p>
<p>Consider as p.d.f. a gaussian with <span class="math notranslate nohighlight">\(\sigma =1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(x|\mu) = \frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2}}
\]</div>
<p>As we did with the Poisson variable we find <span class="math notranslate nohighlight">\(\mu_{best}\)</span> as the value that
maximizes <span class="math notranslate nohighlight">\(P(x|\mu)\)</span> and force it to be positive, i.e. we take
<span class="math notranslate nohighlight">\(\max(0,\mu_{best})\)</span>. The expression for <span class="math notranslate nohighlight">\(P(x|\mu_{best})\)</span> becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x|\mu_{best}) = \left\{
            \begin{array}{rll}
                1/\sqrt{2\pi} &amp; \mbox{if} &amp; x \geq 0 \\
                \exp(-x^2/2)/\sqrt{2\pi} &amp; \mbox{if} &amp; x&lt;0
            \end{array}\right.
\end{split}\]</div>
<p>if <span class="math notranslate nohighlight">\(x&gt;0\)</span> then <span class="math notranslate nohighlight">\(\mu_{best} = x\)</span>, while
if <span class="math notranslate nohighlight">\(x&lt;0\)</span> we have <span class="math notranslate nohighlight">\(\mu_{best} = 0\)</span>  (see <a class="reference internal" href="#fig-xgt0st0"><span class="std std-numref">Fig. 56</span></a>).</p>
<figure class="align-center" id="fig-xgt0st0">
<a class="reference internal image-reference" href="_images/xgt0st0.png"><img alt="_images/xgt0st0.png" src="_images/xgt0st0.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 56 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\mu_{best}\)</span> is the value that maximizes <span class="math notranslate nohighlight">\(P(x|\mu)\)</span>.</span><a class="headerlink" href="#fig-xgt0st0" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The likelihood ratio is then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
R(x) = \frac{P(x|\mu)}{P(x|\mu_{best})} =
             \left\{
             \begin{array}{rll}
                   \exp(-(x-\mu)^2/2) &amp; \mbox{if} &amp; x \geq 0 \\
                   \exp(x\mu-\mu^2/2) &amp; \mbox{if} &amp; x&lt;0
             \end{array}\right.
\end{split}\]</div>
<p>With this we can now add to the
acceptance region values of x ordering them by their rank. If <span class="math notranslate nohighlight">\(x\)</span> is far
from the boundary, the rank is just <span class="math notranslate nohighlight">\(P(x|\mu)\)</span> and we get to the usual
ordering. If instead it is close to the boundary the rank develops a
tail to the left as shown in <a class="reference internal" href="#fig-rank"><span class="std std-numref">Fig. 57</span></a> that will modify the ordering.</p>
<figure class="align-center" id="fig-rank">
<a class="reference internal image-reference" href="_images/rank.png"><img alt="_images/rank.png" src="_images/rank.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 57 </span><span class="caption-text">The rank is asymmetric.</span><a class="headerlink" href="#fig-rank" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For a given value of <span class="math notranslate nohighlight">\(\mu\)</span>, one finds the interval <span class="math notranslate nohighlight">\([x_1,x_2]\)</span> such that
<span class="math notranslate nohighlight">\(\int_{x_1}^{x_2} P(x|\mu)dx = \alpha\)</span> and <span class="math notranslate nohighlight">\(R(x_1) = R(x_2)\)</span> (see
<a class="reference internal" href="#fig-rank2"><span class="std std-numref">Fig. 58</span></a> and <a class="reference internal" href="#fig-rank3"><span class="std std-numref">Fig. 59</span></a>).
The condition that the rank at the two extremes coincides guarantees
that the order we used to pick the values of <span class="math notranslate nohighlight">\(x\)</span> is correct. If the rank
was higher (or lower) at one extreme, it would mean that those values of
<span class="math notranslate nohighlight">\(x\)</span> should have (have not) be used for the interval.</p>
<figure class="align-center" id="fig-rank2">
<a class="reference internal image-reference" href="_images/rank2.png"><img alt="_images/rank2.png" src="_images/rank2.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 58 </span><span class="caption-text">Classical construction in blue, FC in red.</span><a class="headerlink" href="#fig-rank2" title="Link to this image">#</a></p>
<div class="legend">
<figure class="align-center" id="fig-rank3">
<a class="reference internal image-reference" href="_images/rank3.png"><img alt="_images/rank3.png" src="_images/rank3.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 59 </span><span class="caption-text">Classical construction in blue, FC in black.</span><a class="headerlink" href="#fig-rank3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</figcaption>
</figure>
<p>As expected, far away from the boundary the two intervals coincide.
Below <span class="math notranslate nohighlight">\(x=1.28\)</span>, the lower endpoint of the FC confidence intervals is
zero, so that there is automatically a transition from two-sided
confidence intervals to an upper confidence limit given by <span class="math notranslate nohighlight">\(\mu_2\)</span>. The
point of this transition is fixed by the calculation of the acceptance
interval for <span class="math notranslate nohighlight">\(\mu=0\)</span> the solution has <span class="math notranslate nohighlight">\(x_1 =-\infty\)</span>, and so
<span class="math notranslate nohighlight">\(\int_{x_1}^{x_2} P(x|\mu)dx = \alpha\)</span> is satisfied by <span class="math notranslate nohighlight">\(x_2 =1.28\)</span> when
<span class="math notranslate nohighlight">\(\alpha=90\)</span>%.</p>
</section>
<section id="neutrino-oscillations">
<h3>Neutrino oscillations<a class="headerlink" href="#neutrino-oscillations" title="Link to this heading">#</a></h3>
<p>The study of neutrino oscillations was the initial reason to develop
this methodology. The physics needed to follow the argument is rather
simple. Neutrino oscillations originate from the difference between
neutrino mass and flavor eigenstates. Considering only two flavors
<span class="math notranslate nohighlight">\(\nu_e, \mu_\mu\)</span> we can write the flavor eigenstates as a linear
combination of the mass eigenstates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
|\nu_e  \rangle &amp;=&amp;   |\nu_1\rangle \cos \theta + |\nu_2\rangle \sin \theta \\
|\nu_\mu\rangle &amp;=&amp; - |\nu_1\rangle \sin \theta + |\nu_1\rangle \cos \theta \end{aligned}
\end{split}\]</div>
<p>Studying this two-states quantum mechanical system we obtain that the
probability of a <span class="math notranslate nohighlight">\(\nu_\mu\)</span> to oscillate to <span class="math notranslate nohighlight">\(\nu_e\)</span> is :</p>
<div class="math notranslate nohighlight">
\[
P(\nu_\mu \to \nu_e) = \sin^2(2\theta)\sin^2\left(\frac{1.27 \Delta m^2 L}{E}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta m = |m_1^2 - m_2^2|\)</span> in <span class="math notranslate nohighlight">\(eV^2\)</span>, L is the distance in km
between the creation and the detection points and E is the energy of the
neutrino in MeV. The experimental data are used to measure the
oscillations parameters <span class="math notranslate nohighlight">\(\Delta m^2\)</span> and <span class="math notranslate nohighlight">\(\sin^2 2\theta\)</span>. The results
are usually presented in a 2D plane <span class="math notranslate nohighlight">\((\Delta m^2,\sin^2 2\theta)\)</span> where
the excluded values (rejection region) of the parameters are on the
right of the boundary (see <a class="reference internal" href="#fig-exclplot"><span class="std std-numref">Fig. 60</span></a>).</p>
<p>Ignoring the physics behind, just think of this problem as a fit of the
data (number of events) to a function of two variables
<span class="math notranslate nohighlight">\(f(x,y) = P(\Delta m^2, \sin^2 2\theta)\)</span>.</p>
<p>The computation of the excluded region has the complications seen above:
the parameter <span class="math notranslate nohighlight">\(\sin^2 2\theta\)</span> is bounded in <span class="math notranslate nohighlight">\([0,1]\)</span> and the expected
number of oscillation events is extremely small on a potentially large
background.</p>
<p>The neutrino data and the expected background are collected in
histograms in bins of energy as <span class="math notranslate nohighlight">\(N=\{n_i\}\)</span> and <span class="math notranslate nohighlight">\(B=\{b_i\}\)</span>
respectively. The signal contribution (i.e. the expected number of
events coming from oscillations) is collected as a histogram in bins of
energy <span class="math notranslate nohighlight">\(T=\{\mu_i|\sin^2(2\theta), \Delta m^2\}\)</span>.</p>
<p>To find the upper limits on the oscillation parameters we can use the FC
construction. To build the exclusion region (this time is in 2D!) we fix
a point in the <span class="math notranslate nohighlight">\((\Delta m^2,\sin^2 2\theta)\)</span> plane (typically the plane
is finely binned and instead of a point we approximate one region in the
plane with the averaged values of the parameters in that region) and
compute the rank of the measured values using as ordering principle the
likelihood ratio:</p>
<div class="math notranslate nohighlight">
\[
R = \frac{P(N|T(\sin^2(2\theta), \Delta m^2))}{P(N|T(\sin^2(2\theta_{best}), \Delta m_{best}^2))}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sin^2(2\theta_{best}), \Delta m_{best}^2\)</span> are the values giving
the highest probability <span class="math notranslate nohighlight">\(P(N|T)\)</span> in the physically allowed range of the
parameters. This is equivalent to compute for each bin in the 2D plane:</p>
<div class="math notranslate nohighlight">
\[
R = \Delta \chi^2 = 2\sum_i \left( \mu_i -\mu_{best} + n_i\ln\left(\frac{\mu_{best} + b_i}{\mu_i + b_i}\right) \right)
\]</div>
<p><a class="reference internal" href="#fig-exclplot"><span class="std std-numref">Fig. 60</span></a> shows the exclusion region on the 2D plane
obtained with a toy study. Neutrinos are created uniformly at a distance
between 600m and 1000m from the detector with a flat energy spectrum
between 10 and 60 GeV. The background flux from <span class="math notranslate nohighlight">\(\nu_\mu\)</span> misidentified
as <span class="math notranslate nohighlight">\(\mu_e\)</span> is set to 500 events flat in the energy range. The total
<span class="math notranslate nohighlight">\(\nu_\mu\)</span> flux is such that we get 100 events for an oscillation
probability <span class="math notranslate nohighlight">\(P(\nu_\mu\to\nu_e)=0.01\)</span>. For each toy experiment we
compute <span class="math notranslate nohighlight">\(\Delta\chi^2\)</span>. For each bin in the
<span class="math notranslate nohighlight">\((\Delta m^2,\sin^2 2\theta)\)</span> plane the we need to find
<span class="math notranslate nohighlight">\(\Delta \chi^2_c\)</span> such that <span class="math notranslate nohighlight">\(\alpha\)</span> (1-CL) of the simulated experiments
have <span class="math notranslate nohighlight">\(\Delta\chi^2 &lt; \Delta\chi^2_c\)</span>. To get the exclusion limit from
data (the histogram <span class="math notranslate nohighlight">\(N\)</span>) we compute the <span class="math notranslate nohighlight">\(\Delta \chi^2\)</span> for each bin of
the <span class="math notranslate nohighlight">\((\Delta m^2,\sin^2 2\theta)\)</span> plane and compare it with the
<span class="math notranslate nohighlight">\(\Delta \chi_c^2\)</span> found above. The boundary is given by:</p>
<div class="math notranslate nohighlight">
\[
\Delta \chi^2 (N|\sin^2(2\theta),\Delta m^2) &lt; \Delta \chi^2_c(\sin2(2\theta),\Delta m^22).
\]</div>
<figure class="align-center" id="fig-exclplot">
<a class="reference internal image-reference" href="_images/exclPlot.png"><img alt="_images/exclPlot.png" src="_images/exclPlot.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 60 </span><span class="caption-text">Confidence
region for an example of the toy model in which <span class="math notranslate nohighlight">\(\sin^2(2\theta) = 0\)</span>.
The 90% confidence region is the area to the right of the curve. In
other words the null hypothesis H<span class="math notranslate nohighlight">\(_0\)</span> is no oscillations and with the
data at hand I can exclude at 90% CL the region of parameters to the
right of the curve.</span><a class="headerlink" href="#fig-exclplot" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="other-examples">
<h3>Other examples<a class="headerlink" href="#other-examples" title="Link to this heading">#</a></h3>
<p>The Feldman-Cousins construction was used in CMS [&#64;Hig12045] for the
determination of the intervals on the Higgs measured signal strength
(<span class="math notranslate nohighlight">\(\hat{\mu} = \sigma/\sigma_M\)</span> ratio of the measured cross section to
the Standard Model expectation). In <a class="reference internal" href="#fig-higgsfc"><span class="std std-numref">Fig. 61</span></a> the
intervals obtained without limiting <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> to be positive and the
one imposing <span class="math notranslate nohighlight">\(\hat{\mu}&gt;0\)</span> obtained with the FC prescription. In
<a class="reference internal" href="#fig-higgsfc2"><span class="std std-numref">Fig. 62</span></a> the
signal strength is fitted on the plane of the different production
mechanisms gluon-gluon-fusion-plus-ttH and VBF-plus-VH.</p>
<figure class="align-center" id="fig-higgsfc">
<a class="reference internal image-reference" href="_images/higgsFC.png"><img alt="_images/higgsFC.png" src="_images/higgsFC.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 61 </span><span class="caption-text">(left) Values of
<span class="math notranslate nohighlight">\(\hat{\mu} = \sigma/\sigma_M\)</span> for the combination (solid vertical line)
and for contributing channels (points). The horizontal bars indicate the
<span class="math notranslate nohighlight">\(\pm1\sigma\)</span> uncertainties on the <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> values for individual
channels; they include both statistical and systematic uncertainties.
(right) The same intervals imposing the signal strength to be positive
and computed using the Feldman-Cousins
construction.</span><a class="headerlink" href="#fig-higgsfc" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-higgsfc2">
<a class="reference internal image-reference" href="_images/higgsFC2.png"><img alt="_images/higgsFC2.png" src="_images/higgsFC2.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 62 </span><span class="caption-text">(Left plot) The 68%
CL intervals for signal strength in the gluon-gluon-fusion-plus-ttH and
in VBF-plus-VH production mechanisms: <span class="math notranslate nohighlight">\(\mu_{ggF + ttH}\)</span> and
<span class="math notranslate nohighlight">\(\mu_{VBF+VH}\)</span>, respectively. The different colors show the results
obtained by combining data from each of the five analyzed decay modes:
<span class="math notranslate nohighlight">\(\gamma\gamma\)</span> (green), <span class="math notranslate nohighlight">\(WW\)</span> (blue), <span class="math notranslate nohighlight">\(ZZ\)</span>(red), <span class="math notranslate nohighlight">\(\tau\tau\)</span> (violet),
<span class="math notranslate nohighlight">\(bb\)</span> (cyan). The crosses indicate the best-fit values. The diamond at
(1,1) indicates the expected values for the SM Higgs boson. (right) The
same intervals imposing the signal strength to be positive and computed
using the Feldman-Cousins
construction.</span><a class="headerlink" href="#fig-higgsfc2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="underfluctuations-and-significance">
<h3>Underfluctuations and significance<a class="headerlink" href="#underfluctuations-and-significance" title="Link to this heading">#</a></h3>
<p>The FC prescription, while giving a frequentist solution to several
problems, it stumbles on a problem with eperiments having large
background underfluctuations. Suppose you have two experiments:</p>
<ul class="simple">
<li><p>a super optimized experiment expects no background events and
observes zero events in a given data taking period</p></li>
<li><p>a less optimized experiment expects 10 background events and
observes five events in a given data taking period</p></li>
</ul>
<p>Using the FC prescription the upper limit on a signal at 90% CL for the
first one is 2.44, while the second is 1.85. The worse experiment,
clearly observing a lucky underfluctuation, obtains a better limit. This
is not a desirable feature of the method. Quoting the authors: “The
origin of these concerns lies in the natural tendency to want to
interpret these results as the probability <span class="math notranslate nohighlight">\(P(\mu|x_0)\)</span> of a hypothesis
given data, rather than what they are really related to, namely the
probability <span class="math notranslate nohighlight">\(P(x_0|\mu)\)</span> of obtaining data given a hypothesis. It is the
former that a scientist may want to know in order to make a decision,
but the latter which classical confidence intervals relate to. […]
scientists may make Bayesian inferences of <span class="math notranslate nohighlight">\(P(\mu|x_0)\)</span> based on
experimental results combined with their personal, subjective prior
probability distribution function. It is thus incumbent on the
experimenter to provide information that will assist in this
assessment.” The suggested way is to provide, together with the limit,
also the <em>sensitivity</em> of the experiment, defined as “the average upper
limit that would be obtained by an ensemble of experiments with the
expected background and no true signal”. This extra information allows
the reader to better understand if the value of a tight limit is just an
artifact coming from an under-fluctuation of the background. When a
significant portion of the upper limit curve is below the sensitivity of
the experiment, the authors suggest to show both the sensitivity curve
and the upper limit. <a class="reference internal" href="#fig-exclplotsensitivity"><span class="std std-numref">Fig. 63</span></a></p>
<figure class="align-center" id="fig-exclplotsensitivity">
<a class="reference internal image-reference" href="_images/exclPlotsensitivity.png"><img alt="_images/exclPlotsensitivity.png" src="_images/exclPlotsensitivity.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 63 </span><span class="caption-text">Confidence region for an example of the toy model in which <span class="math notranslate nohighlight">\(\sin^2(2\theta) = 0\)</span>
together with the sensitivity of the experiment.</span><a class="headerlink" href="#fig-exclplotsensitivity" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="lep-test-statistic-l-s-b-l-b">
<h2>LEP test statistic: <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span><a class="headerlink" href="#lep-test-statistic-l-s-b-l-b" title="Link to this heading">#</a></h2>
<p>To understand the test statistics <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span> we will use as an
example the search for the SM Higgs at LEP.
<a class="reference internal" href="#fig-leptight"><span class="std std-numref">Fig. 64</span></a> shows the Higgs candidate invariant mass
distribution in data together with the histograms of the expected
background and the expected signal at a mass of 115 GeV. The invariant
mass is the discriminating variable used to extract the Higgs signal
from background. We assume that, for each bin in invariant mass, we know
what is the expected number of signal and background events. It is
important to notice the difference between the <em>invariant mass</em> of a
Higgs candidate <span class="math notranslate nohighlight">\(m_H^{rec}\)</span>, which is the invariant mass computed from
the reconstructed decay products and the <em>test mass</em> <span class="math notranslate nohighlight">\(m_H\)</span> which is the
value of the mass at which we build the signal model we want to test.</p>
<p>The typical search for Higgs boson is a combination of the analyses of
different Higgs production (at LEP mostly Higgsstrahlung and vector
boson fusion) and decay modes (predominantly <span class="math notranslate nohighlight">\(H\to b\bar{b}\)</span> and
<span class="math notranslate nohighlight">\(H\to\tau^+\tau^-\)</span>). The (extended) likelihood used for each
production/decay mode is:</p>
<div class="math notranslate nohighlight">
\[
L_{s+b} = \frac{(s(m_H)+b)^n}{n!}e^{-(s(m_H)+b)} \prod_{j=1}^{n_{bins}}\frac{s(m_H)S(x_j,m_H)+bB(x_j)}{s(m_H)+b}
\]</div>
<p>which, for the background only hypothesis reduces to:</p>
<div class="math notranslate nohighlight">
\[
L_{b} = \frac{b^n}{n!}e^{-b}\prod_{j=1}^{n_{bins}}B(x_j).
\]</div>
<p>Here <span class="math notranslate nohighlight">\(s\)</span> is the number of expected signal events (which is a function of the test
mass <span class="math notranslate nohighlight">\(m_H\)</span>), <span class="math notranslate nohighlight">\(b\)</span> the number of expected background events, <span class="math notranslate nohighlight">\(n\)</span> the
number of observed events, <span class="math notranslate nohighlight">\(x_j\)</span> the value of the invariant mass
<span class="math notranslate nohighlight">\(m_H^{rec}\)</span> (our discriminating variable) and <span class="math notranslate nohighlight">\(S(x_j,m_H)\)</span>, <span class="math notranslate nohighlight">\(B(x_j)\)</span> the
signal (function of the test mass) and background pdf computed at <span class="math notranslate nohighlight">\(x_j\)</span>
(i.e. the signal and background shapes).</p>
<figure class="align-center" id="fig-leptight">
<a class="reference internal image-reference" href="_images/LEPtight.png"><img alt="_images/LEPtight.png" src="_images/LEPtight.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 64 </span><span class="caption-text">Reconstructed invariant mass spectrum for the
Higgs search at LEP.</span><a class="headerlink" href="#fig-leptight" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The complete likelihood for the signal+background hypothesis is the
product of the likelihoods of each production/decay:</p>
<div class="math notranslate nohighlight" id="equation-eq-lsb">
<span class="eqno">(14)<a class="headerlink" href="#equation-eq-lsb" title="Link to this equation">#</a></span>\[L_{s+b} = \prod_{k=1}^{N}\frac{(s_k(m_H)+b_k)^{n_k}}{n_k!}e^{-(s_k(m_H)+b_k)}\prod_{j=1}^{n^k_{bins}}\frac{s_k(m_H)S_k(x_{jk}; m_H)+b_kB_k(x_{jk})}{s_k(m_H)+b_k}\]</div>
<p>where the index <span class="math notranslate nohighlight">\(k\)</span> runs over the <span class="math notranslate nohighlight">\(N\)</span> production/decay modes analysed.
The likelihood for the background only is trivially obtained again
setting <span class="math notranslate nohighlight">\(s\)</span> to zero.</p>
<p>The discovery test statistics used at LEP is based on the likelihood
ratio:</p>
<div class="math notranslate nohighlight">
\[
Q = \frac{L_{s+b}}{L_b}.
\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Multiplying several positive numbers smaller than one, will quickly reach the machine numerical precision.</p>
</aside>
<p>For the same numerical reasons
already encountered for the likelihood, instead of Q, the logarithm of Q
is used:</p>
<div class="math notranslate nohighlight" id="equation-eq-qlep">
<span class="eqno">(15)<a class="headerlink" href="#equation-eq-qlep" title="Link to this equation">#</a></span>\[q = -2 \ln Q = -2\ln\left(\frac{L_{s+b}}{L_b}\right).\]</div>
<p>Computing <span class="math notranslate nohighlight">\(q\)</span> explicitly with <a class="reference internal" href="#equation-eq-lsb">(14)</a> we obtain:</p>
<div class="math notranslate nohighlight">
\[
q = -2\ln Q(m_H) = 2\sum_{k=1}^{N}\left[ s_k(m_H) - \sum_{j=1}^{n^k_{bins}} \ln \left( 1+\frac{s_k(m_H)S_k(x_{jk}, m_H)}{b_k B_k(x_{jk})}\right)\right]
\]</div>
<p>This expression shows that each bin contributes to the likelihood with a
weight <span class="math notranslate nohighlight">\(\ln(1+S/B)\)</span> to the final test statistics. Because of this, a
typical way to present the results is to plot the data as a function of
<span class="math notranslate nohighlight">\(\ln(1+s/b)\)</span> as in <a class="reference internal" href="#fig-ln1psoverb"><span class="std std-numref">Fig. 65</span></a>. The region at large values of <span class="math notranslate nohighlight">\(\ln(1+s/b)\)</span>
has the highest sensitivity to the signal.</p>
<figure class="align-center" id="fig-ln1psoverb">
<a class="reference internal image-reference" href="_images/ln1pSoverB.png"><img alt="_images/ln1pSoverB.png" src="_images/ln1pSoverB.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 65 </span><span class="caption-text">Data (dots), signal (yellow histogram) and
background (dashed histogram) as a function of <span class="math notranslate nohighlight">\(\ln(1+s/b)\)</span>. In both
cases the signal histogram is built for a test mass of
115 GeV.</span><a class="headerlink" href="#fig-ln1psoverb" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The test statistic <span class="math notranslate nohighlight">\(-2\ln(Q)\)</span> is used to test for the presence of signal
in data. The intuition goes as: compute the test statistics on the
collected data and compare it with the “typical” test statistic values
under the signal+background or background only hypotheses (see <a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a>).</p>
<figure class="align-center" id="fig-q115">
<a class="reference internal image-reference" href="_images/Q115.png"><img alt="_images/Q115.png" src="_images/Q115.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 66 </span><span class="caption-text">Distribution of the test
statistic <span class="math notranslate nohighlight">\(-2\ln(Q)\)</span> for the signal+background hypothesis in brown and
background only hypothesis in blue. The value of the test statistic
computed on data (observed) is represented by the vertical black
line.</span><a class="headerlink" href="#fig-q115" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The “typical” values are obtained as the median of the pdf of the test
statistic for the signal+background and the background only hypothesis.
The pdfs are build from toy Monte Carlo samples. From the model in
<a class="reference internal" href="#equation-eq-lsb">(14)</a> we
generate several toy datasets and for each of them we compute the test
statistics <span class="math notranslate nohighlight">\(-2\ln(Q)\)</span>. The blue and brown pdfs in
<a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a> are
the normalized histograms of the test statistics under the two
hypothesis. To have a good estimation of the means, the histograms have
to be well populated which means tossing a large number of toy
experiments. This procedure is unfortunately very computing expensive.
We will see later, when discussing the test statistics used at the LHC,
how this problem can be mitigated. We can see from
<a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a> that
the pdf for the background only hypothesis clusters at large <span class="math notranslate nohighlight">\(-2\ln(Q)\)</span>
values, while the signal+background at low values. The observed is
somewhere in the middle and we will see in the next section how to use
this value to set a limit on the Higgs boson production. Note that the
values of the test statistic is a function of the test mass <span class="math notranslate nohighlight">\(m_H\)</span>, in
this plot the chosen value is <span class="math notranslate nohighlight">\(m_H=115\)</span> GeV. Scanning the value of the
test mass we obtain <a class="reference internal" href="#fig-biglep"><span class="std std-numref">Fig. 67</span></a>. Here the median of the signal+background
and the background-only hypotheses together with the observed value are
plotted as curves (dashed brown and blue, and continuous black
respectively) as a function of the test mass. The green(yellow) band
covers the 68%(95%) area around the median of the background only
hypothesis. These bands are a very common way to convey the statistical
uncertainty of the background: presence of signal would appear in this
plot as a deviation of the observed (black curve) from the expected for
background only (dashed-blue); in absence of signal, the observed would
be contained within the uncertainty bands. In
<a class="reference internal" href="#fig-adlo"><span class="std std-numref">Fig. 68</span></a> the same curves are plotted separately
for each of the four LEP experiments (ALEPH, DELPHI, L3, OPAL). We see
that for all experiments but ALEPH the observed fluctuates around the
expected background only curve. ALEPH shows a signal-like fluctuation
around 115 GeV.</p>
<figure class="align-center" id="fig-biglep">
<a class="reference internal image-reference" href="_images/bigLEP.png"><img alt="_images/bigLEP.png" src="_images/bigLEP.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 67 </span><span class="caption-text">Test statistics
as a function of the test mass for the combined LEP experiments. The
insert is <a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a> rotated and placed at the test mass of 115 GeV.</span><a class="headerlink" href="#fig-biglep" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-center" id="fig-adlo">
<a class="reference internal image-reference" href="_images/ADLO.png"><img alt="_images/ADLO.png" src="_images/ADLO.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 68 </span><span class="caption-text">Test statistics as a function of the test mass for the single
LEP experiments.</span><a class="headerlink" href="#fig-adlo" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>To better characterize data fluctuations we can compute the already
encountered <span class="math notranslate nohighlight">\(p\)</span>-value as the integral</p>
<div class="math notranslate nohighlight" id="equation-eq-pvaluelep">
<span class="eqno">(16)<a class="headerlink" href="#equation-eq-pvaluelep" title="Link to this equation">#</a></span>\[\int_{-\infty}^{-2\ln(Q_{obs})=q_{obs}}pdf(q|bkg)dq.\]</div>
<p>The smaller the
<span class="math notranslate nohighlight">\(p\)</span>-value (the further out in the tail the <span class="math notranslate nohighlight">\(q_{0,obs}\)</span> lies), the
poorest the agreement with the background only hypothesis.
Conventionally if the <span class="math notranslate nohighlight">\(p\)</span>-value is below <span class="math notranslate nohighlight">\(p = 2.87 \cdot 10^{-7}\)</span>,
corresponding to a 5<span class="math notranslate nohighlight">\(\sigma\)</span> gaussian probability tail, we talk about
“discovery”. <a class="reference internal" href="#fig-pvalues"><span class="std std-numref">Fig. 69</span></a> shows the <span class="math notranslate nohighlight">\(p\)</span>-value for the four LEP
experiments combined and the separately for each of them. The ALEPH
fluctuation observed in <a class="reference internal" href="#fig-adlo"><span class="std std-numref">Fig. 68</span></a> corresponds to a <span class="math notranslate nohighlight">\(p\)</span>-value of <span class="math notranslate nohighlight">\(\sim3~10^{-3}\)</span>
(i.e. <span class="math notranslate nohighlight">\(\sim3\sigma\)</span>), all other experiments are compatible with the
background only hypothesis at that mass. The combined <span class="math notranslate nohighlight">\(p\)</span>-value shows a
<span class="math notranslate nohighlight">\(\sim2\sigma\)</span> fluctuation around 95 GeV and a smaller one at around 115
GeV.</p>
<figure class="align-center" id="fig-pvalues">
<a class="reference internal image-reference" href="_images/pValues.png"><img alt="_images/pValues.png" src="_images/pValues.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 69 </span><span class="caption-text">p-values as a
function of the test mass for the combined (left) and single (right) LEP
experiments.</span><a class="headerlink" href="#fig-pvalues" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="nuisance-parameters">
<h3>Nuisance parameters<a class="headerlink" href="#nuisance-parameters" title="Link to this heading">#</a></h3>
<p>FIXME: the only difference between a poi and nuisance parameters is that
the latter has a constraint.</p>
<p>FIXME: add syst - nuisance - constraint gaussians, lognorm, flat</p>
<p>So far we have not considered any uncertainty on the signal and
background models. <em>Systematic</em> uncertainties can arise from several
sources and can affect both the signal (e.g. energy scale affecting the
position of the signal, energy resolution affecting its width, etc…)
and the background (the shape could come from some control region or
sidebands, etc…). To include this uncertainties, we add more
parameters to the model. These parameters are called <em>nuisance
parameters</em>: <span class="math notranslate nohighlight">\(\vec{\nu}\)</span>. For instance we can add an uncertainty
<span class="math notranslate nohighlight">\(\delta m\)</span> on the mass position of the signal <span class="math notranslate nohighlight">\(m_0\)</span>. The effect of these
uncertainties is to widen the test statistics pdfs. The reason for this
is rather intuitive, we are reducing the information in the model by
including uncertainties on the parameters, and so the separation power
between signal+background and background-only is reduced. The <span class="math notranslate nohighlight">\(p\)</span>-values
will become a function also of these extra parameters, e.g.:</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{q_{obs}}\mbox{pdf}(q|m_0)dq \qquad\to\qquad\int_{-\infty}^{q_{obs}}\mbox{pdf}(q|m_0, \delta m)dq.
\]</div>
<p>Ideally we would like any statement we make based on the pdf of the test
statistics to be valid for any value of the nuisances. This turns out to
be very restrictive when the values of the nuisance are disfavored by
the data. At LEP a <em>“hybrid frequentist-bayesian”</em> procedure was
followed to take into account nuisance parameters. Suppose you have a
prior <span class="math notranslate nohighlight">\(\pi(\nu)\)</span> describing the degree of belief on where the nuisance
parameter <span class="math notranslate nohighlight">\(\nu\)</span> lies. From the example above, the uncertainty on the
mass scale could be constrained by calibration measurements to be
gaussian distributed. Using the prior we can marginalize the likelihood:</p>
<div class="math notranslate nohighlight">
\[
L_{\mbox{marginalized}} (x) = \int L(x|\nu) \pi(\nu) d\nu
\]</div>
<p>and then
proceed using this new likelihood to perform any frequentist test
(<span class="math notranslate nohighlight">\(p\)</span>-value, intervals, etc…). The marginalization is usually done
with Monte Carlo techniques, sampling the distribution <span class="math notranslate nohighlight">\(\pi(\nu)\)</span> and
computing the likelihood at the sampled value <span class="math notranslate nohighlight">\(\nu\)</span>.</p>
<p>The Neyman-Pearson lemma says that the likelihood ratio <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span>
for simple hypotheses is the optimal test statistics. The inclusion of
nuisance parameters in the model changes the problem from the test of a
simple hypothesis to a composite one, so strictly speaking the
Neyman-Pearson lemma is not applicable. Nevertheless, when the nuisance
parameters are well constrained, we can effectively consider the
hypothesis to be simple and the likelihood ratio to be close to optimal.</p>
</section>
</section>
<section id="the-issue-of-sensitivity-and-the-cls-procedure">
<h2>The issue of sensitivity and the CLs procedure<a class="headerlink" href="#the-issue-of-sensitivity-and-the-cls-procedure" title="Link to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The larger the expected signal the easier is to exclude it.</p>
</aside>
<p>In absence of a clear signal observation (conventionally corresponding
to a significance of <span class="math notranslate nohighlight">\(3\sigma\)</span>) we can still use our data to provide
useful information about the largest signal we can exclude. We have
already encountered the concept of upper limit in Sec.<a class="reference internal" href="#confidenceIntervals.html#confidence-belt-neyman-frequentist-construction"><span class="xref myst">Confidence Belt</span></a>.
Let’s apply it to the Standard Model Higgs boson search at LEP. The
search results were presented in terms of lower limits on the mass. The
procedure to set the limit is straightforward. Look again at
<a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a>. In the previous section we used the observed value of the test statistics
to define the <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(p_b\)</span> on the pdf of the background only
hypothesis (yellow area). This gave us the probability of the background
to over-fluctuate to mimic a signal. Now we use the observed value of
the test statistics to define the <span class="math notranslate nohighlight">\(p\)</span>-value <span class="math notranslate nohighlight">\(p_{s+b}\)</span> on the pdf of the
signal+background hypothesis (green area).</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Here we assuming that the searched for signal is the Standard
Model Higgs boson, i.e. its production cross section/couplings/etc.
are the ones predicted by the Standard Model.</p>
</aside>
<p>This will give us the
probability for the signal+background to under-fluctuate to mimic
absence of signal. When <span class="math notranslate nohighlight">\(p_{s+b} = 0.05\)</span> we can exclude the presence of
signal at the 95% CL. To set a limit on the mass of the SM Higgs boson
we scan the values of the test mass until we find <span class="math notranslate nohighlight">\(p_{s+b} = 0.05\)</span>.</p>
<p>In <a class="reference internal" href="#fig-qxx"><span class="std std-numref">Fig. 70</span></a> we
show the pdf of the test statistics for different test mass values. The
central plot is again <a class="reference internal" href="#fig-q115"><span class="std std-numref">Fig. 66</span></a>, the left one is computed at 110 GeV and the right
one at 120 GeV. The lower the mass the larger the separation power
between the signal+background and the background only hypothesis and
this translates, for a given dataset, to lower and lower values of
<span class="math notranslate nohighlight">\(p_{s+b}\)</span>. We can read this as “given the expected SM-Higgs and the
SM-backgrounds, it’s easier to exclude low mass values than high ones”.
Going to high masses we see that the overlap between the two pdfs
increases. The physical meaning of this is that we are not able anymore
to separate the two hypothesis (in this case, we’re are reaching the
kinematic limit of LEP to produce SM Higgs bosons).</p>
<figure class="align-center" id="fig-qxx">
<a class="reference internal image-reference" href="_images/Qxx.png"><img alt="_images/Qxx.png" src="_images/Qxx.png" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 70 </span><span class="caption-text">The pdf of the test statistics for different test mass values.</span><a class="headerlink" href="#fig-qxx" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This situation is rather dangerous. Let’s take a deeper look at the
meaning of the <span class="math notranslate nohighlight">\(p\)</span>-values in
<a class="reference internal" href="#fig-pvaluefluctuations"><span class="std std-numref">Fig. 71</span></a> to understand why. The left plot
focuses on the background only hypothesis: the right tail of the
distribution contains experiments where the background under-fluctuates
(i.e. we are lacking events also for the background only hypothesis),
while the left tail instead contains the experiments where the backgroud
over-fluctuates mimicking a signal. The right plot instead focuses on
the signal+background hypothesis: the left tail contains experiments
where the signal+background over-fluctuates, while in the right tail
there are the under-fluctuations that mimic absence of signal.</p>
<figure class="align-center" id="fig-pvaluefluctuations">
<a class="reference internal image-reference" href="_images/pValuefluctuations.png"><img alt="_images/pValuefluctuations.png" src="_images/pValuefluctuations.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 71 </span><span class="caption-text">Signal and background under/over
fluctuations.</span><a class="headerlink" href="#fig-pvaluefluctuations" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let’s contrast this picture with
<a class="reference internal" href="#fig-nosep"><span class="std std-numref">Fig. 72</span></a>. In
this case the two pdfs largely overlaps providing very small separation
power. Here is where the situation becomes dangerous. Suppose we are
setting a 95% CL on a signal and the test mass used for
<a class="reference internal" href="#fig-nosep"><span class="std std-numref">Fig. 72</span></a> is
the one providing <span class="math notranslate nohighlight">\(p_{s+b} = 0.05\)</span>. In this case we can exclude the
presence of a signal with such a mass at 95% CL, but we are also sitting
on the right tail of the background. <em>The background itself has an
under-fluctuation!</em> From the point of view of a signal search, it
doesn’t make any sense. We are excluding the signal+background and the
background-only hypotheses at the same CL. A situation like that could
arise when trying to exclude a Higgs boson of 1 TeV at LEP where the
kinematic reach is just above 100 GeV. While that is kinematically
obvious, it is worth noticing that the procedure for limits settings
detailed so far does not prevent to incur in such kind of troubles.</p>
<figure class="align-center" id="fig-nosep">
<a class="reference internal image-reference" href="_images/noSep.png"><img alt="_images/noSep.png" src="_images/noSep.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 72 </span><span class="caption-text">Poor separation power.</span><a class="headerlink" href="#fig-nosep" title="Link to this image">#</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This typically generate some confusion, the CL is a parameter set
by hand and not a function of the data!</p>
</aside>
<p>The root of the problem is that there is not enough information used in
the limit setting procedure and we risk a “spurious exclusion”. A way to
overcome this issue is to include somehow the information coming from
<span class="math notranslate nohighlight">\(p_b\)</span>. This is what the “CLs” procedure does. To illustrate the “CLs”
procedure we will follow the notation of the original paper
Ref.<span id="id2">[<a class="reference internal" href="bibliography.html#id16" title="Alexander L. Read. Modified frequentist analysis of search results (The CL(s) method). In Workshop on Confidence Limits, 81–101. 8 2000.">Rea00</a>]</span>. The <span class="math notranslate nohighlight">\(p_{s+b}\)</span> is renamed “confidence level”. The
confidence level of the signal+background hypothesis is then the
integral <span class="math notranslate nohighlight">\(CL_{s+b} = P(Q&gt;Q_{obs}|s+b)\)</span> calculated on the p.d.f. for
signal+background hypothesis. Small values of <span class="math notranslate nohighlight">\(CL_{s+b}\)</span> correspond to a
poor compatibility with the signal+background hypothesis and so favor
the background only hypothesis and viceversa.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The idea stemmed from the frequentist approach of Zech to the
problem of setting limits for a counting experiment in presence of
background [&#64;Zech].</p>
</aside>
<p>The <span class="math notranslate nohighlight">\(CL_s\)</span> procedure “corrects” the <span class="math notranslate nohighlight">\(CL_{s+b}\)</span> dividing it by <span class="math notranslate nohighlight">\(CL_b\)</span>, defined as
<span class="math notranslate nohighlight">\(CL_b = 1 - P(Q&lt;Q_{obs}|b)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-cls">
<span class="eqno">(17)<a class="headerlink" href="#equation-eq-cls" title="Link to this equation">#</a></span>\[CL_s = \frac{CL_{s+b}}{CL_b} = \frac{p_{s+b}}{1-p_b}\]</div>
<p>Remember that
despite the misleading name, this is not a confidence level ! It’s not
even a <span class="math notranslate nohighlight">\(p\)</span>-value, it’s a ratio of p-values. Nevertheless we will say
that a signal is excluded at the confidence level CL if <span class="math notranslate nohighlight">\(1-CL_s\ge CL\)</span>.</p>
<p>The false exclusion rate is now reduced:</p>
<ul class="simple">
<li><p>in case of a clear separation between the two hypothesis <span class="math notranslate nohighlight">\(p_b\to 0\)</span>
and <span class="math notranslate nohighlight">\(CL_s \to CL_{s+b}\)</span>, so we recover the standard p-value
definition;</p></li>
<li><p>in case of poor separation <span class="math notranslate nohighlight">\(p_b \to 1\)</span> and <span class="math notranslate nohighlight">\(CL_s \to 1\)</span>, preventing
spurious exclusions.</p></li>
</ul>
<p>The results of the CLs procedure applied to the Higgs search at LEP are
shown in <a class="reference internal" href="#fig-lepcls"><span class="std std-numref">Fig. 73</span></a>. This is the famous plot excluding Higgs masses
below 114.4 GeV <span id="id3">[<a class="reference internal" href="bibliography.html#id24" title="Marumi M. Kado and Christopher G. Tully. The searches for higgs bosons at lep. Annual Review of Nuclear and Particle Science, 52(1):65-113, 2002. doi:10.1146/annurev.nucl.52.050102.090656.">KT02</a>]</span>.</p>
<figure class="align-center" id="fig-lepcls">
<a class="reference internal image-reference" href="_images/LEPCLs.png"><img alt="_images/LEPCLs.png" src="_images/LEPCLs.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 73 </span><span class="caption-text">LEP exclusion plot for the Standard Model Higgs search.</span><a class="headerlink" href="#fig-lepcls" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The CLs will deviate from the standard <span class="math notranslate nohighlight">\(p\)</span>-value the smaller the
separation power of the test. The price to pay for this is that the
limits obtained with the <span class="math notranslate nohighlight">\(CL_s\)</span> procedure will by construction
“over-cover” resulting in conservative limits (you exclude less phase
space). This is clearly not a desirable feature for a frequentist-based
approach, but because “it works” it has been adopted as the standard way
to set the limits at colliders. Opponents to this rather arbitrary
procedure advocates the use of a Bayesian approach, which on the other
hand raises the usual issues about setting a prior on the parameter
under test.</p>
<p>For simplicity the CLs procedure has been detailed here using the LEP
test statistics <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span>, but it can be applied precisely in the
same way to the LHC test statistics that we will encounter in the next
section.</p>
</section>
<section id="lhc-test-statistics-2-ln-lambda-mu">
<h2>LHC test statistics: <span class="math notranslate nohighlight">\(-2\ln(\lambda(\mu))\)</span><a class="headerlink" href="#lhc-test-statistics-2-ln-lambda-mu" title="Link to this heading">#</a></h2>
<p>In this section we describe the LHC test statistics and review the large
sample approximations (Wald’s theorem and asymptotic formulas). We will
develop the main concepts step by step using the discovery test
statistic <span class="math notranslate nohighlight">\(q_0\)</span>. Then we will develop the test statistic for upper
limits and give some examples.</p>
<p>*In this section we will follow closely the paper in Ref. <span id="id4">[<a class="reference internal" href="bibliography.html#id17" title="Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells. Asymptotic formulae for likelihood-based tests of new physics. The European Physical Journal C, feb 2011. URL: https://doi.org/10.1140%2Fepjc%2Fs10052-011-1554-0, doi:10.1140/epjc/s10052-011-1554-0.">CCGV11</a>]</span>.</p>
<section id="profile-likelihood-ratio">
<h3>Profile likelihood ratio<a class="headerlink" href="#profile-likelihood-ratio" title="Link to this heading">#</a></h3>
<p>Throughout this section we will use a concrete example to keep a uniform
notation and help visualizing the results. For this “prototype
experiment”, let’s assume that the data are represented by a histogram
<span class="math notranslate nohighlight">\(\textbf{n} = (n_1,\dots, n_N)\)</span> in only one variable <span class="math notranslate nohighlight">\(x\)</span>. For example x
could be the candidate invariant mass (the generalization to several
variables is trivial). The expected number of events in each bin of the
histogram depends on our expectations of the signal and background:</p>
<div class="math notranslate nohighlight">
\[
E[n_i]=\mu s_i + b_i
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_i\)</span> is the number of signal events expected in bin <span class="math notranslate nohighlight">\(i\)</span>:
<span class="math notranslate nohighlight">\(s_i = s_{tot} \int_{bin_i} f_s(x|\theta_s) dx\)</span>. The distribution
<span class="math notranslate nohighlight">\(f_s\)</span> is the p.d.f. of the variable <span class="math notranslate nohighlight">\(x\)</span> for the signal, <span class="math notranslate nohighlight">\(\theta_s\)</span>
are the parameters characterising the shape of the signal and
<span class="math notranslate nohighlight">\(s_{tot}\)</span> is the total mean number of expected signal events;</p></li>
<li><p><span class="math notranslate nohighlight">\(b_i\)</span> is the number of background events expected in bin <span class="math notranslate nohighlight">\(i\)</span>:
<span class="math notranslate nohighlight">\(b_i = b_{tot} \int_{bin_i} f_b(x|\theta_b)dx\)</span>. The distribution
<span class="math notranslate nohighlight">\(f_b\)</span> is the p.d.f. of the variable <span class="math notranslate nohighlight">\(x\)</span> for the background,
<span class="math notranslate nohighlight">\(\theta_b\)</span> are the parameters characterising the shape of the
background and <span class="math notranslate nohighlight">\(b_{tot}\)</span> is the total mean number of expected
background events.</p></li>
<li><p>the parameter <span class="math notranslate nohighlight">\(\mu\)</span> is the <em>signal strength</em> that we have already
encountered and which allows to go in a continuous way from the
background only hypothesis <span class="math notranslate nohighlight">\(\mu=0\)</span> to the nominal signal+background
hypothesis <span class="math notranslate nohighlight">\(\mu=1\)</span></p></li>
</ul>
<p>We group all parameters, but the signal strength <span class="math notranslate nohighlight">\(\mu\)</span> our parameter of
interest, in a vector <span class="math notranslate nohighlight">\(\vec{\theta}=(\theta_s,\theta_b,s_{tot},b_{tot})\)</span>
of nuisance parameters.</p>
<p>You want to extract the fraction of signal events in a data
sample <span class="math notranslate nohighlight">\(D\)</span>. The statistical model used is:</p>
<div class="math notranslate nohighlight">
\[
L(D|f_{sig}) = f_{sig}~\mbox{Gauss}(m;m_0,\sigma) + (1-f_{sig})~e^{-\alpha m}
\]</div>
<p>where the observable <span class="math notranslate nohighlight">\(m\)</span> is the invariant mass of the candidates,
<span class="math notranslate nohighlight">\(f_{sig}\)</span> is the fraction of signal events, <span class="math notranslate nohighlight">\(m_0\)</span> is the position of the
resonance, <span class="math notranslate nohighlight">\(\sigma\)</span> is the width of the resonance and <span class="math notranslate nohighlight">\(\alpha\)</span> is the
slope of the background. The parameter of interest is <span class="math notranslate nohighlight">\(f_{sig}\)</span>, all the
other parameters of the model are <span class="math notranslate nohighlight">\(\vec{\theta}=\{m_0,~\sigma,~\alpha\}\)</span>.</p>
<p>Typically in a measurement, together with the main dataset, we use
several other samples to help constraining the parameters of the model
(e.g. the background in the signal region can be constrained by the
measurement of the number of events in a control sample). These
constraints are usually collected in auxiliary histograms
<span class="math notranslate nohighlight">\(\textbf{m} = (m_1, \ldots, m_M)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
E[m_i] = u_i(\theta)
\]</div>
<p>where the
<span class="math notranslate nohighlight">\(u_i\)</span> are quantities that depend on <span class="math notranslate nohighlight">\(\theta\)</span> and model e.g. the shape of
the background. With this we can build the complete likelihood used to
model the data:</p>
<div class="math notranslate nohighlight">
\[
L(\mu,\theta) = \prod_{j=1}^{N} \frac{(\mu s_j + b_j)^{n_j}}{n_j!} e^{-(\mu s_j + b_j)} \prod_{k=1}^{M} \frac{u_k^{m_k}}{m_k!} e^{-u_k}.
\]</div>
<p>The test statistic developed at the LHC is based on a <strong>profile
likelihood ratio</strong> defined by:</p>
<div class="math notranslate nohighlight" id="equation-eq-lr">
<span class="eqno">(18)<a class="headerlink" href="#equation-eq-lr" title="Link to this equation">#</a></span>\[\lambda(\mu) = \frac{L(\mu,\hat{\hat{\theta}})}{L(\hat{\mu},\hat{\theta})}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span> is the value we are testing</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\hat{\theta}}\)</span> is the best fit of the nuisance parameters once
we fixed the <span class="math notranslate nohighlight">\(\mu\)</span> we want to test (i.e. conditional to the test
value <span class="math notranslate nohighlight">\(\mu\)</span> in the likelihood). We say in this case that the
parameters <span class="math notranslate nohighlight">\(\theta\)</span> are <em>profiled</em>. The value of
<span class="math notranslate nohighlight">\(\hat{\hat{\theta}}\)</span> is a function of <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> are the best fit values of <span class="math notranslate nohighlight">\(\mu\)</span> and
<span class="math notranslate nohighlight">\(\theta\)</span> (the parameter of interest and the nuisances) when both are
left floating in the likelihood. In other words <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span> are the values that maximize the likelihood.</p></li>
</ul>
<p>The denominator of this ratio is just a likelihood function, the
numerator is called “profile likelihood” and the ratio is called
“profile likelihood ratio”. The fitted value <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is allowed to
take any positive or negative (unphysical) values (provided that
<span class="math notranslate nohighlight">\(\mu s_i + b_i\)</span> in the Poisson remains positive) even in the case where
the search targets a positive signal. This assumption, rather arbitrary
at this point, will be needed in the following to model <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> as a
gaussian distributed variable, that will allow write the test statistics
in an analytical closed form.<br />
The values taken by the profile likelihood ratio <span class="math notranslate nohighlight">\(\lambda(\mu)\)</span> are in
the interval <span class="math notranslate nohighlight">\([0,1]\)</span>. The ratio will get to unity the closer the test
value of <span class="math notranslate nohighlight">\(\mu\)</span> is to the value of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> preferred by the data. On
the contrary it will approach zero for test values of <span class="math notranslate nohighlight">\(\mu\)</span> very
different from <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>.<br />
As already noticed for the LEP test statistics, the inclusion of
nuisance parameters changes the hypothesis under test from simple to
composite. Generally, however, the nuisance parameters are well
constrained and the likelihood ratio test is close to optimal. The
inclusion of the nuisance parameters should give enough flexibility to
the likelihood to be able to model the “true” unknown values of the
parameters. Ideally, when making statements based on a test statistics
(e.g. when setting a limit on <span class="math notranslate nohighlight">\(\mu\)</span> at <span class="math notranslate nohighlight">\(1-\alpha\)</span> level) we would like
it to be correct for all values of the nuisances. In general it is not
possible to cover all values of the nuisances and as consequence the
coverage is not guaranteed. Nevertheless the choice of a profile
likelihood ratio allows to have the correct coverage at least in a
“trajectory” given by <span class="math notranslate nohighlight">\((\mu, \hat{\hat{\theta}})\)</span> <span id="id5">[<a class="reference internal" href="bibliography.html#id25" title="Kyle S Cranmer. Statistics for the LHC: Progress, Challenges, and Future. preprint, 2008. URL: http://cds.cern.ch/record/1099969, doi:10.5170/CERN-2008-001.47.">Cra08</a>]</span>.</p>
</section>
<section id="discovery-test-statistics">
<h3>Discovery test statistics<a class="headerlink" href="#discovery-test-statistics" title="Link to this heading">#</a></h3>
<p>From the definition in Eq.<a class="reference internal" href="#equation-eq-lr">(18)</a> we can build several test statistics. Instead of
listing all of them, let’s learn how to use one and come back later to
some of the other cases. We will start from the <strong>discovery test
statistics</strong> for a positive signal; this is the typical case for
searches at the LHC. We want to test for <span class="math notranslate nohighlight">\(\mu=0\)</span> which correspond to the
background only hypothesis. Rejecting the background only hypothesis
corresponds to acknowledge the presence of something else in data which
is not described correctly: a signal. The discovery test statistics for
positive signals is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-discovery">
<span class="eqno">(19)<a class="headerlink" href="#equation-eq-discovery" title="Link to this equation">#</a></span>\[\begin{split}        q_0 = \left\{
            \begin{array}{rll}
                -2 \ln \lambda(0) &amp; \mbox{if} &amp; \hat{\mu} \ge 0 \\
                0                 &amp; \mbox{if} &amp; \hat{\mu} &lt; 0 
            \end{array}\right.\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda(0)\)</span> is the is the
profile likelihood ratio for <span class="math notranslate nohighlight">\(\mu = 0\)</span> as defined in
Eq.<a class="reference internal" href="#equation-eq-lr">(18)</a>. With
this definition, large values of <span class="math notranslate nohighlight">\(q_0\)</span> correspond to increasing
incompatibility between the data and the background only hypothesis.
Remember that <span class="math notranslate nohighlight">\(\mu\)</span> is the value you are testing, in this case <span class="math notranslate nohighlight">\(\mu=0\)</span>,
while <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is the value you fit from data (the so called “best
fit”). The idea to have different definitions for positive and negative
values of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> has a simple explanation. If the measured value
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is negative, it means that we are observing fewer events
than what we would expect from the background only hypothesis. In
absence of signal, that should happen 50% of the times: for large enough
statistics allowing for a gaussian description, 50% of the times the
background fluctuates up, 50% it fluctuates down. Under-fluctuations of
the background are of no interest for the search of an excess of events
in data. To discover a new signal we are only interested in upper
fluctuations of data with little compatibility with the background only
model, i.e. <span class="math notranslate nohighlight">\(\hat{\mu} \ge 0\)</span>.<br />
To quantify the level of disagreement between the background only
hypothesis and the observed value of the test statistics, we can compute
the <span class="math notranslate nohighlight">\(p\)</span>-value</p>
<div class="math notranslate nohighlight">
\[
p=\int_{q_{0,obs}}^\infty f(q_0|0) dq_0
\]</div>
<p><span class="math notranslate nohighlight">\(f(q_0|0)\)</span> denotes the pdf of the statistic <span class="math notranslate nohighlight">\(q_0\)</span> under assumption of the
background-only (<span class="math notranslate nohighlight">\(\mu = 0\)</span>) hypothesis (see
<a class="reference internal" href="#fig-measuredpvalue"><span class="std std-numref">Fig. 74</span></a>). Note that the extremes of the
<span class="math notranslate nohighlight">\(p\)</span>-value integral are different from the LEP case in
Eq.<a class="reference internal" href="#equation-eq-pvaluelep">(16)</a> because of the different test statistics
definition.</p>
<figure class="align-center" id="fig-measuredpvalue">
<a class="reference internal image-reference" href="_images/MeasuredPValue.png"><img alt="_images/MeasuredPValue.png" src="_images/MeasuredPValue.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 74 </span><span class="caption-text">Example of a measured p-value when having the
observed test statistics <span class="math notranslate nohighlight">\(q_{0,obs}\)</span>. Good compatibility with <span class="math notranslate nohighlight">\(H_{0}\)</span>
corresponds to a <span class="math notranslate nohighlight">\(q_{0,obs}\)</span> which is on the left side (i.e. has a large
p-value), whereas a <span class="math notranslate nohighlight">\(q_{0,obs}\)</span> on the far right indicates bad
compatibility.</span><a class="headerlink" href="#fig-measuredpvalue" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In order to compute the integral we need to know <span class="math notranslate nohighlight">\(f(q_0|0)\)</span>. The brute
force way to build the p.d.f. for the test statistics <span class="math notranslate nohighlight">\(q_0\)</span> is through
toy experiments. Each toy experiment is created by generating random
data on the background only model; to be representative of the
luminosity in the data set we are analysing, the number of events in
each toy has to match the one of the measured data sample. For each toy
we then compute the test statistics and fill a histogram. The number of
toy experiments to be generated depends on the significance we are
trying to estimate. Because the <span class="math notranslate nohighlight">\(p\)</span>-value is computed by integrating the
histogram above <span class="math notranslate nohighlight">\(q_{0,obs}\)</span>, we will need to properly populate the tail
of the distribution above <span class="math notranslate nohighlight">\(q_{0,obs}\)</span>. To quantify a deviation
corresponding to a discovery (<span class="math notranslate nohighlight">\(2.87 \cdot 10^{-7}\)</span>) we will need to
generate a number of toy experiments significantly larger than
<span class="math notranslate nohighlight">\(1/(2.87 \cdot 10^{-7})\)</span></p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>Suppose you want to estimate the <span class="math notranslate nohighlight">\(p\)</span>-value for the measured
test statistics <span class="math notranslate nohighlight">\(q_{0,obs}\)</span> where the signal appears in the distribution
of the invariant mass as gaussian bump on an exponentially falling
background. The procedure is depicted in
<a class="reference internal" href="#fig-toys"><span class="std std-numref">Fig. 75</span></a>. From
the exponential distribution describing the background we generate
random events following e.g. the “hit or miss” method shown in
Sec. <a class="reference internal" href="#monteCarlo.html#acceptance-rejection-method"><span class="xref myst">MC</span></a>. The
number of events to be generated has to be same as the one
experimentally collected (i.e. representing the same integrated
luminosity). We repeat the data generation a large number of times, we
compute the test statistics for each “toy experiment” and fill a
histogram with those values. The observed <span class="math notranslate nohighlight">\(p\)</span>-value is simply the
integral of the histogram from <span class="math notranslate nohighlight">\(q_{0,obs}\)</span> to infinity.</p>
</div>
<figure class="align-center" id="fig-toys">
<a class="reference internal image-reference" href="_images/toys.png"><img alt="_images/toys.png" src="_images/toys.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 75 </span><span class="caption-text">Cartoon showing how to
extract the <span class="math notranslate nohighlight">\(p\)</span>-value from a toy study. The pdf <span class="math notranslate nohighlight">\(f(q_0|0)\)</span> is
approximated by the histogram normalized to unit
area.</span><a class="headerlink" href="#fig-toys" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="asymptotic-formulas">
<h3>Asymptotic Formulas<a class="headerlink" href="#asymptotic-formulas" title="Link to this heading">#</a></h3>
<p>So far we have seen how to build the test statistics <span class="math notranslate nohighlight">\(f(q_0|0)\)</span> tossing
toy experiments and how CPU expensive that is. In recent years Cowan at
al. in Ref. <span id="id6">[<a class="reference internal" href="bibliography.html#id17" title="Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells. Asymptotic formulae for likelihood-based tests of new physics. The European Physical Journal C, feb 2011. URL: https://doi.org/10.1140%2Fepjc%2Fs10052-011-1554-0, doi:10.1140/epjc/s10052-011-1554-0.">CCGV11</a>]</span> have used results proved by Wilks and Wald in
the early ’40s to overcome this problem and find an analytic formula to
describe the generic pdf <span class="math notranslate nohighlight">\(f(q_\mu|\mu')\)</span>.</p>
<p>The Wald’s theorem basically states that in the limit of a sufficiently
large data sample we can approximate the test statistics
<span class="math notranslate nohighlight">\(-2\ln\lambda(\mu)\)</span> (see Eq.<a class="reference internal" href="#equation-eq-lr">(18)</a>) as:</p>
<div class="math notranslate nohighlight" id="equation-eq-wald">
<span class="eqno">(20)<a class="headerlink" href="#equation-eq-wald" title="Link to this equation">#</a></span>\[-2\ln\lambda(\mu) = \frac{(\mu-\hat{\mu})^2}{\sigma^2} + o\left( \frac{1}{\sqrt{N}}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the value we are testing, <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is the estimator of
<span class="math notranslate nohighlight">\(\mu\)</span> and the width <span class="math notranslate nohighlight">\(\sigma\)</span> can be extracted from the second derivative
of the likelihood (Fisher information) as</p>
<div class="math notranslate nohighlight">
\[
V^{-1}_{ij} = -E\left[ \frac{\partial^2 \ln L}{\partial \theta_i \partial \theta_j} \right]
\]</div>
<p>or using the Asimov dataset that we will describe in the next section.<br />
The importance of Wald’s theorem is that, for large enough statistics,
the estimator <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is gaussian distributed around <span class="math notranslate nohighlight">\(\mu'\)</span> (the
true value of the parameter <span class="math notranslate nohighlight">\(\mu\)</span> - unknown in data, only nature knows
it, or known in a Monte Carlo sample, you decided its value) and that
all the parameters of the gaussian distribution can be computed. The
“large enough” sample limitation is needed to be able to neglect the
term <span class="math notranslate nohighlight">\(o\left( \frac{1}{\sqrt{N}}\right)\)</span>, but we will see later that the
approximation is valid for relatively low number of events.<br />
Neglecting the term <span class="math notranslate nohighlight">\(o(1\sqrt{N})\)</span> the test statistics
<span class="math notranslate nohighlight">\(t_{\mu} = -2\ln \lambda({\mu})\)</span> is distributed as a “non-central
<span class="math notranslate nohighlight">\(\chi^2\)</span>” distribution for one degree of freedom.</p>
<div class="math notranslate nohighlight">
\[
f(t_\mu|\Lambda) = \frac{1}{2\sqrt{t_\mu}}\frac{1}{\sqrt{2\pi}}\left[ \exp \left( -\frac{1}{2} (\sqrt{t_\mu} + \sqrt{\Lambda})^2 \right) + \exp\left( -\frac{1}{2} (\sqrt{t_\mu} - \sqrt{\Lambda})^2 \right) \right]
\]</div>
<p>with the non-centrality parameter <span class="math notranslate nohighlight">\(\Lambda\)</span></p>
<div class="math notranslate nohighlight">
\[
\Lambda = \frac{(\mu-\mu')^2}{\sigma^2}
\]</div>
<p>The Wilks’ theorem is a
special case of the Wald’s theorem for <span class="math notranslate nohighlight">\(\mu'=\mu\)</span>, <span class="math notranslate nohighlight">\(\Lambda = 0\)</span>. In
that case <span class="math notranslate nohighlight">\(-2\ln\lambda(\mu)\)</span> approaches a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution for one
degree of freedom.</p>
<p>We can now apply these results to the discovery test statistics. In the
approximation of large test statistics Eq.<a class="reference internal" href="#equation-eq-wald">(20)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-walddiscovery">
<span class="eqno">(21)<a class="headerlink" href="#equation-eq-walddiscovery" title="Link to this equation">#</a></span>\[\begin{split}        q_0 = \left\{
            \begin{array}{rll}
                -2 \ln \lambda(0) &amp; \mbox{if} &amp; \hat{\mu} \ge 0 \\
                0                 &amp; \mbox{if} &amp; \hat{\mu} &lt; 0 
            \end{array}\right.
\qquad \Rightarrow \qquad
        q_0 = \left\{
            \begin{array}{rll}
                \hat{\mu}^2/\sigma^2 &amp; \mbox{if} &amp; \hat{\mu} \ge 0 \\
                0                    &amp; \mbox{if} &amp; \hat{\mu} &lt; 0 
            \end{array}\right.            \end{split}\]</div>
<p>where the estimator <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is
gaussian distributed around the mean <span class="math notranslate nohighlight">\(\mu'\)</span>. The pdf for the test
statistics for a generic <span class="math notranslate nohighlight">\(\mu'\)</span> becomes:</p>
<div class="math notranslate nohighlight" id="equation-eq-asymptoticdiscoverygen">
<span class="eqno">(22)<a class="headerlink" href="#equation-eq-asymptoticdiscoverygen" title="Link to this equation">#</a></span>\[f(q_0|\mu') = \left( 1-\Phi\left(\frac{\mu'}{\sigma} \right)\right)\delta(q_0) + \frac{1}{2}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{q_0}} \exp\left[-\frac{1}{2}\left(\sqrt{q_0} - \frac{\mu'}{\sigma} \right)^2\right]\]</div>
<p>The special case where <span class="math notranslate nohighlight">\(\mu'=0\)</span> , i.e. in the hypothesis of background
only, this equation simplifies to</p>
<div class="math notranslate nohighlight" id="equation-eq-asymptoticdiscovery">
<span class="eqno">(23)<a class="headerlink" href="#equation-eq-asymptoticdiscovery" title="Link to this equation">#</a></span>\[f(q_0|0) = \frac{1}{2}\delta(q_0) + \frac{1}{2}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{q_0}} e^{-\frac{q_0}{2}}\]</div>
<p>Compare this formula to the bottom left plot in
<a class="reference internal" href="#fig-toys"><span class="std std-numref">Fig. 75</span></a>. The
delta function describes the first bin of the histogram; this comes from
our choice to set the test statistics to zero when <span class="math notranslate nohighlight">\(\hat{\mu}&lt;0\)</span>. The
coefficient 1/2 of the delta function reflects the 50% probability of
the background to fluctuated below zero. The exponentially falling part
instead describes the tail of distribution for <span class="math notranslate nohighlight">\(\hat{\mu}&gt;0\)</span>.<br />
To get to the <span class="math notranslate nohighlight">\(p\)</span>-value using toys, we had to integrate the histogram
above the observed value of the test statistics. In the approximation of
large sample, using Wald’s theorem, this corresponds to the integral of
Eq.<a class="reference internal" href="#equation-eq-asymptoticdiscovery">(23)</a> above the observed value of the test
statistics. The simple form of the pdf gives an even simpler expression
for the cumulative of Eq.<a class="reference internal" href="#equation-eq-asymptoticdiscoverygen">(22)</a></p>
<div class="math notranslate nohighlight">
\[
F(q_0|\mu') = \Phi\left( \sqrt{q_0} - \frac{\mu'}{\sigma} \right)
\]</div>
<p>which, for <span class="math notranslate nohighlight">\(\mu'=0\)</span> as in Eq.<a class="reference internal" href="#equation-eq-asymptoticdiscovery">(23)</a>, becomes:</p>
<div class="math notranslate nohighlight">
\[
F(q_0|0) = \Phi\left( \sqrt{q_0} \right).
\]</div>
<p>The <span class="math notranslate nohighlight">\(p\)</span>-value can them be
simply computed as $<span class="math notranslate nohighlight">\(p_0 = 1-F(q_0|0)\)</span>$ obtaining for the significance</p>
<div class="math notranslate nohighlight">
\[
Z_0 = \Phi^{-1}(1-p_0) = \sqrt{q_0}.
\]</div>
<p>The signal significance is
simply the square root of the observed test statistics! To fully
appreciate this result, think about the evaluation of the significance
for a <span class="math notranslate nohighlight">\(5\sigma\)</span> signal: using toys you need to produce <span class="math notranslate nohighlight">\(o(10^8)\)</span> data
samples to populate the high tail of <span class="math notranslate nohighlight">\(f(q_0|0)\)</span> to compute the integral
above <span class="math notranslate nohighlight">\(q_0^{obs}\)</span> (and then convert it to a significance); with the
asymptotic to get you just take the square root of <span class="math notranslate nohighlight">\(q_0^{obs}\)</span>: if
<span class="math notranslate nohighlight">\(q_0^{obs} = 25\)</span> you have a <span class="math notranslate nohighlight">\(5\sigma\)</span> deviation!</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p><a class="reference internal" href="#fig-muhff"><span class="std std-numref">Fig. 76</span></a> shows the scan of the test statistics as a
function of <span class="math notranslate nohighlight">\(\mu\)</span>. The minimum is obtained when <span class="math notranslate nohighlight">\(\hat{\mu} = \mu\)</span> and it
is zero by construction of the likelihood ratio. The intercept at
<span class="math notranslate nohighlight">\(\mu=0\)</span> i.e. <span class="math notranslate nohighlight">\(q_0^{obs} = -2\ln(\lambda(0))\)</span> is the square of the
significance. This means that we can read off the vertical axis the
significance of the signal as <span class="math notranslate nohighlight">\(\sqrt{14.25} = 3.8\sigma\)</span>.</p>
</div>
<figure class="align-center" id="fig-muhff">
<a class="reference internal image-reference" href="_images/muHff.png"><img alt="_images/muHff.png" src="_images/muHff.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 76 </span><span class="caption-text">Observation of the Higgs
coupling to fermions.</span><a class="headerlink" href="#fig-muhff" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p><a class="reference internal" href="#fig-hggpvalue"><span class="std std-numref">Fig. 77</span></a> shows results of the <span class="math notranslate nohighlight">\(H\to\gamma\gamma\)</span>
search at CMS using the <span class="math notranslate nohighlight">\(p\)</span>-value computed at each test mass hypothesis.
The black continuous line is the observed <span class="math notranslate nohighlight">\(p\)</span>-value, that is computed on
the asymptotic pdf for <span class="math notranslate nohighlight">\(f(q_0|0)\)</span> using the measured <span class="math notranslate nohighlight">\(q_0^{obs}\)</span>, the
dashed black line represent the expected <span class="math notranslate nohighlight">\(p\)</span>-value for the Standard
Model signal (<span class="math notranslate nohighlight">\(\mu=1\)</span>). The other curves represent the observe and
expected <span class="math notranslate nohighlight">\(p\)</span>-values for different datasets collected by CMS at the LHC
with different centre of mass energies (<span class="math notranslate nohighlight">\(\sqrt{s}=7\)</span> TeV and
<span class="math notranslate nohighlight">\(\sqrt{s}=8\)</span> TeV).</p>
</div>
<figure class="align-center" id="fig-hggpvalue">
<a class="reference internal image-reference" href="_images/Hggpvalue.png"><img alt="_images/Hggpvalue.png" src="_images/Hggpvalue.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 77 </span><span class="caption-text">Expected
(dashed) and observed (continuous) <span class="math notranslate nohighlight">\(p\)</span>-values for the CMS
<span class="math notranslate nohighlight">\(H\to\gamma\gamma\)</span> search. The red curves are computed on the first data
collected in 2011, the blue ones for the data collected in 2012 and the
black curves on the two data set
combined.</span><a class="headerlink" href="#fig-hggpvalue" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="asimov-dataset">
<h3>Asimov dataset<a class="headerlink" href="#asimov-dataset" title="Link to this heading">#</a></h3>
<p>There are cases where you want to have the estimation of the <em>expected</em>
significance of a signal. Typically this happens during the design phase
of an experiment or, after a measurement, when you want to compare the
observed and expected significances. To do this you need to have access
to two pdfs: <span class="math notranslate nohighlight">\(f(q_0|0)\)</span>, the distribution of the test statistics in the
background only hypothesis and <span class="math notranslate nohighlight">\(f(q_0|1)\)</span>, the distribution of the test
statistics in the signal+background hypothesis. In the latter case
<span class="math notranslate nohighlight">\(\mu=1\)</span> indicates the expected value of the signal strength. A sketch of
these two functions is shown in
<a class="reference internal" href="#fig-expectedsignificance"><span class="std std-numref">Fig. 78</span></a>.</p>
<figure class="align-center" id="fig-expectedsignificance">
<a class="reference internal image-reference" href="_images/expectedSignificance.png"><img alt="_images/expectedSignificance.png" src="_images/expectedSignificance.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 78 </span><span class="caption-text">Discovery statistics distribution under
the background only <span class="math notranslate nohighlight">\(f(q_0|0)\)</span> and signal + background <span class="math notranslate nohighlight">\(f(q_0|1)\)</span>.</span><a class="headerlink" href="#fig-expectedsignificance" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We have already encountered above the pdf <span class="math notranslate nohighlight">\(f(q_0|0)\)</span>: the most probable
value of the test statistic is zero and the large tail corresponds to
signal-like fluctuation of the background only hypothesis. The pdf
<span class="math notranslate nohighlight">\(f(q_0|1)\)</span> instead clusters at high values of <span class="math notranslate nohighlight">\(q_0\)</span>. We can understand
this from the definition of the likelihood ratio
<span class="math notranslate nohighlight">\(\lambda(0) = L(0, \hat{\hat{\theta}})/L(\hat{\mu},\hat{\theta})\)</span>. Here
<span class="math notranslate nohighlight">\(\hat{\mu} = 1\)</span> by construction, so the ratio <span class="math notranslate nohighlight">\(\lambda(0)\)</span> will cluster
around small values of the test statistics and consequently
<span class="math notranslate nohighlight">\(-2\ln(\lambda(0))\)</span> will cluster at large ones.</p>
<p>To compute the expected significance of a signal, we compute the
<span class="math notranslate nohighlight">\(p\)</span>-value as the integral from the median of the <span class="math notranslate nohighlight">\(f(q_0|1)\)</span> distribution
to infinity. We use the median as the “most representative” value for
the expected signal+background.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The name comes from the short story “Franchise” from I. Asimov,
where in the far future of 2008 the U.S.A. elections were to be
replaced by the choice of a single citizen chosen by a computer
which would represent the perfect average of the whole population</p>
</aside>
<p>To compute the median value of the test statistics we first need the pdf
from toys or the asymptotic formulas. Can we produce a single dataset
such that if we compute the test statistics on it we get the median
value of the pdf ? This is the idea behind the “Asimov” dataset.
This can be thought as “the perfect average” of the experiments outcome.
To understand how to build this dataset, we can use the prototype
analysis:</p>
<div class="math notranslate nohighlight">
\[
L(\mu,\theta) = \prod_{j=1}^{N} \frac{(\mu s_j + b_j)^{n_j}}{n_j!} e^{-(\mu s_j + b_j)} \prod_{k=1}^{M} \frac{u_k^{m_k}}{m_k!} e^{-u_k}
\]</div>
<p>From here we can find the ML estimator for the parameters as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \ln L}{\partial \theta_j} = \sum_{i=1}^{N} \left( \frac{n_i}{\nu_i}-1  \right) \frac{\partial \nu_i}{\partial \theta_i} + \sum_{i=1}^{M}\left( \frac{m_i}{u_i} -1 \right) \frac{\partial u_i}{\partial \theta_j} = 0
\]</div>
<p>and define the Asimov dataset bin by bin as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
n_{i,A} &amp;=&amp; E[n_i] = \nu_i = \mu' s_i(\theta) + b_i(\theta)\\
m_{i,A} &amp;=&amp; E[m_i] = u_i(\theta).\end{aligned}
\end{split}\]</div>
<p>Each bin of the Asimov
dataset has by construction the number of entries equal to the expected
value, with the correct statistical uncertainty. The only difference
with respect to a toy data set is that there are no statistical
fluctuation associated to the entries per bin.<br />
On the Asimov dataset we can compute the Asimov likelihood and use it to
compute the likelihood ratio:</p>
<div class="math notranslate nohighlight">
\[
\lambda_A(\mu) = \frac{L_A(\mu, \hat{\hat{\theta}})}{L_A(\hat{\mu},\hat{\theta})} \sim \frac{L_A(\mu, \hat{\hat{\theta}})}{L_A(\mu',\theta)}
\]</div>
<p>where by construction of the Asimov, <span class="math notranslate nohighlight">\(\hat{\mu}=\mu'\)</span> and the nuisance
parameters <span class="math notranslate nohighlight">\(\hat{\theta}=\theta\)</span>.</p>
<p>The Asimov can also be used to obtain the width <span class="math notranslate nohighlight">\(\sigma\)</span> for the
approximate formula in Eq.<a class="reference internal" href="#equation-eq-wald">(20)</a>:</p>
<div class="math notranslate nohighlight">
\[
q_\mu = -2\ln\lambda(\mu) = \frac{(\mu-\hat{\mu})^2}{\sigma^2} + o\left( \frac{1}{\sqrt{N}}\right)  \qquad\Rightarrow\qquad q_{\mu,A}  = -2\ln\lambda_A(\mu) \sim \frac{(\mu -\mu')^2}{\sigma^2}.
\]</div>
<p>From here we can extract</p>
<div class="math notranslate nohighlight">
\[
\sigma_A^2 = \frac{(\mu - \mu')^2}{q_{\mu,A}}
\]</div>
<p>Finally, it’s easy to
verify that the test statistic computed on the Asimov dataset coincides
with the median of the distribution <span class="math notranslate nohighlight">\(f(q_\mu|\mu')\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mbox{med}[q_0] = q_0(\mbox{med}[\hat{\mu}]) = \frac{\mu'}{\sigma} = -2\ln \lambda_A(0)
\]</div>
<p>where the first equality comes from the fact that the median of the
<span class="math notranslate nohighlight">\(q_0\)</span> is the value of <span class="math notranslate nohighlight">\(q_0\)</span> computed at the median value of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>,
the second equality comes by construction of the Asimov dataset
<span class="math notranslate nohighlight">\(\mbox{med}[\hat{\mu}] = \mu'\)</span>, <span class="math notranslate nohighlight">\(q_0(\mu') = (0-\mu')^2/\sigma^2\)</span> and
the last one comes from the Wald’s theorem in
Eq.<a class="reference internal" href="#equation-eq-walddiscovery">(21)</a>. The expected significance
<span class="math notranslate nohighlight">\(Z_0 = \sqrt{q_0}\)</span> computed on the Asimov dataset is then simply
<span class="math notranslate nohighlight">\(\mbox{med}[Z_0] = \sqrt{-2\ln\lambda_A(0)}\)</span>.</p>
</section>
<section id="upper-limits-test-statistic">
<h3>Upper limits test statistic<a class="headerlink" href="#upper-limits-test-statistic" title="Link to this heading">#</a></h3>
<p>To set an upper limit we can define the following test statistics:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
q_\mu = \left\{
            \begin{array}{rll}
                -2 \ln \lambda(\mu) &amp; \mbox{if} &amp; \hat{\mu} \le \mu \\
                0                 &amp; \mbox{if} &amp; \hat{\mu} &gt; \mu 
            \end{array}\right.
\end{split}\]</div>
<p>Here we are using the same profile
likelihood ratio as for the discovery test statistics, but this time we
test for a generic <span class="math notranslate nohighlight">\(\mu\)</span>. Notice that that is not the only difference in
the definition. The <span class="math notranslate nohighlight">\(\le\)</span> and <span class="math notranslate nohighlight">\(&gt;\)</span> signs are swapped with respect to
Eq.<a class="reference internal" href="#equation-eq-discovery">(19)</a>. The reason for this is that, given that we
are only considering cases where signal is associated to an excess of
events above the background, we can only exclude an hypothesised value
of <span class="math notranslate nohighlight">\(\mu\)</span> if the observed <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> fluctuates below that. Vice versa,
we cannot exclude a value of <span class="math notranslate nohighlight">\(\mu\)</span> if the observed <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is
measured above the tested value <span class="math notranslate nohighlight">\(\mu\)</span> and so we set the test statistics
to zero.</p>
<p>As we have already done for the discovery test statistics we can define
a <span class="math notranslate nohighlight">\(p\)</span>-value as:</p>
<div class="math notranslate nohighlight">
\[
p_\mu = \int_{q_{\mu,obs}}^\infty f(q_\mu|\mu)dq_\mu
\]</div>
<p>How do we set a upper limit with this test statistics? Suppose you want
to set an upper limit at 95% CL. You need to scan the values of <span class="math notranslate nohighlight">\(\mu\)</span>
until you find the largest value of <span class="math notranslate nohighlight">\(\mu\)</span> such that the <span class="math notranslate nohighlight">\(p\)</span>-value is
equal to 0.05. Practically you will also need to guess what is the range
of <span class="math notranslate nohighlight">\(\mu\)</span> values to scan and what step to use in the scan.<br />
 To set the expected upper limit on the signal strength of a signal you
need first to get the pdf for <span class="math notranslate nohighlight">\(f(q_\mu|\mu')\)</span>. Analogous to what we have
seen with the discovery test statistics the distributions for <span class="math notranslate nohighlight">\(\mu'=\mu\)</span>
and <span class="math notranslate nohighlight">\(\mu'\neq\mu\)</span> are as shown in
<a class="reference internal" href="#fig-expectedupperlimit"><span class="std std-numref">Fig. 79</span></a>. In particular we will need the pdf
<span class="math notranslate nohighlight">\(f(q_\mu|0)\)</span> describing the test statistics in absence of signal, from
which we extract the median value, and then scan the values of <span class="math notranslate nohighlight">\(\mu\)</span>
(same procedure used for the observed) until you find the largest value
of <span class="math notranslate nohighlight">\(\mu\)</span> such that the <span class="math notranslate nohighlight">\(p\)</span>-value is equal to the desired <span class="math notranslate nohighlight">\(\alpha\)</span>=1-CL.</p>
<figure class="align-center" id="fig-expectedupperlimit">
<a class="reference internal image-reference" href="_images/expectedUpperLimit.png"><img alt="_images/expectedUpperLimit.png" src="_images/expectedUpperLimit.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 79 </span><span class="caption-text">Sketch of the pdf for <span class="math notranslate nohighlight">\(f(q_\mu|\mu')\)</span>.</span><a class="headerlink" href="#fig-expectedupperlimit" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Very often when displaying the expected upper limits we also show the
1<span class="math notranslate nohighlight">\(\sigma\)</span> and 2<span class="math notranslate nohighlight">\(\sigma\)</span> uncertainty bands around the expected median. To
do this, from the median value (i.e. 50% quantile) of the distribution
giving the desired <span class="math notranslate nohighlight">\(\alpha\)</span> = 1-CL, i.e. med<span class="math notranslate nohighlight">\([q_\mu|0]\)</span>, we can quote
the “median<span class="math notranslate nohighlight">\(\pm 1 \sigma\)</span>” (16%, 84% quantiles), and the
“median<span class="math notranslate nohighlight">\(\pm 2 \sigma\)</span>” (5%, 95% quantiles), as shown in
<a class="reference internal" href="#fig-expectedbands"><span class="std std-numref">Fig. 80</span></a>.</p>
<figure class="align-center" id="fig-expectedbands">
<a class="reference internal image-reference" href="_images/expectedBands.png"><img alt="_images/expectedBands.png" src="_images/expectedBands.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 80 </span><span class="caption-text">Median (50% quantile) and median <span class="math notranslate nohighlight">\(-1\sigma\)</span> (15.87% quantile).</span><a class="headerlink" href="#fig-expectedbands" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The observed limits together with the expected one and its respective
uncertainty bands are shown as a function of the test mass in
<a class="reference internal" href="#fig-exclusionlimit"><span class="std std-numref">Fig. 81</span></a>.</p>
<figure class="align-center" id="fig-exclusionlimit">
<a class="reference internal image-reference" href="_images/exclusionLimit.png"><img alt="_images/exclusionLimit.png" src="_images/exclusionLimit.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 81 </span><span class="caption-text">Describe the red line as SM limit.</span><a class="headerlink" href="#fig-exclusionlimit" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Using Wald’s theorem the asymptotic formula for the test statistic
becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
q_\mu = \left\{
            \begin{array}{rll}
                -2 \ln \lambda(\mu) &amp; \mbox{if} &amp; \hat{\mu} \le \mu \\
                0                 &amp; \mbox{if} &amp; \hat{\mu} &gt; \mu 
            \end{array}\right.
         \qquad \Rightarrow \qquad
        q_\mu = \left\{
            \begin{array}{rll}
                \frac{(\mu-\hat{\mu})^2}{\sigma^2}&amp; \mbox{if} &amp; \hat{\mu} \le \mu \\
                0                 &amp; \mbox{if} &amp; \hat{\mu} &gt; \mu 
            \end{array}\right.
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is, as for the
discovery test statistics, gaussian distributed around <span class="math notranslate nohighlight">\(\mu'\)</span> with
standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. From this expression, it is possible to
compute the closed form expression for <span class="math notranslate nohighlight">\(f(q_\mu|\mu')\)</span>:</p>
<div class="math notranslate nohighlight">
\[
f(q_\mu|\mu') = \Phi\left(\frac{\mu' - \mu}{\sigma} \right)\delta(q_\mu) + \frac{1}{2}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{q_\mu}} \exp\left[-\frac{1}{2}\left(\sqrt{q_\mu} - \frac{\mu-\mu'}{\sigma} \right)^2\right]
\]</div>
<p>which, for the special case where <span class="math notranslate nohighlight">\(\mu = \mu'\)</span>, becomes:</p>
<div class="math notranslate nohighlight">
\[
f(q_\mu|\mu) = \frac{1}{2}\delta(q_\mu) + \frac{1}{2}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{q_\mu}} e^{-\frac{q_\mu}{2}}
\]</div>
<p>Using the cumulative distribution
<span class="math notranslate nohighlight">\(F(q_\mu|\mu') = \Phi\left(\sqrt{q_\mu}-\frac{\mu - \mu'}{\sigma}\right)\)</span>
we get <span class="math notranslate nohighlight">\(p_\mu = 1-F(q_\mu|\mu') = 1 - \Phi(\sqrt{q_\mu})\)</span>, which, as for
the discovery test statistic, gives:</p>
<div class="math notranslate nohighlight">
\[
Z_\mu = \Phi^{-1}(1-p_\mu)=\sqrt{q_\mu}
\]</div>
<p>The upper limit at
<span class="math notranslate nohighlight">\((1-\alpha)\)</span> CL on <span class="math notranslate nohighlight">\(\mu\)</span> is the largest value of <span class="math notranslate nohighlight">\(\mu\)</span> such that
<span class="math notranslate nohighlight">\(p_\mu\leq \alpha\)</span>. With the asymptotic formulas we just need to set
<span class="math notranslate nohighlight">\(p_\mu = \alpha\)</span> and solve
<span class="math notranslate nohighlight">\(\mu_{up} = \hat{\mu} + \sigma\Phi^{-1}(1-\alpha)\)</span>.</p>
</section>
</section>
<section id="combining-measurements">
<h2>Combining measurements<a class="headerlink" href="#combining-measurements" title="Link to this heading">#</a></h2>
<p>As we have already seen in Sec.<a class="reference internal" href="#likelihood.html#combination-of-measurements-with-the-ml-method"><span class="xref myst">Likelihood:Combining measurements</span></a>, the results of different measurements of a
given parameter <span class="math notranslate nohighlight">\(\mu\)</span> can easily be combined by multiplying their
likelihoods:</p>
<div class="math notranslate nohighlight">
\[
L(\mu,\vec{\theta}) = \prod_i L_i(\mu, \vec{\theta}_i)
\]</div>
<p>where the subscript <span class="math notranslate nohighlight">\(i\)</span> stands for the different experiments (e.g.
ATLAS, CMS) or different processes (e.g. <span class="math notranslate nohighlight">\(H\to \gamma\gamma\)</span>, <span class="math notranslate nohighlight">\(H\to ZZ\)</span>,
etc…).</p>
<p>Whenever different measurements are combined we need to pay attention at
the possible correlations between the nuisances among the different
experiments/channels. Typically the nuisances are chosen to be:</p>
<ul class="simple">
<li><p>uncorrelated: we use different parameters in the likelihoods of the
different experiments to describe the nuisance</p></li>
<li><p>fully correlated: we use the same parameter in all the likelihoods
to describe the nuisance</p></li>
<li><p>fully anticorrelated: we use again one parameter but we flip its
sign.</p></li>
</ul>
<p>To compute the expected significance for the combination the easiest
approach is to use the Asimov dataset:</p>
<div class="math notranslate nohighlight">
\[
\lambda_A(\mu) = \prod_i \lambda_{A,i}(\mu) \qquad \mbox{where}\qquad \lambda_{A,i}(\mu) = \frac{L_{A,i}(\mu, \hat{\hat{\theta}})}{L_{A,i}(\hat{\mu},\hat{\theta})} = \frac{L_{A,i}(\mu, \hat{\hat{\theta}})}{L_{A,i}(\hat{\mu'},\hat{\theta})}.
\]</div>
<p>where the last equality comes from the properties of the Asimov dataset
(<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> converges by construction to <span class="math notranslate nohighlight">\(\mu'\)</span>).</p>
</section>
<section id="discovery-significance-s-sqrt-b">
<h2>Discovery significance: <span class="math notranslate nohighlight">\(S/\sqrt{B}\)</span><a class="headerlink" href="#discovery-significance-s-sqrt-b" title="Link to this heading">#</a></h2>
<p>Consider a counting experiment where you observe <span class="math notranslate nohighlight">\(n\)</span> events, the
expected number of background events is <span class="math notranslate nohighlight">\(b\)</span> and the expected number of
events in case of signal is <span class="math notranslate nohighlight">\(s+b\)</span>. To simplify let’s first consider the
case of large statistics such that the we can approximate the Poisson
distribution with a Gaussian <span class="math notranslate nohighlight">\(G(x|\mu, \sigma)\)</span> with <span class="math notranslate nohighlight">\(\mu = s+b\)</span> and
<span class="math notranslate nohighlight">\(\sigma = \sqrt{s+b}\)</span> (the fact that the n is a discrete variable, wile
x is continuous is irrelevant in this context).</p>
<p>The significance to reject the background hypothesis can be quantified
as the <span class="math notranslate nohighlight">\(p\)</span>-value associated to the observation of <span class="math notranslate nohighlight">\(x\)</span> events
Prob<span class="math notranslate nohighlight">\((x&gt;x_{obs} | s = 0)\)</span> which, for a gaussian, is simply
<span class="math notranslate nohighlight">\(p_0 = 1-\Phi((x_{obs}-b)/\sqrt{b})\)</span>. The significance is then
<span class="math notranslate nohighlight">\(Z_0 = \Phi^{-1}(1-p_0) = (x_{obs} - b)/\sqrt(b)\)</span>.</p>
<p>The median significance to for a signal <span class="math notranslate nohighlight">\(s\neq0\)</span> can be computed in the
same way replacing <span class="math notranslate nohighlight">\(x_{obs}\)</span> with the median signal plus background i.e.
<span class="math notranslate nohighlight">\(s+b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mbox{median}[Z_0 | s+b] = \frac{s+b-b}{\sqrt{b}} = \frac{s}{\sqrt{b}}
\]</div>
<p>giving the famous formula for the the signal significance. This is the
typical quantity we try to maximize when optimizing a selection in a
search.<br />
This formula is only valid in the limit of large statistics. For the
most general case we need to go back to the Poisson distribution:</p>
<div class="math notranslate nohighlight">
\[
L(n|s+b) = \mbox{Poisson}(n|s+b) = \frac{(s+b)^n e^{-(s+b)}}{n!}
\]</div>
<p>or taking the logarithm</p>
<div class="math notranslate nohighlight">
\[
\ln L(n|s+b) = n\ln(s+b) -(s+b) -\ln n!
\]</div>
<p>Recalling that the ML estimator for <span class="math notranslate nohighlight">\(s\)</span> is simply <span class="math notranslate nohighlight">\(\hat{s} = n-b\)</span>, we
can build the test statistic:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
q_0 = \left\{
            \begin{array}{lll}
                -2 \ln \lambda(0)   = -2\ln \frac{L(n|0+\hat{b})}{L(n|\hat{s}+\hat{b})}  = 2( n \ln \frac{n}{b} + b -n) &amp; \mbox{if} &amp; n &gt; b \\
                0                 &amp; \mbox{if} &amp; n \leq b 
            \end{array}\right.
\end{split}\]</div>
<p>Now we can use Wald’s theorem and
rewrite the significance to reject the background only hypothesis as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z_0 \sim \left\{
            \begin{array}{lll}
                \sqrt{q_0} = \sqrt{2( n \ln \frac{n}{b} + b -n)} &amp; \mbox{if} &amp; n &gt; b \\
                0                 &amp; \mbox{if} &amp; n \leq b 
            \end{array}\right.
\end{split}\]</div>
<p>The median significance from the
expected signal plus background <span class="math notranslate nohighlight">\(s+b\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mbox{median}[Z_0 | s+b] = \sqrt{2( (s+b) \ln (s/b +1) -s)}.
\]</div>
<p>This formula can be considered as a generalization of <span class="math notranslate nohighlight">\(s/\sqrt{b}\)</span>. If we
expand this result to the second order in the limit of <span class="math notranslate nohighlight">\(s &lt;&lt; b\)</span> we get
back to <span class="math notranslate nohighlight">\(s/\sqrt{b}\)</span>. This is an important condition to keep in mind
when using <span class="math notranslate nohighlight">\(s/\sqrt{b}\)</span>. If <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are both large, then the
<span class="math notranslate nohighlight">\(p\)</span>-value goes to zero! The <span class="math notranslate nohighlight">\(p\)</span>-value is the probability to observe a
fluctuation as large or larger than the one observed, if both <span class="math notranslate nohighlight">\(s\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> are large that probability is vanishingly small.</p>
</section>
<section id="examples-from-the-search-of-the-higgs-at-the-lhc">
<h2>Examples from the search of the Higgs at the LHC<a class="headerlink" href="#examples-from-the-search-of-the-higgs-at-the-lhc" title="Link to this heading">#</a></h2>
<p>In this section we will walk through some results from search for the
Higgs boson at the LHC, taking the <span class="math notranslate nohighlight">\(H\to \gamma\gamma\)</span> as the
conceptually easy example of a search of a “bump” on top of a falling
background. All plots are taken from the public CMS results <span id="id7">[<a class="reference internal" href="bibliography.html#id26" title="CMS. Public results. URL: https://cms-results-search.web.cern.ch/.">CMS</a>]</span></p>
<section id="best-fit-signal-strength">
<h3>Best fit signal strength<a class="headerlink" href="#best-fit-signal-strength" title="Link to this heading">#</a></h3>
<p>The plot in <a class="reference internal" href="#fig-bestfitmuhat"><span class="std std-numref">Fig. 82</span></a> shows the best fit of the signal strength
modifier <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> as a function of the test mass <span class="math notranslate nohighlight">\(m_H\)</span>. The value
obtained fluctuates around <span class="math notranslate nohighlight">\(\hat{\mu}\sim 0\)</span> everywhere in but in the
region around 125 GeV where the excess was observed in the exclusion
limits (<a class="reference internal" href="#fig-exclusionlimit"><span class="std std-numref">Fig. 81</span></a>) and in the <span class="math notranslate nohighlight">\(p\)</span>-value plot in
<a class="reference internal" href="#fig-hggpvalue"><span class="std std-numref">Fig. 77</span></a>. In that region, the signal strength steeply
raise to a value compatible with the Standard Model expectation of
<span class="math notranslate nohighlight">\(\mu=1\)</span>. The green band represent the uncertainty on <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> defined
on the likelihood ratio as
<span class="math notranslate nohighlight">\(-2\ln \lambda(\mu) = -2 \ln L(\mu) / L(\hat{\mu}) &lt;1\)</span> which is
equivalent to the familiar 68% uncertainty band on a fitted parameter in
a maximum likelihood fit <span class="math notranslate nohighlight">\(\ln L(\mu) &gt; \ln L(\hat{\mu}) - 1/2\)</span>.</p>
<figure class="align-center" id="fig-bestfitmuhat">
<a class="reference internal image-reference" href="_images/bestfitmuhat.png"><img alt="_images/bestfitmuhat.png" src="_images/bestfitmuhat.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 82 </span><span class="caption-text">Best fit of the signal strength <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> as a function of the test
mass <span class="math notranslate nohighlight">\(m_H\)</span>.</span><a class="headerlink" href="#fig-bestfitmuhat" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="extracting-other-parameters">
<h3>Extracting other parameters<a class="headerlink" href="#extracting-other-parameters" title="Link to this heading">#</a></h3>
<p>Up to now we have considered only the case where the parameter of
interested in our likelihood was the signal strength. Given that all
parameters in the likelihood are treated in the same way, we can use the
same techniques to extract information about any other parameter. To
study the properties of the Higgs boson, we rewrite the signal “<span class="math notranslate nohighlight">\(s\)</span>” as
a function of the parameter we are interested in, let’s call it “<span class="math notranslate nohighlight">\(a\)</span>”,
and plug <span class="math notranslate nohighlight">\(s(a)\)</span> in the likelihood. Then all we need to do is to rewrite
the test statistics as:</p>
<div class="math notranslate nohighlight">
\[
q(a) = -2 \ln \frac{L(\mbox{data}|s(a)+b, \hat{\theta}_a)}{L(\mbox{data}|s(\hat{a})+b, \hat{\theta})}.
\]</div>
<p>As in the previous definition of the test statistics, the profile
likelihood at the numerator is maximized fixing <span class="math notranslate nohighlight">\(a\)</span> and floating
<span class="math notranslate nohighlight">\(\hat{\theta}_a\)</span>, the value of the nuisance parameters once <span class="math notranslate nohighlight">\(a\)</span> is fixed
(i.e. the nuisances are profiled as before), and, at the denominator,
the likelihood is maximized against both <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>. As before the
68% and 95% CL interval on <span class="math notranslate nohighlight">\(a\)</span> is evaluated from <span class="math notranslate nohighlight">\(q(a) = 1~(4)\)</span> with all
other unconstrained parameters treated as nuisance parameters.<br />
The same idea can be used to scan simultaneously two parameters of
interest <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. In this case the 68% and 95% CL interval becomes a
2D region such that <span class="math notranslate nohighlight">\(q(a,b) = 2.3~(6)\)</span>. It is important to remember that
the boundaries of the 2D confidence regions projected on either
parameter axis are not necessarily identical to the 1D confidence
interval for that parameter, because of the possible correlations
between the two (I’m integrating on all the other variables I’m not
considering). Examples of 1D and 2D parameter scans are shown in
<a class="reference internal" href="#fig-scans1d2d"><span class="std std-numref">Fig. 83</span></a>. All the properties of the Higgs boson are
extracted from data using this procedure.</p>
<figure class="align-center" id="fig-scans1d2d">
<a class="reference internal image-reference" href="_images/scans1D2D.png"><img alt="_images/scans1D2D.png" src="_images/scans1D2D.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 83 </span><span class="caption-text">Examples of
parameters scans in 1D (left: <span class="math notranslate nohighlight">\(m_H\)</span>) and 2D (right: <span class="math notranslate nohighlight">\((\mu,m_H)\)</span>).</span><a class="headerlink" href="#fig-scans1d2d" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="bayesian-approach-to-upper-limits">
<h2>Bayesian approach to upper limits<a class="headerlink" href="#bayesian-approach-to-upper-limits" title="Link to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The initial Higgs results were indeed verified using both approaches.</p>
</aside>
<p>The standard prescription to present the LHC results is based on the
frequentist paradigma. Nevertheless it is interesting to see how to
extract the same results using the bayesian statistics. Using Bayes
theorem we can write the posterior for the signal strength as:</p>
<div class="math notranslate nohighlight">
\[
f(\mu) = f(\mu|\mbox{data}) = \int\frac{1}{N}L(\mbox{data}|\mu,\vec{\theta})\pi(\vec{\theta})d\vec{\theta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi\)</span> is the prior function describing the nuisance parameters and
N is a normalization factor to have <span class="math notranslate nohighlight">\(\int f(\mu)d\mu\)</span> =1<br />
The prior presents the usual issues, and it is usually chosen to be flat
in the parameter of interest <span class="math notranslate nohighlight">\(\mu\)</span> (assume “total ignorance”). To find
the upper limit on <span class="math notranslate nohighlight">\(\mu\)</span> at the <span class="math notranslate nohighlight">\(1-\alpha\)</span> CL (credible level) we need
to solve numerically for <span class="math notranslate nohighlight">\(\mu_{1-\alpha}\)</span> in</p>
<div class="math notranslate nohighlight">
\[
\int_0^{\mu_{1-\alpha}} f(\mu) d\mu = 1-\alpha
\]</div>
<p>see <a class="reference internal" href="#fig-bayeshiggs"><span class="std std-numref">Fig. 84</span></a>. In general <span class="math notranslate nohighlight">\(\mu\)</span> is a function of the test
mass <span class="math notranslate nohighlight">\(\mu = \mu(m_H)\)</span> and to obtain the exclusion plot as in
<a class="reference internal" href="#fig-exclusionlimit"><span class="std std-numref">Fig. 81</span></a>, we will need to compute this integral
for each value of the test mass.</p>
<figure class="align-center" id="fig-bayeshiggs">
<a class="reference internal image-reference" href="_images/bayesHiggs.png"><img alt="_images/bayesHiggs.png" src="_images/bayesHiggs.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 84 </span><span class="caption-text">FIXME Thesis G. Petrucciani.</span><a class="headerlink" href="#fig-bayeshiggs" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="look-elsewhere-effect">
<h2>Look-Elsewhere Effect<a class="headerlink" href="#look-elsewhere-effect" title="Link to this heading">#</a></h2>
<p>Before formalizing the Look-Elsewhere Effect (LEE) for a HEP search,
let’s consider this easy example:</p>
<div class="tip admonition">
<p class="admonition-title">Example:</p>
<p>In a city the average number of accidents per day is
<span class="math notranslate nohighlight">\(7\pm 1\)</span>. When looking at the number of accidents with full moon, the
number is 10 which is <span class="math notranslate nohighlight">\(3\sigma\)</span> from the average. Is this sufficient to
claim that full moon has an influences on the number of incidents? To
answer this question we need to consider the fact that this result was
obtained looking at the statistics of this one city. To have a better
understanding of the phenomenon we would need to verify it on a larger
number of cities. If we repeat the observation on 100 cities, then you
would expect that at least one shows a <span class="math notranslate nohighlight">\(3\sigma\)</span> deviation. The question
becomes how many cities above 3<span class="math notranslate nohighlight">\(\sigma\)</span> should I observe to convince
myself of a supernatural influence of the full moon on the drivers
capabilities ?</p>
</div>
<p>Back to HEP: consider the search for a resonance in an invariant mass
distribution. In case you know that the resonance is expected at
<span class="math notranslate nohighlight">\(m = m_0\)</span> (e.g. because you have a theoretical prediction about its
position) you can build the discovery test statistics for this fixed
mass hypothesis:</p>
<div class="math notranslate nohighlight">
\[
t_{\mbox{local}} = -2 \ln \frac{L(0)}{L(\hat{\mu}, m_0)}.
\]</div>
<p>Notice that there is no reference to the mass position <span class="math notranslate nohighlight">\(m_0\)</span> at the numerator
because we’re considering the case where there is no signal (and so no
need to worry about its position). To measure the level of compatibility
of your data with the background only hypothesis you can compute the
<span class="math notranslate nohighlight">\(p\)</span>-value:</p>
<div class="math notranslate nohighlight">
\[
p_{\mbox{local}}=\int_{t_{\mbox{local}}}^\infty f(t_{\mbox{local}} | 0) dt_{\mbox{local}}
\]</div>
<p>This is what generally goes under the name of “local” <span class="math notranslate nohighlight">\(p\)</span>-value (often
indicated with <span class="math notranslate nohighlight">\(p_0\)</span>).</p>
<p>If instead you don’t know where the peak will appear (a much more
frequent situation) the <span class="math notranslate nohighlight">\(p\)</span>-value we are interested in is the one which
tells the probability to observe a fluctuation <em>anywhere</em> in the
experimentally accessible mass range. The goal is to take care of the
trivial fact that with a large enough dataset and a large enough number
of bins, we are bound to find a deviation from the background only
hypothesis somewhere because of statistical fluctuations. To take this
into account the position of the resonance <span class="math notranslate nohighlight">\(m_0\)</span> is replaced by an
adjustable parameter in the likelihood:</p>
<div class="math notranslate nohighlight">
\[
t_{\mbox{global}} = -2 \ln \frac{L(0)}{L(\hat{\mu}, \hat{m})}
\]</div>
<p>the denominator is allowed to fit the strength parameter (<span class="math notranslate nohighlight">\(\hat{\mu}\)</span>)
anywhere (<span class="math notranslate nohighlight">\(\hat{m}\)</span>). The corresponding <span class="math notranslate nohighlight">\(p\)</span>-value is:</p>
<div class="math notranslate nohighlight">
\[
p_{\mbox{global}}=\int_{t_{\mbox{global}}}^\infty f(t_{\mbox{global}} | 0) dt_{\mbox{global}}.
\]</div>
<p>In order to compute this integral, as usual, we need to find the p.d.f.
<span class="math notranslate nohighlight">\(f(t_{\mbox{global}} | 0)\)</span>. To do this we can proceed brute force
tossing toy experiments. This is a particularly computing intensive
approach, since we need to scan both the signal strength and the bump
position. To overcome this issue it would be tempting to use the
asymptotic formulas based on Wald’s theorem we discussed in the previous
sections. Unfortunately the Wald’s theorem only works under some
“regularity conditions” which require to have the same parameter of
interest appearing both in the null and the alternative hypotheses. In
our case the alternative hypothesis (denominator) has both the position
<span class="math notranslate nohighlight">\(m_0\)</span> of the excess and its signal strength <span class="math notranslate nohighlight">\(\mu\)</span>, while the null
hypothesis (numerator) has only the signal strength <span class="math notranslate nohighlight">\(\mu = 0\)</span> because
the position of the possible excess is not defined in absence of a
signal.</p>
<p>A solution to this problem has been developed in <span id="id8">[<a class="reference internal" href="bibliography.html#id18" title="Eilam Gross and Ofer Vitells. Trial factors for the look elsewhere effect in high energy physics. The European Physical Journal C, 70(1-2):525–530, oct 2010. URL: https://doi.org/10.1140%2Fepjc%2Fs10052-010-1470-8, doi:10.1140/epjc/s10052-010-1470-8.">GV10</a>]</span>. The main
idea behind the method is to first compute the <span class="math notranslate nohighlight">\(p_{local}\)</span> as if the
position of the mass was known and then apply a correction factor to
bring it to <span class="math notranslate nohighlight">\(_{global}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p_{global} \sim p_{local}+ \langle N(t_{local}) \rangle
\]</div>
<p>The factor <span class="math notranslate nohighlight">\(\langle N(t_{local}) \rangle\)</span> is the mean number of times the
statistics <span class="math notranslate nohighlight">\(-2\ln L\)</span> cross the <span class="math notranslate nohighlight">\(t_{local}\)</span> threshold from below (slang:
“up-crossings”). To understand what this is, let’s take the example of
the original paper in <a class="reference internal" href="#fig-lee"><span class="std std-numref">Fig. 85</span></a></p>
<figure class="align-center" id="fig-lee">
<a class="reference internal image-reference" href="_images/LEE.png"><img alt="_images/LEE.png" src="_images/LEE.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 85 </span><span class="caption-text">(top) An example
pseudo-experiment with background only. The solid line shows the best
signal fit, while the dotted line shows the background fit. (bottom) The
likelihood ratio test statistic <span class="math notranslate nohighlight">\(t_{global}\)</span>. The dotted line marks the
reference level <span class="math notranslate nohighlight">\(t_{local}\)</span> with the up-crossings marked by the dark
dots.</span><a class="headerlink" href="#fig-lee" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The top part of the figure shows a mass spectrum generated on the
background only hypothesis. Around 25 there is a hint of an excess,
while around 50 and 70 there is an hint of a deficit. The test
statistics will be larger the larger the discrepancy of the data from
the background only model (the denominator will get the value preferred
by the data, while the numerator will adjust the background shape
forcing the signal strength to zero). This behavior is clearly visible
in the bottom part of the figure, where the three hints of discrepancy
manifests themselves as bumps in the test statistics. The procedure
would now consist in generating toy experiments and for each of them
count how often the <span class="math notranslate nohighlight">\(t_{global}\)</span> test statistics up-cross the value of
<span class="math notranslate nohighlight">\(t_{local}\)</span> we observe in data and average this number on the total
number of toy-experiments. Up to this point the procedure suffers from
the same limitation as the brute force approach to generate
toy-experiments to populate the p.d.f. for <span class="math notranslate nohighlight">\(t_{global}\)</span>: if we have a
large fluctuation in data (remember we’re thinking about a discovery)
then <span class="math notranslate nohighlight">\(t_{local}\)</span> is going to be very high and only very few toys will
up-cross that high threshold (hence the need of generating a large
number of toy experiments). The idea of the paper is that one can
compute <span class="math notranslate nohighlight">\(\langle N(t_{local}) \rangle\)</span> for a small value of <span class="math notranslate nohighlight">\(t_{local}\)</span>
and then estimate what <span class="math notranslate nohighlight">\(\langle N(t_{local}) \rangle\)</span> would be at any
other value of <span class="math notranslate nohighlight">\(t_{local}\)</span> by:</p>
<div class="math notranslate nohighlight">
\[
\langle N(t_{local}) \rangle = \langle N(t_{local_0}) \rangle e^{-\frac{(t_{local} - t_{local_0})}{2}}
\]</div>
<p>The advantage of this formula is that you can compute
<span class="math notranslate nohighlight">\(\langle N(t_{local}) \rangle\)</span> with a very small number of
toy-experiments and then propagate the result.</p>
<p>The example treated in this section refers to the most common case of
the search of a resonance appearing as an excess in a mass distribution.
The same ideas obviously can be applied to any other search where
instead of a bump in a mass spectrum the deviation from the background
only hypothesis appears in another variable or in a more general case in
a set of variables (e.g. search for an excess where you don’t know
neither the position nor the width).</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>Most of the material of this section is taken from:</p>
<ul class="simple">
<li><p>R. Barlow, <span id="id9">[<a class="reference internal" href="bibliography.html#id8" title="Roger Barlow. Statistics - A guide to the use of statistical methods in the physical sciences. Wiley, 1989. URL: https://www.wiley.com/en-us/Statistics:+A+Guide+to+the+Use+of+Statistical+Methods+in+the+Physical+Sciences-p-9780471922957.">Bar89</a>]</span>, “ A guide to the use of statistical methods in the physical sciences”. Ch. 7</p></li>
<li><p>G. Feldman and R. Cousins, <span id="id10">[<a class="reference internal" href="bibliography.html#id15" title="Gary J. Feldman and Robert D. Cousins. Unified approach to the classical statistical analysis of small signals. Physical Review D, 57(7):3873–3889, apr 1998. URL: https://doi.org/10.1103%2Fphysrevd.57.3873, doi:10.1103/physrevd.57.3873.">FC98</a>]</span>, “Unified approach to the
classical statistical analysis of small signals.”</p></li>
<li><p>A. L. Read, <span id="id11">[<a class="reference internal" href="bibliography.html#id16" title="Alexander L. Read. Modified frequentist analysis of search results (The CL(s) method). In Workshop on Confidence Limits, 81–101. 8 2000.">Rea00</a>]</span>, “Modified frequentist analysis in search results (CLs
method)”</p></li>
<li><p>G. Cowan, K. Cranmer, E. Gross, O. Vitells, <span id="id12">[<a class="reference internal" href="bibliography.html#id17" title="Glen Cowan, Kyle Cranmer, Eilam Gross, and Ofer Vitells. Asymptotic formulae for likelihood-based tests of new physics. The European Physical Journal C, feb 2011. URL: https://doi.org/10.1140%2Fepjc%2Fs10052-011-1554-0, doi:10.1140/epjc/s10052-011-1554-0.">CCGV11</a>]</span>, “Asymptotic formulae for
likelihood-based tests of new physics”</p></li>
<li><p>E. Gross and O. Vitells, <span id="id13">[<a class="reference internal" href="bibliography.html#id18" title="Eilam Gross and Ofer Vitells. Trial factors for the look elsewhere effect in high energy physics. The European Physical Journal C, 70(1-2):525–530, oct 2010. URL: https://doi.org/10.1140%2Fepjc%2Fs10052-010-1470-8, doi:10.1140/epjc/s10052-010-1470-8.">GV10</a>]</span>, “Trial factors for the look elsewhere effect in
high energy physics”</p></li>
</ul>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="hypothesisTesting.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Hypotheses Testing</p>
      </div>
    </a>
    <a class="right-next"
       href="interactive-nbs/Poisson_CI.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Poisson Confidence Intervals</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-belt-neyman-frequentist-construction">Confidence belt - Neyman/Frequentist construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-likelihood-or-the-chi-2-to-set-confidence-intervals">Use the likelihood or the <span class="math notranslate nohighlight">\(\chi^2\)</span> to set confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limits-near-boundaries">Limits near boundaries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feldman-cousins">Feldman-Cousins</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-flip-flop-problem">The flip-flop problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-with-background">Poisson with background</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-with-boundary-at-the-origin">Gaussian with boundary at the origin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neutrino-oscillations">Neutrino oscillations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-examples">Other examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#underfluctuations-and-significance">Underfluctuations and significance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lep-test-statistic-l-s-b-l-b">LEP test statistic: <span class="math notranslate nohighlight">\(L_{s+b}/L_b\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nuisance-parameters">Nuisance parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-issue-of-sensitivity-and-the-cls-procedure">The issue of sensitivity and the CLs procedure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lhc-test-statistics-2-ln-lambda-mu">LHC test statistics: <span class="math notranslate nohighlight">\(-2\ln(\lambda(\mu))\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profile-likelihood-ratio">Profile likelihood ratio</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discovery-test-statistics">Discovery test statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-formulas">Asymptotic Formulas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asimov-dataset">Asimov dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-limits-test-statistic">Upper limits test statistic</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-measurements">Combining measurements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#discovery-significance-s-sqrt-b">Discovery significance: <span class="math notranslate nohighlight">\(S/\sqrt{B}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-from-the-search-of-the-higgs-at-the-lhc">Examples from the search of the Higgs at the LHC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-fit-signal-strength">Best fit signal strength</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extracting-other-parameters">Extracting other parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approach-to-upper-limits">Bayesian approach to upper limits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#look-elsewhere-effect">Look-Elsewhere Effect</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mauro Donega
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>