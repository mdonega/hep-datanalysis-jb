{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence Intervals\n",
    "====================\n",
    "\n",
    "We have seen in previous chapters how to estimate the parameters of a\n",
    "p.d.f. fitting them from data (\"point estimation\") and how to get their\n",
    "uncertainties as the covariance matrix. If the estimator is gaussian\n",
    "distributed then the uncertainty is just given in terms of the \"standard\n",
    "deviation\".\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "A certain manufacturer produces silicon wafers with\n",
    "thickness of $500~\\mu m \\pm 5 \\mu m$. Assuming that the production\n",
    "process gives gaussian distributed thicknesses, we can read the\n",
    "uncertainty as a way to communicate that 68% of the wafers will have a\n",
    "thickness between $495 \\mu m$ and $505 \\mu m$. If you say that the\n",
    "thickness of the sensor is between $495 \\mu m$ and $505 \\mu m$ you are\n",
    "correct 68% of the times: you're making a 68% confidence level (CL)\n",
    "statement. The larger the CL you choose (95%, 99%,\\...), the wider the\n",
    "interval is going to be(490-510$\\mu m$, 485-515$\\mu m$,\\...).\n",
    "```\n",
    "\n",
    "In the general case, when the distribution of the estimator is not\n",
    "gaussian, the statistical uncertainty is reported as **confidence\n",
    "intervals** at a given **confidence level**.\n",
    "\n",
    "The choice of the interval you quote is matter of choice:\n",
    "\n",
    "-   symmetric intervals $\\mu - x_- = \\mu + x_+$\n",
    "\n",
    "-   shortest interval $min_{|x_- - x_+|} (x_-,x_+)$\n",
    "\n",
    "-   central $\\int_{-\\infty}^{x_-} P(x)dx = \\int_{x_+}^{+\\infty} P(x)dx$\n",
    "\n",
    "-   \\...\n",
    "\n",
    "In the gaussian case all the intervals above coincide; for a generic\n",
    "distribution, that is not necessarily the case (see  {numref}`fig:asymIntervals`\n",
    "\n",
    "```{figure} ./images/ch9/asymIntervals.png\n",
    "---\n",
    "width: 800px\n",
    "align: center\n",
    "name: fig:asymIntervals\n",
    "---\n",
    "Example of intervals on an asymmetric\n",
    "p.d.f.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence belt - Neyman/Frequentist construction\n",
    "-------------------------------------------------\n",
    "\n",
    "The general way to communicate the statistical uncertainty on a\n",
    "measurement is to give a confidence interval. In this section we will\n",
    "describe the frequentist/classical construction given by Neyman in\n",
    "1937.\n",
    "\n",
    "Before going to the formal construction let's take a look at an example\n",
    "to show a common pitfall.\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "The weight of an empty dish is measured to be\n",
    "$25.30 \\pm 0.14$ g. A sample of powder is placed on the dish, and the\n",
    "combined weight measured as $25.50 \\pm 0.14$ g. By subtraction and error\n",
    "propagation the weight of the powder is $0.20 \\pm 0.20$ g. This is a\n",
    "perfectly sensible results. However, look at what happens to the\n",
    "probabilities. The naive statement now says that there is a 32% chance\n",
    "of the weight being more than $1~\\sigma$ away from the mean, which is\n",
    "evenly split, making a 16% chance that the weight is negative\n",
    "! [@Barlow]\n",
    "```\n",
    "\n",
    "The general issue addressed by the construction of the confidence belt\n",
    "is how to turn the knowledge from a measurement $x\\pm \\sigma$ into a\n",
    "statement about the value X of the random variable.\n",
    "\n",
    "Suppose you have a set of measurements $\\{x_1,\\ldots,x_n\\}$ and you use\n",
    "them to compute the observed value of an estimator\n",
    "$\\hat{\\theta}(x_1,\\ldots,x_n) = \\hat{\\theta}_{obs}$. Suppose also that\n",
    "given any \"true value\" of $\\theta$ you are able to compute the pdf of\n",
    "$\\hat{\\theta}$: $g(\\hat{\\theta},\\theta)$. From the p.d.f.\n",
    "$g(\\hat{\\theta},\\theta)$ (see {numref}`fig:pdf`) we can determine two values $u_\\alpha$ and\n",
    "$v_\\beta$ such that there is a probability $\\alpha$ to observe\n",
    "$\\hat{\\theta} \\geq u_\\alpha$ and $\\hat{\\theta} \\leq v_\\beta$.\n",
    "\n",
    "```{figure} ./images/ch9/pdf.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:pdf\n",
    "---\n",
    "Example of $g(\\hat{\\theta},\\theta)$.\n",
    "```\n",
    "\n",
    "The values of $u_\\alpha$, $v_\\beta$ depends on the true value of\n",
    "$\\theta$ and, because we know the p.d.f. $g(\\hat{\\theta},\\theta)$, we\n",
    "can compute them inverting: $$\\label{eq:alpha}\n",
    "\\alpha = P(\\hat{\\theta} \\geq u_\\alpha(\\theta)) = \\int_{u_\\alpha(\\theta)}^\\infty g(\\hat{\\theta},\\theta) d\\hat{\\theta}$$\n",
    "\n",
    "```{math}\n",
    "\\beta = P(\\hat{\\theta} \\leq v_\\beta(\\theta)) = \\int_{-\\infty}^{v_\\beta(\\theta)}g(\\hat{\\theta},\\theta) d\\hat{\\theta}\n",
    "```\n",
    "\n",
    "We can now \"build horizontally\" as in in {numref}`fig:belt`.\n",
    "\n",
    "```{figure} ./images/ch9/belt.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:belt\n",
    "---\n",
    "Example of how to built \"horizontally\" a confidence belt.\n",
    "```\n",
    "\n",
    "For each value of $\\theta$ on the y-axis you compute the boundaries\n",
    "$u_\\alpha$, $v_\\beta$. By scanning the whole y-axis you obtain two\n",
    "curves $u_\\alpha(\\theta)$ and $v_\\beta(\\theta)$. The region in between\n",
    "the two curves is called **confidence belt**. By construction, for any\n",
    "value of $\\theta$ the probability for the estimator to be inside the\n",
    "belt is\n",
    "\n",
    "$$\n",
    "P(v_\\beta(\\theta) \\leq \\hat{\\theta} \\leq u_\\alpha(\\theta)) < 1 -\\alpha - \\beta.\n",
    "$$\n",
    "\n",
    "Now we can \"read the plot vertically\" (see\n",
    "{numref}`fig:readthebelt`: take your data and compute the observed\n",
    "value $\\hat{\\theta}_{obs}$. Now take that value and read off the plot on\n",
    "the y-axis $a(\\hat{\\theta})$ and $b(\\hat{\\theta})$. The interval $[a,b]$\n",
    "is the **confidence interval** at a **confidence level**\n",
    "$1-\\alpha - \\beta$. Note that you're making a statement about *the\n",
    "interval you choose, not on the value of $\\theta$*.\n",
    "\n",
    "```{figure} ./images/ch9/readthebelt.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:readthebelt\n",
    "---\n",
    "Example of how to read \"vertically\" a confidence belt.\n",
    "```\n",
    "\n",
    "Formally you need to require that the two functions $u_\\alpha(\\theta)$\n",
    "and $v_\\beta(\\theta)$ are monotonically increasing (which is generally\n",
    "the case for well behaved estimators) and so you can invert them:\n",
    "$$a(\\hat{\\theta}) = u^{-1}_\\alpha(\\hat{\\theta}) \\qquad ; \\qquad  b(\\hat{\\theta}) = v^{-1}_\\beta(\\hat{\\theta})$$\n",
    "If that is the case then you can translate the inequalities\n",
    "\n",
    "```{math}\n",
    ":label: eq:alpha\n",
    "\\hat{\\theta} \\geq u_\\alpha(\\theta) \\qquad ; \\qquad \\hat{\\theta} \\leq v_\\beta(\\theta)\n",
    "```\n",
    "\n",
    "into\n",
    "\n",
    "```{math}\n",
    ":label: eq:beta\n",
    "a(\\hat{\\theta}) \\geq \\theta \\qquad ; \\qquad b(\\hat{\\theta}) \\leq \\theta .\n",
    "```\n",
    "\n",
    "So the Eq.{eq}`eq:alpha` and Eq.{eq}`eq:beta` become\n",
    "\n",
    "$$\n",
    "P(a(\\hat{\\theta}) \\geq \\theta) = \\alpha \\qquad ; \\qquad P(b(\\hat{\\theta})\\leq \\theta) = \\beta\n",
    "$$\n",
    "which proves that $P(a(\\hat{\\theta}) \\leq \\theta \\leq b(\\hat{\\theta})) = 1-\\alpha-\\beta$.\n",
    "\n",
    "```{margin}\n",
    "Here and in the following, we will use the convention used by\n",
    "Cowan also keeping the term \"significance level\" for the goodness of\n",
    "fit and using \"confidence level\" for the coverage probability of a\n",
    "confidence interval.\n",
    "```\n",
    "The confidence level is also called **coverage probability**\n",
    "because, from a frequentist point of view, if the experiment were to be\n",
    "repeated many times, the interval $[a,b]$ would cover the true (unknown)\n",
    "value $\\theta_{true}$, $1-\\alpha-\\beta$ of the times.\n",
    "\n",
    "The confidence interval $[a,b]$ is typically expressed by reporting the\n",
    "result of a measurement as $\\hat{\\theta}_{-c}^{+d}$ where\n",
    "$c = \\hat{\\theta}-a$ and $d = b-\\hat{\\theta}$. These values for a\n",
    "central confidence level of 68% are also the one used to represent\n",
    "graphically the error bars.\n",
    "\n",
    "The case we have just treated is the so called \"two-sided\" confidence\n",
    "interval. There are however cases where we might be interested in giving\n",
    "only a one sided interval:\n",
    "\n",
    "-   lower limit $a \\leq \\theta$ with coverage probability $1-\\alpha$\n",
    "\n",
    "-   upper limit $\\theta \\leq b$ with coverage probability $1-\\beta$\n",
    "\n",
    "The value $a$ is the value of $\\theta$ for which a fraction $\\alpha$ of\n",
    "the measurements would be higher than the observed one (and similarly\n",
    "for $b$). To get the value of $a$ ($b$) we have to solve (typically\n",
    "numerically) the equation for a ($b$):\n",
    "\n",
    "$$\n",
    "\\alpha  = \\int_{\\hat{\\theta}_{obs}}^\\infty g(\\hat{\\theta},a) d\\hat{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta  = \\int_{-\\infty}^{\\hat{\\theta}_{obs}}g(\\hat{\\theta},b) d\\hat{\\theta}\n",
    "$$\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "Let's take as an example a gaussian distributed estimator\n",
    "with mean $\\theta$ (true value unknown) and standard deviation\n",
    "$\\sigma_{\\hat{\\theta}}$ (known; if unknown you can estimate it from data\n",
    "getting $\\hat{\\sigma}_\\theta$ and move from the gaussian to the\n",
    "Student's $t-$distribution). The gaussian estimator is very common\n",
    "thanks to the central limit theorem (where any estimator that is a sum\n",
    "of random variables is approximately gaussian in the large sample\n",
    "limit). The integrals in\n",
    "Eq.{eq}`eq:alpha` and Eq.{eq}`eq:beta`  \n",
    "become the well known cumulative functions of the gaussian:\n",
    "\n",
    "$$\n",
    "G(\\hat{\\theta},\\theta,\\sigma_{\\hat{\\theta}}) = \\int_{-\\infty}^{\\hat{\\theta}} \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\hat{\\theta}}^2}} exp\\left( \\frac{-(\\hat{\\theta}' - \\theta)^2}{2\\sigma_{\\hat{\\theta}^2}} \\right) d\\hat{\\theta}'\n",
    "$$\n",
    "\n",
    "and the general case exposed above, simplifies considerably. We need to\n",
    "solve for $a$ and $b$ the equations:\n",
    "\n",
    "$$\n",
    "\\alpha = 1-G(\\hat{\\theta}_{obs},a,\\sigma_{\\hat{\\theta}}) = 1 - \\Phi\\left( \\frac{\\hat{\\theta}_{obs} - a}{\\sigma_{\\hat{\\theta}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = G(\\hat{\\theta}_{obs},b,\\sigma_{\\hat{\\theta}}) = \\Phi\\left( \\frac{\\hat{\\theta}_{obs} - b}{\\sigma_{\\hat{\\theta}}}\\right)\n",
    "$$\n",
    "\n",
    "that can be easily done by using the quantile of the gaussian\n",
    "$\\Phi^{-1}$ (the inverse function of $\\Phi$):\n",
    "\n",
    "$$\n",
    "a = \\hat{\\theta}_{obs} - \\sigma_{\\hat{\\theta}} \\Phi^{-1}(1-\\alpha)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\hat{\\theta}_{obs} + \\sigma_{\\hat{\\theta}} \\Phi^{-1}(1-\\beta)\n",
    "$$\n",
    "\n",
    "where we used $\\Phi^{-1}(\\beta) = -\\Phi^{-1}(1-\\beta)$. Thus, the\n",
    "general curve for $u_\\alpha(\\theta)$ and $v_\\beta(\\theta)$ become linear\n",
    "function.\n",
    "```\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "Another very frequent case is when the estimator is a\n",
    "Poisson variable $n$. Suppose you have performed a measurement and you\n",
    "found $\\hat{\\nu}_{obs} = n_{obs}$ and from this you want to build the\n",
    "confidence interval for the mean $\\nu$ (see\n",
    "{numref}`fig:poisson9evts`. The procedure is the same as for the\n",
    "general case shown above, but here we are dealing with a discrete\n",
    "variable. This means that in general, once $\\alpha$ and $\\beta$ are\n",
    "fixed, because $\\hat{\\nu}$ can only take discrete values, the\n",
    "Eq.{eq}`eq:alpha`, Eq.{eq}`eq:beta` only holds for particular values of $\\nu$ (and in\n",
    "general because of rounding we will give conservative intervals). Using\n",
    "the Poisson p.d.f. we have these equations to solve numerically:\n",
    "\n",
    "$$\n",
    "\\alpha = \\sum_{n=n_{obs}}^\\infty f(n;a) = 1 - \\sum_{n=0}^{n_{obs}-1}f(n;a) = 1 - \\sum_{n=0}^{n_{obs}-1}\\frac{a^n}{n!}e^{-a}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta  = \\sum_{n=0}^{n=n_{obs}}f(n;b) = \\sum_{n=0}^{n_{obs}} \\frac{b^n}{n!}e^{-b}\n",
    "$$\n",
    "```\n",
    "\n",
    "```{figure} ./images/ch9/poisson9evts.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:poisson9evts\n",
    "---\n",
    "Here\n",
    "we observed 9 radioactive decays and we constructed the 90% symmetric CL\n",
    "$\\lambda \\in [\\lambda_{low}-9, \\lambda_{up}-9]$, i.e.\n",
    "$\\lambda = 9^{+6.7}_{-4.3}.$\n",
    "```\n",
    "\n",
    "\n",
    "**Upper limit when $n_{obs} = 0$:** Again from the formulas in the\n",
    "previous example, you can compute what is the upper limit on the\n",
    "frequency of a rare event in case you don't observe any. At\n",
    "$1-\\beta = 95$% CL you get $b=2.996$, typically rounded to 3. This is\n",
    "why anytime you see a paper with zero observed events the quoted upper\n",
    "limit is 3.\n",
    "\n",
    "**Error bars on empty bins:** From the formulas in the example above we\n",
    "can see that the upper limit $b$ when $n_{obs} = 0$ becomes\n",
    "$\\beta=e^{-b}$. If we set the central confidence level at\n",
    "$1-\\beta=0.6827$ we find $b=-\\log(0.3173/2) \\sim 1.8$. This is the\n",
    "reason of the error bar at 1.8 when we observe zero events in a counting\n",
    "experiment. (In FIXME `ROOT` you can get the correct Poisson error bars using\n",
    "the method `TH1::SetBinErrorOption(TH1::kPoisson)`).\n",
    "\n",
    "When the p.d.f. of the estimator is not a gaussian, and approximating it\n",
    "to a gaussian would lead to biases or under/over coverage of the\n",
    "confidence interval, one can always extract the p.d.f. of the estimator\n",
    "tossing toy experiments and compute the confidence belt numerically. In\n",
    "some specific cases one can also try to transform the estimator such\n",
    "that the transformed variable is gaussian distributed.\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "Consider, as an example for which the p.d.f. of the\n",
    "estimator is not gaussian, the correlation coefficient $\\rho$ of a\n",
    "two-dimensional gaussian and the estimator $r$ as\n",
    "\n",
    "$$\n",
    "r=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\left( \\sum_{j=1}^n(x_j-\\bar{x})^2\\cdot\\sum_{k=1}^{n}(y_k-\\bar{y})  \\right)^{1/2}}\n",
    "$$\n",
    "\n",
    "The distribution for this estimator is shown in\n",
    "{numref}`fig:corrEst` and it only approaches a gaussian distribution in the large sample\n",
    "limit.\n",
    "\n",
    "For this specific case, Fisher showed that the transformed estimator\n",
    "\n",
    "$$\n",
    "z=\\tanh^{-1} r = \\frac{1}{2} \\log\\frac{1+r}{1-r}\n",
    "$$ \n",
    "\n",
    "reaches the gaussian limit much more quickly. You can use this transformation for an\n",
    "estimator $\\zeta$\n",
    "\n",
    "$$\n",
    "\\zeta = \\tanh^{-1} \\rho = \\frac{1}{2} \\log\\frac{1+\\rho}{1-\\rho}\n",
    "$$\n",
    "\n",
    "The expectation value and the variance for $z$ are approximately given by:\n",
    "\n",
    "$$\n",
    "<z> = \\frac{1}{2}\\log\\frac{1+\\rho}{1-\\rho}   + \\frac{\\rho}{2(n-1)} \\qquad ; \\qquad V[z] =  \\frac{1}{n-3}\n",
    "$$\n",
    "\n",
    "Assuming that the sample is large enough such that you can neglect the\n",
    "bias $\\frac{\\rho}{2(n-1)}$, you can use these to determine the\n",
    "confidence interval $[a,b] = [z-\\hat{\\sigma}_z,z+\\hat{\\sigma}_z]$ such\n",
    "that the lower limit $a$ for $\\zeta$ is at confidence level $1-\\alpha$\n",
    "and the upper limit $b$ is at $1-\\beta$ confidence level. From $[a,b]$\n",
    "on $\\zeta$ you can go back to the interval $[A,B]$ on $\\rho$ simply\n",
    "inverting $\\zeta = \\tanh^{-1} \\rho$\n",
    "($[A = \\tanh \\alpha, B = \\tanh\\beta]$).\n",
    "```\n",
    "\n",
    "```{figure} ./images/ch9/corrEst.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:corrEst\n",
    "---\n",
    "Distribution of the\n",
    "correlation estimator for a sample of size n=20 and different values of\n",
    "$\\rho$.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the likelihood or the $\\chi^2$ to set confidence intervals\n",
    "--------------------------------------------------------------\n",
    "\n",
    "In the large sample limit approximation it is easy to extract confidence\n",
    "intervals using the likelihood method or equivalently the $\\chi^2$\n",
    "method ($L=\\exp(-\\chi^2/2)$).\n",
    "\n",
    "In the gaussian case, as we have already seen when estimating the\n",
    "uncertainty on the likelihood estimator, we can get the estimated\n",
    "$\\hat{\\sigma}_{\\hat{\\theta}}$ for $\\sigma_{\\hat{\\theta}}$ from:\n",
    "\n",
    "$$\n",
    "\\log ~L(\\hat{\\theta} \\pm N \\sigma_{\\hat{\\theta}}) = \\log L_{max} - \\frac{N^2}{2}\n",
    "$$\n",
    "\n",
    "and again as we have already seen, this amounts to move by 0.5 from the\n",
    "maximum value. Even if the likelihood is not gaussian the central\n",
    "confidence interval $[a,b] = [\\hat{\\theta}-c, \\hat{\\theta}+d]$ can still\n",
    "be approximated by:\n",
    "\n",
    "$$\n",
    "{\\color{blue}{LIKELIHOOD}}~:~\\log ~L(\\hat{\\theta}^{+d}_{-c}) = \\log L_{max} - \\frac{N^2}{2} \\qquad \\; \\qquad {\\color{blue}{\\chi^2}}~:~\\chi^2(\\hat{\\theta}^{+d}_{-c}) = \\chi^2_{min} + N^2\n",
    "$$\n",
    "\n",
    "for the likelihood and the $\\chi^2$ methods respectively, where\n",
    "$N=\\Phi^{-1}(1-\\gamma/2)$ is the quantile of the standard gaussian\n",
    "distribution corresponding to the desired confidence level. The $\\chi^2$\n",
    "prescription is just the likelihood one where we use $\\log L=-\\chi^2/2$.\n",
    "This is by far the most used method to report the statistical\n",
    "uncertainties, but it has to be remembered that it exact *only* for\n",
    "gaussian likelihood or in the large sample limit and for all other cases\n",
    "it is only an approximation !\n",
    "\n",
    "```{admonition} Example:\n",
    ":class: tip\n",
    "Consider the lifetime measurement of an unstable particle.\n",
    "We can extract the value of the lifetime by fitting the exponential\n",
    "distribution of the proper decay time of a large number of particles, as\n",
    "in Sec.[Likelhood](likelihood.html#).\n",
    "\n",
    "The ML estimator comes out to\n",
    "be just the average $\\hat{\\tau}=\\frac{1}{n}\\sum_{i=1}^{n}t_i$. For a\n",
    "small statistics as in {numref}`fig:lifetime`, where we only have 5 measurements, the\n",
    "likelihood is non parabolic and it is preferable to report the\n",
    "asymmetric values given by $\\log L_{max} - \\frac{1}{2}$: i.e.\n",
    "$[\\hat{\\tau} - \\Delta\\hat{\\tau}_-, \\hat{\\tau} + \\Delta\\hat{\\tau}_+]$.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{figure} ./images/ch9/lifetime.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:lifetime\n",
    "---\n",
    "Non-parabolic log-Likelihood scan of a lifetime measurement done on a small statistics\n",
    "sample.\n",
    "```\n",
    "\n",
    "\n",
    "The same method can be applied to find confidence intervals in several\n",
    "dimensions (a.k.a. confidence regions). What in one dimension was an\n",
    "interval in n-dimensions becomes a region bounded by an hyper-ellipsoid\n",
    "defined by constant values of $Q(\\vec{\\hat{\\theta}},\\vec{\\theta})$.\n",
    "\n",
    "If $\\vec{\\hat{\\theta}}$ is described by a n-dimensional gausssian p.d.f. \n",
    "$g(\\vec{\\hat{\\theta}}, \\vec{\\theta})$\n",
    ",then $Q(\\vec{\\hat{\\theta}}, \\vec{\\theta})$ is distributed according to\n",
    "a $\\chi^2$ distribution with n degrees of freedom. We can then compute\n",
    "the probability to have the estimate $\\vec{\\hat{\\theta}}$ near the true\n",
    "value (and viceversa) as\n",
    "\n",
    "$$\n",
    "P(Q(\\vec{\\theta},\\vec{\\hat{\\theta}}) < Q_\\gamma) = \\int_0^{Q_\\gamma} f(z;n)dz = 1-\\gamma\n",
    "$$\n",
    "\n",
    "where $f(z;n)$ is the $\\chi^2$ distribution with n degrees of freedom.\n",
    "Inverting using the quantile of the $\\chi^2$ distribution we get:\n",
    "\n",
    "$$\n",
    "Q_\\gamma = F^{-1}(1-\\gamma;n)\n",
    "$$ \n",
    "\n",
    "The region defined by the\n",
    "$Q(\\vec{\\theta},\\vec{\\hat{\\theta}}) < Q_\\gamma$ is called *confidence\n",
    "region* with confidence level $1-\\gamma$. In the case of a gaussian\n",
    "likelihood, the confidence region can be constructed by finding the\n",
    "region of the space such that:\n",
    "\n",
    "$$\n",
    "\\log L(\\vec{\\theta}) = \\log L_{max} - \\frac{Q_\\gamma}{2}\n",
    "$$ \n",
    "\n",
    "Typical values of the quantiles of the $\\chi^2$ distribution are listed in\n",
    "{numref}`fig:quantiles`. Note that for increasing number of\n",
    "dimensions the confidence level, at a fixed quantile $Q_\\gamma$\n",
    "decreases.\n",
    "\n",
    "```{figure} ./images/ch9/quantiles.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:quantiles\n",
    "---\n",
    "Quantiles and confidence levels in n-Dimensions.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limits near boundaries\n",
    "----------------------\n",
    "\n",
    "When performing searches, very often we are faced with the problem of\n",
    "looking for a new effect at the boundaries of the phase space. Suppose\n",
    "we want to measure the neutrino mass; because of the oscillation\n",
    "phenomenon we know neutrinos are not (all of them) massless. Still their\n",
    "mass is very small, i.e. near the boundary m = 0.\n",
    "\n",
    "```{margin}\n",
    "Deciding how to quote the result of a measurement after seeing the\n",
    "results is called \"flip-flop\" and will be addressed using the\n",
    "Feldman-Cousins construction in Sec. [Flip-Flop](/confidenceIntervals.html#the-flip-flop-problem).\n",
    "```\n",
    "Depending on the resolution of the measurement, we typically face two\n",
    "situations:\n",
    "\n",
    "-   the data yields a value significantly different from zero; we will\n",
    "    then quote the measured value with an uncertainty\n",
    "\n",
    "-   the data yields a value compatible with zero; we will then proceed\n",
    "    to quote an upper limit on the value of the parameter at a given CL.\n",
    "\n",
    "The problem gets trickier when the estimator takes values in a\n",
    "physically forbiddend region. To get the idea: suppose you measure a\n",
    "mass as $m^2 = E^2 - p^2$. Depending on the uncertainties on energy and\n",
    "momentum you can obtain negative values for the mass. From the purely\n",
    "statistical point of view you can proceed as before and set the upper limit\n",
    "$\\theta_{up}$ at $1-\\beta$ CL is given by:\n",
    "\n",
    "$$\n",
    "\\theta_{up} = \\hat{\\theta}_{obs} + \\sigma_{\\hat{\\theta}}\\Phi^{-1} (1-\\beta).\n",
    "$$\n",
    "\n",
    "The interval $(-\\infty, \\theta_{up})$ will contain the true value of\n",
    "$\\theta$ with a probability of 95%.\n",
    "\n",
    "Now, take as an example a measurement giving $\\theta_{obs} = -2.0$ with\n",
    "a standard deviation $\\hat{\\sigma}=1$: the upper limit is\n",
    "$\\theta_{up} = -0.355$ at 95% CL. We are setting a negative upper limit\n",
    "on a mass! Having a negative upper limit has to happen 5% of the times\n",
    "when using a 95%CL. We just got one of the 5%. The annoying point is\n",
    "that, because we knew from the beginning that the mass has to be\n",
    "positive, the experiment does not bring any new piece of information. In\n",
    "such a case we should still quote the value we found such that, when\n",
    "combined with more experiments, we will contribute to the measurement of\n",
    "the correct value. If you encounter a very asymmetric likelihood, you\n",
    "should in general publish the scan of the likelihood on the parameter\n",
    "you are estimating.\n",
    "\n",
    "A naive approach to avoid the negative upper limit is to shift the\n",
    "negative estimate to the boundary ($\\hat{\\theta}=0$ in this case):\n",
    "\n",
    "$$\n",
    "\\theta_{up}=max(\\hat{\\theta}_{obs},0) + \\sigma_{\\hat{\\theta}}\\Phi^{-1} (1-\\beta).\n",
    "$$\n",
    "\n",
    "In this case, if the limit is positive you will quote the same value\n",
    "given by the classical construction, while for negative values you will\n",
    "end up with an unwanted coverage larger than the quoted $1-\\beta$.\n",
    "\n",
    "The most natural way to include boundaries in the computation of limits\n",
    "is to use the Bayes theorem:\n",
    "\n",
    "$$\n",
    "p(\\theta|x) = \\frac{L(x|\\theta)\\pi(\\theta)}{\\int L(x|\\theta')\\pi(\\theta')d\\theta'}\n",
    "$$\n",
    "\n",
    "where $L(x|\\theta)$ is the likelihood to observe a value $x$ given the\n",
    "parameter $\\theta$ and $\\pi(\\theta)$ is the prior on $\\theta$. The most\n",
    "probable value of the posterior $p(\\theta|x)$ coincides with the ML\n",
    "estimator if the prior is flat (the denominator only contributes as a\n",
    "normalization constant). To set limits (without considering boundaries\n",
    "for the moment), we can use the expression for the posterior and find\n",
    "numerically the interval $[a,b]$ such that:\n",
    "\n",
    "$$\n",
    "\\alpha = \\int_{-\\infty}^{a}p(\\theta|x)d\\theta \\qquad \\beta =\\int_{b}^{+\\infty}p(\\theta|x)d\\theta\n",
    "$$\n",
    "\n",
    "where $\\alpha$ and $\\beta$ define the CL.\n",
    "\n",
    "Using the Bayes theorem, the inclusion of boundaries is trivial: we just\n",
    "need to write them in the prior. In the example of a mass limited to\n",
    "positive values, we can write the prior to be zero for negative masses\n",
    "and flat above: \n",
    "\n",
    "$$\n",
    "\\pi(\\theta) = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                0 & \\mbox{if} & \\theta \\le 0 \\\\\n",
    "                1 & \\mbox{if} & \\theta >  0\n",
    "            \\end{array}\\right.\n",
    "$$ \n",
    "            \n",
    "The upper limit on the mass will be\n",
    "found as before by solving numerically for $\\theta_{up}$ the integral on\n",
    "the posterior:\n",
    "\n",
    "$$\n",
    "1-\\beta = \\int_{-\\infty}^{\\theta_{up}} p(\\theta|x) = \\frac{\\int_{-\\infty}^{\\theta_{up}}L(x|\\theta)\\pi(\\theta)d\\theta }{\\int_{-\\infty}^{+\\infty}L(x|\\theta)\\pi(\\theta)d\\theta }{}\n",
    "$$\n",
    "\n",
    "This method is clearly affected by the issues on the encoding of\n",
    "ignorance in the prior (i.e. the choice of a flat prior):\n",
    "\n",
    "-   using a flat prior assumes that the probability for the parameter\n",
    "    $\\theta$ is the same everywhere in the physically allowed phase\n",
    "    space; applied to the neutrino mass example, this would mean that\n",
    "    the probability is the same in the range $[0,1]$eV as in\n",
    "    $[10^{10}, 10^{10}+1]$eV.\n",
    "\n",
    "-   a flat prior won't remain flat under a non-trivial transformation of\n",
    "    the parameter $\\theta$.\n",
    "\n",
    "With the Bayes approach, you don't need anymore to go for the \"build\n",
    "horizontally / read vertically\" classical construction. You just need to\n",
    "compute the integrals above. For the example of the neutrino mass: the\n",
    "denominator is an integral on the full phase space $(0,+\\infty)$, while\n",
    "the numerator runs on $(0, \\theta_{up})$. For large values of\n",
    "$x = \\theta_{obs}$ the bayesian limit approaches the classical one: the\n",
    "likelihood you are integrating is far from the boundary and the tail\n",
    "below zero is negligible. The closer you get to the boundary the larger\n",
    "is the area of the likelihood leaking in the unphysical region. Still,\n",
    "the upper limit will never cross zero because it is extracted from the\n",
    "ratio of two integrals where the numerator is by construction always a\n",
    "fraction of the denominator.\n",
    "\n",
    "{numref}`fig:comparison` shows the comparison of the different\n",
    "methods discussed so far: the classical construction; the \"shifted\"\n",
    "classical construction where the upper limit if forced to be flat for\n",
    "$\\hat{\\theta}<0$; and the bayesian approach where the upper limit on\n",
    "$\\theta$ never falls below zero.\n",
    "\n",
    "```{figure} ./images/ch9/comparison.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:comparison\n",
    "---\n",
    "Comparison of the upper limits obtained with the classical construction, the\n",
    "\"shifted\" classical construction and the bayesian approach.\n",
    "```\n",
    "\n",
    "Another frequent case is when we try to set an upper limit on a rare\n",
    "signal with a counting experiment. Here the boundary is given by the\n",
    "number of events that has to be greater or equal than zero. Let's start\n",
    "from the simpler situation where we don't set any constraint (or\n",
    "equivalently we are far away from the boundary) and compute the upper\n",
    "limit following the classical construction. We observe $n$ events some\n",
    "of which come from signal $n_s$ and some others from background $n_b$:\n",
    "$n = n_s+n_b$. The number of events follow a Poisson distribution with\n",
    "an expected number of events $\\nu_s$ for signal and $\\nu_b$ for\n",
    "background (suppose we know the expected number of background events\n",
    "with negligible uncertainty). We want to get the upper limit on the\n",
    "number of signal events in the sample. The ML estimator for the number\n",
    "of signal events is simply $\\hat{\\nu}_s = n - \\nu_b$. The confidence\n",
    "interval can be extracted as usual by solving (numerically) the\n",
    "equation:\n",
    "\n",
    "$$\n",
    "\\beta = P(\\hat{\\nu}_s\\leq\\hat{\\nu}_s^{obs}| \\nu^{up}) = \\sum_{n\\leq \\hat{\\nu}_{obs}}\\frac{(\\nu_s^{up} + \\nu_b)^n e^{(\\nu_s^{up} + \\nu_b)}}{n!}\n",
    "$$\n",
    "\n",
    "The results for some values of observed events and expected backgrounds\n",
    "are shown in {numref}`fig:poissonLim`.\n",
    "\n",
    "```{figure} ./images/ch9/poissonLim.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:poissonLim\n",
    "---\n",
    "Upper\n",
    "limit on the number of signal events as a function of the expected\n",
    "number of background events $\\nu_b$ for different observed $n$.\n",
    "```\n",
    "\n",
    "Before moving to the Bayesian solution to setting the upper limit, let's\n",
    "see where the classical one ends into troubles. Suppose you have a small\n",
    "number of expected signal events and large background. In this situation\n",
    "the number of observed events will be dominated by the background and we\n",
    "can find cases where the number of observed events is smaller than the\n",
    "expected number of background events (e.g. $\\nu_b$ = 8, $n$ = 1). By\n",
    "reading off the limit from\n",
    "{numref}`fig:poissonLim` we find a negative number of signal events.\n",
    "As we have already seen for the example of the neutrino mass measurement\n",
    "this is not wrong. We are setting an upper limit at 95% CL and we got\n",
    "one large downward fluctuations which should happen in 5% of the cases.\n",
    "Nevertheless from a physics content the result is not particularly\n",
    "interesting (we knew before setting up the experiment that the number of\n",
    "events has to be greater than or equal to zero).\n",
    "\n",
    "Using the bayesian approach the posterior can be written as:\n",
    "\n",
    "$$\n",
    "p(\\nu_s|n_{obs}) = \\frac{L(n_{obs}|\\nu_s)\\pi(\\nu_s)}{\\int L(n_{obs}|\\nu_s')\\pi(\\nu_s')d\\nu_s'}\n",
    "$$\n",
    "\n",
    "To include the boundary $\\nu_s>0$ we can simply define the prior as:\n",
    "\n",
    "$$\n",
    "\\pi(\\nu_s) = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                0 & \\mbox{if} & \\nu_s \\le 0 \\\\\n",
    "                1 & \\mbox{if} & \\nu_s >  0\n",
    "            \\end{array}\\right.\n",
    "$$ \n",
    "\n",
    "To compute the upper limit we just need to plug this prior in the posterior expression\n",
    "$$\n",
    "1-\\beta = \\frac{\\int_{0}^{\\nu_s^{up}}L(n_{obs}|\\nu_s)d\\nu_s }{\\int_{0}^{+\\infty}L(n_{obs}|\\nu_s)d\\nu_s}\n",
    "$$\n",
    "\n",
    "and solve numerically for $\\nu_s^{up}$. The equivalent of\n",
    "{numref}`fig:poissonLim` using a bayesian approach is shown in\n",
    "{numref}`fig:poissonLimBayes`.\n",
    "\n",
    "```{figure} ./images/ch9/poissonLimBayes.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:poissonLimBayes\n",
    "---\n",
    "Bayesian upper limit on the number of signal\n",
    "events as a function of the expected number of background events $\\nu_b$\n",
    "for different observed $n$.\n",
    "```\n",
    "\n",
    "The bayesian limit is always greater or equal than the classical one. As\n",
    "expected the bayesian approaches the classical limit when we move to the\n",
    "region where the number of observed events is much larger than the\n",
    "number of expected background events. The fact that the two limits\n",
    "precisely coincide for $n_{bkg}=0$ is a pure coincidence since the\n",
    "Bayesian limit depends on the particular choice of the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feldman-Cousins\n",
    "---------------\n",
    "\n",
    "The Feldman-Cousins (FC) prescription of the confidence intervals solves\n",
    "two complications related to the classical Neyman construction: the so\n",
    "called \"flip-flop\" problem and the problem of obtaining unphysical (or\n",
    "empty set) interval using the Neyman construction. Their solution\n",
    "provides, in a purely frequentist manner, intervals which are never\n",
    "unphysical, nor empty and unifies the set of classical confidence\n",
    "intervals for setting upper limits and quoting two-sided confidence\n",
    "intervals. However, it still suffers from an apparent paradox when the\n",
    "number of observed events is lower than the background only expectations\n",
    "(background underfluctuations). \n",
    "\n",
    "*In this section we will follow closely the paper in Ref.* [@FeldmanCousins]. FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flip-flop problem\n",
    "\n",
    "The flip-flop problem arises when one decides, *based on the results of\n",
    "the experiment*, whether to publish an upper limit or a central\n",
    "confidence interval. This is a very common (and sensible) attitude.\n",
    "Typically the reasoning goes as: if I see that a result is below\n",
    "3$\\sigma$ I publish an upper limit, if the result has a significance of\n",
    "more than 3$\\sigma$ then I publish a central confidence interval. The\n",
    "typical sentence in papers is: \"the data are compatible with the\n",
    "background expectations (or no excess is observed above the expected\n",
    "background) *hence* we set an upper limit on\\...\". As seen in the\n",
    "previous section, one can even decide that in case of a boundary, e.g. a\n",
    "mass which has to be positive, to use the maximum between the measured\n",
    "value and zero $\\max(m,0)$ and quote the CL corresponding to zero. These\n",
    "choices has an impact on the coverage of the reported confidence\n",
    "intervals.\n",
    "\n",
    "To better understand the flip-flop problem we start by looking at the\n",
    "confidence belt at 90% CL of {numref}`fig:FC1`(left). Here we are considering a gaussian\n",
    "observable with $\\sigma=1$; the $x$-axis is the measured mean $x$, while\n",
    "the $y$-axis is the unknown mean $\\mu$. By construction the coverage is\n",
    "90%, i.e. reading the plot vertically for a measured $x$, we have\n",
    "$P(\\mu \\in [\\mu_1,\\mu_2]) =90\\%$. The corresponding plot for the upper\n",
    "limit at 90%, again using the classical construction, is shown in the\n",
    "{numref}`fig:FC1`(right).\n",
    "\n",
    "\n",
    "```{figure} ./images/ch9/FC1.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:FC1\n",
    "---\n",
    "(left) 90% CL confidence belt; (right) 90% CM upper limit.\n",
    "```\n",
    "\n",
    "If you decide decide a posteriori (i.e. based on the result of the\n",
    "measurement) whether to publish a central confidence interval, an upper\n",
    "limit or the truncated $\\max(x,0)$ limit, you will get a confidence\n",
    "region such as the one reported in {numref}`fig:FC2`.\n",
    "\n",
    "```{figure} ./images/ch9/FC2.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:FC2\n",
    "---\n",
    "\"A posteriori\" confidence interval: if\n",
    "the result x is more than $3\\sigma$ (i.e. x $>$ 3) publish the central\n",
    "confidence interval; if the result x is less than $3\\sigma$ (i.e. x $<$\n",
    "3) publish an upper limit; if the results is negative use max(meas,0)\n",
    "and quote the CL corresponding to 0. \n",
    "```\n",
    "\n",
    "```{figure} ./images/ch9/flipflop.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:flipflop\n",
    "---\n",
    "The interval doesn't have the correct coverage.\n",
    "```\n",
    "\n",
    "This region does not guarantee the 90% coverage. It's not \"built\n",
    "horizontally\" following the classical construction; it's a patchwork of\n",
    "confidence intervals. Take for example the case of $\\mu=2$ (see\n",
    "{numref}`fig:flipflop`. The interval starts with the lower\n",
    "limit at $2-1.28$ on the boundary of the 90% upper limit and ends at\n",
    "$2+1.64$ on the $90\\%$ central interval, giving a coverage of only\n",
    "$1-0.1-0.05 = 85\\%$. In other cases, like at $\\mu=1$ the region\n",
    "over-covers as shown in the same figure, the confidence region\n",
    "overcovers. We will see in the following how the FC prescription allows\n",
    "to unify these intervals and avoid the problem.\n",
    "\n",
    "Typically the flip-flop problem only comes if you publish a confidence\n",
    "interval for a well established signal, but with more data the signal\n",
    "disappears and you are forced to set a limit. While potentially this\n",
    "could be a very serious issue, in reality it only happens very rarely\n",
    "because the experimental sensitivity typically grows with additional\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson with background\n",
    "\n",
    "To understand the FC prescription we will use the example of a counting\n",
    "experiment. The measured number of events is $n$, the number of expected\n",
    "background events is $b$ (set to $b=3$ in this example) and we want to\n",
    "build the confidence interval for the mean $\\mu$:\n",
    "\n",
    "$$\n",
    "P(n|\\mu) = \\frac{(\\mu+b)^n}{n!}e^{-(\\mu+b)}\n",
    "$$ \n",
    "\n",
    "Let's start from the\n",
    "classical construction of the upper limit (UL) as seen in\n",
    "Sec.[Confidence Intervals](confidenceIntervals.html#confidence-belt-neyman-frequentist-construction)\n",
    "(see {numref}`fig:singleDouble`). \n",
    "\n",
    "We build horizontally the UL curve scanning the $y$-axis (the unknown signal mean) $\\mu$ and for each value\n",
    "of $\\mu$ we find the value of the limit solving numerically for $\\mu$\n",
    "the equation $0.1=\\sum_0^{n_{obs-1}}P(n|\\mu+b)$. See\n",
    "{numref}`fig:poisson9evts` for a graphical reminder of the procedure.\n",
    "Following the same procedure we can built the confidence interval for\n",
    "the central interval at $CL=90\\%$. In this case the lower and upper\n",
    "limits are computed solving numerically for $c$ in\n",
    "$0.05=\\sum_0^{n_{obs-1}}P(n|c+b)$ and for $a$ in\n",
    "$0.05=\\sum_{n_{obs}}^\\infty P(n|a+b)$. When $n=0$ we get an empty set.\n",
    "For both the upper and central limits, n = 0 has no solution (in the\n",
    "example b = 3). This is counter-intuitive: if one measures no events,\n",
    "clearly the most likely value of $\\mu$ is zero. Why should one rule out\n",
    "the most likely scenario? Let's see how this issue is addressed using\n",
    "the FC prescription.\n",
    "\n",
    "```{figure} ./images/ch9/singleDouble.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:singleDouble\n",
    "---\n",
    "90%\n",
    "CL upper limit (left) and confidence interval (right) following the\n",
    "classical construction. for the case with expected background\n",
    "$b=3$.\n",
    "```\n",
    "\n",
    "The FC uses the classical Neyman construction, but using a different\n",
    "*ordering principle*. Every time a confidence region is defined, we use\n",
    "(often without even noticing) the concept of ordering principle. An\n",
    "ordering principle is needed to specify which values of the measured $x$\n",
    "to include in the acceptance region. When building an upper limit we\n",
    "start from the smallest values of $x$, for a central limit we start from\n",
    "the value of $x$ closest to the central value, etc\\... There are\n",
    "infinite ordering principles. The FC ordering uses an ordering principle\n",
    "based on a likelihood ratio.\n",
    "\n",
    "To understand the idea let's take the example where we want to build\n",
    "horizontally the interval for $\\mu=0.5$ (remember we set the number of\n",
    "background events $b=3$). The Poisson probability to obtain $0$ events,\n",
    "when the expected number of events is $\\mu = 0.5$ is 0.03. Let's compare\n",
    "this low value with the one we obtain using as $\\mu$ the most probable\n",
    "$\\mu_{best}$. $\\mu_{best}$ is defined as the value of the mean signal\n",
    "$\\mu$ which maximizes $P(n|\\mu)$ when we require $\\mu_{best}$ to be\n",
    "physically allowed (i.e. $\\mu_{best}>0$). This simply means (remembering\n",
    "the ML estimator for a signal in presence of background)\n",
    "$\\mu_{best} = max(0,n-b)$. The probability to obtain $0$ events when the\n",
    "expected number of events is $\\mu_{best} = 0$ is 0.05. So, while\n",
    "$P(n,\\mu=0.05)=0.03$ is quite low on an absolute scale, it is very much\n",
    "comparable with $P(n,\\mu_{best}=0)=0.05$ of the most probable value\n",
    "$\\mu_{best}$.\n",
    "\n",
    "Following this observation the ordering principle is defined on the\n",
    "likelihood ratio: \n",
    "\n",
    "$$\n",
    "R(\\mu) = \\frac{P(n|\\mu)}{P(n|\\mu_{best})}\n",
    "$$ \n",
    "\n",
    "where $R$ is called *rank*. The effect of this ordering principle is to\n",
    "increase the rank of the values with low probability if they are close\n",
    "to $\\mu_{best}$. The interval is then built by including values of $n$\n",
    "in decreasing order of $R$ until the sum of $P(n|\\mu)=1-CL$ i.e. matches\n",
    "the CL we want. Due to the discreteness of $n$, the acceptance region\n",
    "can contain more summed probability than required by the CL, i.e. we\n",
    "could end up over-covering. Notice that while in the typical orderings\n",
    "different values of $\\mu$ do not talk to each other, here $\\mu_{best}$\n",
    "influence the \"weight\" of all the other values (their order).\n",
    "\n",
    "Let's compute the interval for the numerical example used above\n",
    "(expected number of background events is $b=3$, we measure $n$ events\n",
    "and we want to build the confidence interval for $\\mu=0.5$).\n",
    "\n",
    "```{figure} ./images/ch9/tableFC.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:tableFC\n",
    "---\n",
    "Confidence interval construction for $\\mu=0.5$ and background $b=3$.\n",
    "```\n",
    "\n",
    "For each value of $n$ we first compute $\\mu_{best}$ the best (physical)\n",
    "value of the estimator given n (see table in {numref}`fig:tableFC`.\n",
    "We get $\\mu_{best} = max(0,n-b)$: $n=0 \\to \\mu_{best}=0$,\n",
    "$n=1 \\to \\mu_{best}=0$, $n=2 \\to \\mu_{best}=0$, $n=3 \\to \\mu_{best}=0$,\n",
    "$n=4 \\to \\mu_{best}=1$, $n=5 \\to \\mu_{best}=2$,\\.... For each $n$ we\n",
    "also compute $P(n|\\mu=0.05)$ and $P(n|\\mu_{best})$ and the rank\n",
    "$R=P(n|\\mu=0.05)/P(n|\\mu_{best})$. Now we can complete the confidence\n",
    "interval. We add the probabilities $P(n|\\mu=0.05)$ in decreasing order\n",
    "of the rank $R$ until we match the desired CL: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(4|\\mu=0.05) & + P(3|\\mu=0.05)  + P(2|\\mu=0.05)  + P(5|\\mu=0.05)  + P(1|\\mu=0.05)  + \\\\\n",
    "              & P(0|\\mu=0.05)  + P(6|\\mu=0.05) = \\\\\n",
    "0.189         & + 0.216          + 0.185         + 0.132          + 0.106          + 0.030          + 0.077          = 0.935\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We intentionally stay on the conservative side and add passed 0.9. The\n",
    "alternative would be to stop at $R=6$ which is $0.858 < 0.9$, hence\n",
    "under-covering. Repeating the procedure scanning the values of $\\mu$ on\n",
    "the $y$-axis we obtain the complete confidence belt.\n",
    "\n",
    "The resulting FC confidence interval is compared with the classical one\n",
    "in {numref}`fig:compFC`.\n",
    "\n",
    "```{figure} ./images/ch9/compFC.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:compFC\n",
    "---\n",
    "Comparison of the FC confidence interval with the classical one.\n",
    "```\n",
    "\n",
    "At large $n$ the FC and the classical construction gives approximately\n",
    "the same confidence interval: we are far away from the physical boundary\n",
    "and so the background is effectively subtracted without constraint. For\n",
    "small values of $n$ , the confidence interval automatically becomes\n",
    "upper limits on $\\mu$; i.e. the lower endpoint is 0 for $n \\leq 4$ in\n",
    "this case. Thus, flip-flopping between the plots in\n",
    "{numref}`fig:singleDouble` is replaced by one coherent set of\n",
    "confidence interval, (and no interval is the empty set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian with boundary at the origin\n",
    "\n",
    "As another example we can build the FC confidence interval for a\n",
    "gaussian distribution with a boundary $\\mu>0$ (e.g. the physical boundary imposed on the mass being positive). and compare it with what we have already encountered in the example in Sec.[Classical contruction](confidenceIntervals.html#confidence-belt-neyman-frequentist-construction)\n",
    "\n",
    "Consider as p.d.f. a gaussian with $\\sigma =1$:\n",
    "\n",
    "$$\n",
    "P(x|\\mu) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2}}\n",
    "$$ \n",
    "\n",
    "As we did with the Poisson variable we find $\\mu_{best}$ as the value that\n",
    "maximizes $P(x|\\mu)$ and force it to be positive, i.e. we take\n",
    "$\\max(0,\\mu_{best})$. The expression for $P(x|\\mu_{best})$ becomes:\n",
    "\n",
    "$$\n",
    "P(x|\\mu_{best}) = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                1/\\sqrt{2\\pi} & \\mbox{if} & x \\geq 0 \\\\\n",
    "                \\exp(-x^2/2)/\\sqrt{2\\pi} & \\mbox{if} & x<0\n",
    "            \\end{array}\\right.\n",
    "$$ \n",
    "\n",
    "if $x>0$ then $\\mu_{best} = x$, while\n",
    "if $x<0$ we have $\\mu_{best} = 0$  (see {numref}`fig:xgt0st0`).\n",
    "\n",
    "```{figure} ./images/ch9/xgt0st0.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:xgt0st0\n",
    "---\n",
    "$\\mu_{best}$ is the value that maximizes $P(x|\\mu)$.\n",
    "```\n",
    "\n",
    "The likelihood ratio is then:\n",
    "\n",
    "$$\n",
    "R(x) = \\frac{P(x|\\mu)}{P(x|\\mu_{best})} =\n",
    "             \\left\\{\n",
    "             \\begin{array}{rll}\n",
    "                   \\exp(-(x-\\mu)^2/2) & \\mbox{if} & x \\geq 0 \\\\\n",
    "                   \\exp(x\\mu-\\mu^2/2) & \\mbox{if} & x<0\n",
    "             \\end{array}\\right.\n",
    "$$ \n",
    "\n",
    "With this we can now add to the\n",
    "acceptance region values of x ordering them by their rank. If $x$ is far\n",
    "from the boundary, the rank is just $P(x|\\mu)$ and we get to the usual\n",
    "ordering. If instead it is close to the boundary the rank develops a\n",
    "tail to the left as shown in {numref}`fig:rank` that will modify the ordering.\n",
    "\n",
    "\n",
    "```{figure} ./images/ch9/rank.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:rank\n",
    "---\n",
    "The rank is asymmetric.\n",
    "```\n",
    "\n",
    "For a given value of $\\mu$, one finds the interval $[x_1,x_2]$ such that\n",
    "$\\int_{x_1}^{x_2} P(x|\\mu)dx = \\alpha$ and $R(x_1) = R(x_2)$ (see\n",
    "{numref}`fig:rank2` and {numref}`fig:rank3`).\n",
    "The condition that the rank at the two extremes coincides guarantees\n",
    "that the order we used to pick the values of $x$ is correct. If the rank\n",
    "was higher (or lower) at one extreme, it would mean that those values of\n",
    "$x$ should have (have not) be used for the interval.\n",
    "\n",
    "```{figure} ./images/ch9/rank2.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:rank2\n",
    "---\n",
    "Classical construction in blue, FC in red. \n",
    "```{figure} ./images/ch9/rank3.png\n",
    "---\n",
    "width: 400px\n",
    "align: center\n",
    "name: fig:rank3\n",
    "---\n",
    "Classical construction in blue, FC in black.\n",
    "```\n",
    "\n",
    "As expected, far away from the boundary the two intervals coincide.\n",
    "Below $x=1.28$, the lower endpoint of the FC confidence intervals is\n",
    "zero, so that there is automatically a transition from two-sided\n",
    "confidence intervals to an upper confidence limit given by $\\mu_2$. The\n",
    "point of this transition is fixed by the calculation of the acceptance\n",
    "interval for $\\mu=0$ the solution has $x_1 =-\\infty$, and so\n",
    "$\\int_{x_1}^{x_2} P(x|\\mu)dx = \\alpha$ is satisfied by $x_2 =1.28$ when\n",
    "$\\alpha=90$%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutrino oscillations\n",
    "\n",
    "The study of neutrino oscillations was the initial reason to develop\n",
    "this methodology. The physics needed to follow the argument is rather\n",
    "simple. Neutrino oscillations originate from the difference between\n",
    "neutrino mass and flavor eigenstates. Considering only two flavors\n",
    "$\\nu_e, \\mu_\\mu$ we can write the flavor eigenstates as a linear\n",
    "combination of the mass eigenstates: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "|\\nu_e  \\rangle &=&   |\\nu_1\\rangle \\cos \\theta + |\\nu_2\\rangle \\sin \\theta \\\\\n",
    "|\\nu_\\mu\\rangle &=& - |\\nu_1\\rangle \\sin \\theta + |\\nu_1\\rangle \\cos \\theta \\end{aligned}\n",
    "$$\n",
    "\n",
    "Studying this two-states quantum mechanical system we obtain that the\n",
    "probability of a $\\nu_\\mu$ to oscillate to $\\nu_e$ is :\n",
    "\n",
    "$$\n",
    "P(\\nu_\\mu \\to \\nu_e) = \\sin^2(2\\theta)\\sin^2\\left(\\frac{1.27 \\Delta m^2 L}{E}\\right)\n",
    "$$\n",
    "\n",
    "where $\\Delta m = |m_1^2 - m_2^2|$ in $eV^2$, L is the distance in km\n",
    "between the creation and the detection points and E is the energy of the\n",
    "neutrino in MeV. The experimental data are used to measure the\n",
    "oscillations parameters $\\Delta m^2$ and $\\sin^2 2\\theta$. The results\n",
    "are usually presented in a 2D plane $(\\Delta m^2,\\sin^2 2\\theta)$ where\n",
    "the excluded values (rejection region) of the parameters are on the\n",
    "right of the boundary (see {numref}`fig:exclPlot`).\n",
    "\n",
    "Ignoring the physics behind, just think of this problem as a fit of the\n",
    "data (number of events) to a function of two variables\n",
    "$f(x,y) = P(\\Delta m^2, \\sin^2 2\\theta)$.\n",
    "\n",
    "The computation of the excluded region has the complications seen above:\n",
    "the parameter $\\sin^2 2\\theta$ is bounded in $[0,1]$ and the expected\n",
    "number of oscillation events is extremely small on a potentially large\n",
    "background.\n",
    "\n",
    "The neutrino data and the expected background are collected in\n",
    "histograms in bins of energy as $N=\\{n_i\\}$ and $B=\\{b_i\\}$\n",
    "respectively. The signal contribution (i.e. the expected number of\n",
    "events coming from oscillations) is collected as a histogram in bins of\n",
    "energy $T=\\{\\mu_i|\\sin^2(2\\theta), \\Delta m^2\\}$.\n",
    "\n",
    "To find the upper limits on the oscillation parameters we can use the FC\n",
    "construction. To build the exclusion region (this time is in 2D!) we fix\n",
    "a point in the $(\\Delta m^2,\\sin^2 2\\theta)$ plane (typically the plane\n",
    "is finely binned and instead of a point we approximate one region in the\n",
    "plane with the averaged values of the parameters in that region) and\n",
    "compute the rank of the measured values using as ordering principle the\n",
    "likelihood ratio:\n",
    "\n",
    "$$\n",
    "R = \\frac{P(N|T(\\sin^2(2\\theta), \\Delta m^2))}{P(N|T(\\sin^2(2\\theta_{best}), \\Delta m_{best}^2))}\n",
    "$$\n",
    "\n",
    "where $\\sin^2(2\\theta_{best}), \\Delta m_{best}^2$ are the values giving\n",
    "the highest probability $P(N|T)$ in the physically allowed range of the\n",
    "parameters. This is equivalent to compute for each bin in the 2D plane:\n",
    "\n",
    "$$\n",
    "R = \\Delta \\chi^2 = 2\\sum_i \\left( \\mu_i -\\mu_{best} + n_i\\ln\\left(\\frac{\\mu_{best} + b_i}{\\mu_i + b_i}\\right) \\right)\n",
    "$$\n",
    "\n",
    "{numref}`fig:exclPlot` shows the exclusion region on the 2D plane\n",
    "obtained with a toy study. Neutrinos are created uniformly at a distance\n",
    "between 600m and 1000m from the detector with a flat energy spectrum\n",
    "between 10 and 60 GeV. The background flux from $\\nu_\\mu$ misidentified\n",
    "as $\\mu_e$ is set to 500 events flat in the energy range. The total\n",
    "$\\nu_\\mu$ flux is such that we get 100 events for an oscillation\n",
    "probability $P(\\nu_\\mu\\to\\nu_e)=0.01$. For each toy experiment we\n",
    "compute $\\Delta\\chi^2$. For each bin in the\n",
    "$(\\Delta m^2,\\sin^2 2\\theta)$ plane the we need to find\n",
    "$\\Delta \\chi^2_c$ such that $\\alpha$ (1-CL) of the simulated experiments\n",
    "have $\\Delta\\chi^2 < \\Delta\\chi^2_c$. To get the exclusion limit from\n",
    "data (the histogram $N$) we compute the $\\Delta \\chi^2$ for each bin of\n",
    "the $(\\Delta m^2,\\sin^2 2\\theta)$ plane and compare it with the\n",
    "$\\Delta \\chi_c^2$ found above. The boundary is given by:\n",
    "\n",
    "$$\n",
    "\\Delta \\chi^2 (N|\\sin^2(2\\theta),\\Delta m^2) < \\Delta \\chi^2_c(\\sin2(2\\theta),\\Delta m^22).\n",
    "$$\n",
    "\n",
    "```{figure} ./images/ch9/exclPlot.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:exclPlot\n",
    "---\n",
    "Confidence\n",
    "region for an example of the toy model in which $\\sin^2(2\\theta) = 0$.\n",
    "The 90% confidence region is the area to the right of the curve. In\n",
    "other words the null hypothesis H$_0$ is no oscillations and with the\n",
    "data at hand I can exclude at 90% CL the region of parameters to the\n",
    "right of the curve.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "The Feldman-Cousins construction was used in CMS [@Hig12045] for the\n",
    "determination of the intervals on the Higgs measured signal strength\n",
    "($\\hat{\\mu} = \\sigma/\\sigma_M$ ratio of the measured cross section to\n",
    "the Standard Model expectation). In {numref}`fig:higgsFC` the\n",
    "intervals obtained without limiting $\\hat{\\mu}$ to be positive and the\n",
    "one imposing $\\hat{\\mu}>0$ obtained with the FC prescription. In\n",
    "{numref}`fig:higgsFC2` the\n",
    "signal strength is fitted on the plane of the different production\n",
    "mechanisms gluon-gluon-fusion-plus-ttH and VBF-plus-VH.\n",
    "\n",
    "```{figure} ./images/ch9/higgsFC.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:higgsFC\n",
    "---\n",
    "(left) Values of\n",
    "$\\hat{\\mu} = \\sigma/\\sigma_M$ for the combination (solid vertical line)\n",
    "and for contributing channels (points). The horizontal bars indicate the\n",
    "$\\pm1\\sigma$ uncertainties on the $\\hat{\\mu}$ values for individual\n",
    "channels; they include both statistical and systematic uncertainties.\n",
    "(right) The same intervals imposing the signal strength to be positive\n",
    "and computed using the Feldman-Cousins\n",
    "construction.\n",
    "```\n",
    "\n",
    "```{figure} ./images/ch9/higgsFC2.png\n",
    "---\n",
    "width: 600px\n",
    "align: center\n",
    "name: fig:higgsFC2\n",
    "---\n",
    "(Left plot) The 68%\n",
    "CL intervals for signal strength in the gluon-gluon-fusion-plus-ttH and\n",
    "in VBF-plus-VH production mechanisms: $\\mu_{ggF + ttH}$ and\n",
    "$\\mu_{VBF+VH}$, respectively. The different colors show the results\n",
    "obtained by combining data from each of the five analyzed decay modes:\n",
    "$\\gamma\\gamma$ (green), $WW$ (blue), $ZZ$(red), $\\tau\\tau$ (violet),\n",
    "$bb$ (cyan). The crosses indicate the best-fit values. The diamond at\n",
    "(1,1) indicates the expected values for the SM Higgs boson. (right) The\n",
    "same intervals imposing the signal strength to be positive and computed\n",
    "using the Feldman-Cousins\n",
    "construction.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfluctuations and significance\n",
    "\n",
    "The FC prescription, while giving a frequentist solution to several\n",
    "problems, it stumbles on a problem with eperiments having large\n",
    "background underfluctuations. Suppose you have two experiments:\n",
    "\n",
    "-   a super optimized experiment expects no background events and\n",
    "    observes zero events in a given data taking period\n",
    "\n",
    "-   a less optimized experiment expects 10 background events and\n",
    "    observes five events in a given data taking period\n",
    "\n",
    "Using the FC prescription the upper limit on a signal at 90% CL for the\n",
    "first one is 2.44, while the second is 1.85. The worse experiment,\n",
    "clearly observing a lucky underfluctuation, obtains a better limit. This\n",
    "is not a desirable feature of the method. Quoting the authors: \"The\n",
    "origin of these concerns lies in the natural tendency to want to\n",
    "interpret these results as the probability $P(\\mu|x_0)$ of a hypothesis\n",
    "given data, rather than what they are really related to, namely the\n",
    "probability $P(x_0|\\mu)$ of obtaining data given a hypothesis. It is the\n",
    "former that a scientist may want to know in order to make a decision,\n",
    "but the latter which classical confidence intervals relate to. \\[\\...\\]\n",
    "scientists may make Bayesian inferences of $P(\\mu|x_0)$ based on\n",
    "experimental results combined with their personal, subjective prior\n",
    "probability distribution function. It is thus incumbent on the\n",
    "experimenter to provide information that will assist in this\n",
    "assessment.\" The suggested way is to provide, together with the limit,\n",
    "also the *sensitivity* of the experiment, defined as \"the average upper\n",
    "limit that would be obtained by an ensemble of experiments with the\n",
    "expected background and no true signal\". This extra information allows\n",
    "the reader to better understand if the value of a tight limit is just an\n",
    "artifact coming from an under-fluctuation of the background. When a\n",
    "significant portion of the upper limit curve is below the sensitivity of\n",
    "the experiment, the authors suggest to show both the sensitivity curve\n",
    "and the upper limit.\n",
    "\n",
    "![[\\[fig:exclPlotsensitivity\\]]{#fig:exclPlotsensitivity\n",
    "label=\"fig:exclPlotsensitivity\"}Confidence region for an example of the\n",
    "toy model in which $\\sin^2(2\\theta) = 0$ together with the sensitivity\n",
    "of the\n",
    "experiment.](Section9Bilder/exclPlotsensitivity.png){#fig:exclPlotsensitivity\n",
    "width=\"55%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEP test statistic: $L_{s+b}/L_b$\n",
    "---------------------------------\n",
    "\n",
    "To understand the test statistics $L_{s+b}/L_b$ we will use as an\n",
    "example the search for the SM Higgs at LEP.\n",
    "Fig. [1.27](#fig:LEPtight){reference-type=\"ref\"\n",
    "reference=\"fig:LEPtight\"} shows the Higgs candidate invariant mass\n",
    "distribution in data together with the histograms of the expected\n",
    "background and the expected signal at a mass of 115 GeV. The invariant\n",
    "mass is the discriminating variable used to extract the Higgs signal\n",
    "from background. We assume that, for each bin in invariant mass, we know\n",
    "what is the expected number of signal and background events. It is\n",
    "important to notice the difference between the *invariant mass* of a\n",
    "Higgs candidate $m_H^{rec}$, which is the invariant mass computed from\n",
    "the reconstructed decay products and the *test mass* $m_H$ which is the\n",
    "value of the mass at which we build the signal model we want to test.\\\n",
    "The typical search for Higgs boson is a combination of the analyses of\n",
    "different Higgs production (at LEP mostly Higgsstrahlung and vector\n",
    "boson fusion) and decay modes (predominantly $H\\to b\\bar{b}$ and\n",
    "$H\\to\\tau^+\\tau^-$). The (extended) likelihood used for each\n",
    "production/decay mode is:\n",
    "$$L_{s+b} = \\frac{(s(m_H)+b)^n}{n!}e^{-(s(m_H)+b)} \\prod_{j=1}^{n_{bins}}\\frac{s(m_H)S(x_j,m_H)+bB(x_j)}{s(m_H)+b}$$\n",
    "which, for the background only hypothesis reduces to:\n",
    "$$L_{b} = \\frac{b^n}{n!}e^{-b}\\prod_{j=1}^{n_{bins}}B(x_j).$$ Here $s$\n",
    "is the number of expected signal events (which is a function of the test\n",
    "mass $m_H$), $b$ the number of expected background events, $n$ the\n",
    "number of observed events, $x_j$ the value of the invariant mass\n",
    "$m_H^{rec}$ (our discriminating variable) and $S(x_j,m_H)$, $B(x_j)$ the\n",
    "signal (function of the test mass) and background pdf computed at $x_j$\n",
    "(i.e. the signal and background shapes).\n",
    "\n",
    "![[\\[fig:LEPtight\\]]{#fig:LEPtight\n",
    "label=\"fig:LEPtight\"}(left)Reconstructed invariant mass spectrum for the\n",
    "Higgs search at LEP; (right) Data (dots), signal (yellow histogram) and\n",
    "background (dashed histogram) as a function of $\\ln(1+s/b)$. In both\n",
    "cases the signal histogram is built for a test mass of\n",
    "115 GeV.](Section9Bilder/LEPtight.png \"fig:\"){#fig:LEPtight width=\"40%\"}\n",
    "![[\\[fig:LEPtight\\]]{#fig:LEPtight\n",
    "label=\"fig:LEPtight\"}(left)Reconstructed invariant mass spectrum for the\n",
    "Higgs search at LEP; (right) Data (dots), signal (yellow histogram) and\n",
    "background (dashed histogram) as a function of $\\ln(1+s/b)$. In both\n",
    "cases the signal histogram is built for a test mass of\n",
    "115 GeV.](Section9Bilder/ln1pSoverB.png \"fig:\"){#fig:LEPtight\n",
    "width=\"39%\"}\n",
    "\n",
    "The complete likelihood for the signal+background hypothesis is the\n",
    "product of the likelihoods of each production/decay: $$\\label{eq:Lsb}\n",
    "L_{s+b} = \\prod_{k=1}^{N}\\frac{(s_k(m_H)+b_k)^{n_k}}{n_k!}e^{-(s_k(m_H)+b_k)}\\prod_{j=1}^{n^k_{bins}}\\frac{s_k(m_H)S_k(x_{jk}; m_H)+b_kB_k(x_{jk})}{s_k(m_H)+b_k}$$\n",
    "where the index $k$ runs over the $N$ production/decay modes analysed.\n",
    "The likelihood for the background only is trivially obtained again\n",
    "setting $s$ to zero.\\\n",
    "The discovery test statistics used at LEP is based on the likelihood\n",
    "ratio: $$Q = \\frac{L_{s+b}}{L_b}.$$ For the same numerical reasons\n",
    "already encountered for the likelihood, instead of Q, the logarithm of Q\n",
    "is used[^3]: $$\\label{eq:qLEP}\n",
    "q = -2 \\ln Q = -2\\ln\\left(\\frac{L_{s+b}}{L_b}\\right).$$ Computing $q$\n",
    "explicitly with Eq. [\\[eq:Lsb\\]](#eq:Lsb){reference-type=\"ref\"\n",
    "reference=\"eq:Lsb\"} we obtain:\n",
    "$$q = -2\\ln Q(m_H) = 2\\sum_{k=1}^{N}\\left[ s_k(m_H) - \\sum_{j=1}^{n^k_{bins}} \\ln \\left( 1+\\frac{s_k(m_H)S_k(x_{jk}, m_H)}{b_k B_k(x_{jk})}\\right)\\right]$$\n",
    "This expression shows that each bin contributes to the likelihood with a\n",
    "weight $\\ln(1+S/B)$ to the final test statistics. Because of this, a\n",
    "typical way to present the results is to plot the data as a function of\n",
    "$\\ln(1+s/b)$ as in Fig. [1.27](#fig:LEPtight){reference-type=\"ref\"\n",
    "reference=\"fig:LEPtight\"}. The region at large values of $\\ln(1+s/b)$\n",
    "has the highest sensitivity to the signal.\\\n",
    "The test statistic $-2\\ln(Q)$ is used to test for the presence of signal\n",
    "in data. The intuition goes as: compute the test statistics on the\n",
    "collected data and compare it with the \"typical\" test statistic values\n",
    "under the signal+background or background only hypotheses (see\n",
    "Fig. [1.28](#fig:Q115){reference-type=\"ref\" reference=\"fig:Q115\"}). The\n",
    "\"typical\" values are obtained as the median of the pdf of the test\n",
    "statistic for the signal+background and the background only hypothesis.\n",
    "The pdfs are build from toy Monte Carlo samples. From the model in\n",
    "Eq. [\\[eq:Lsb\\]](#eq:Lsb){reference-type=\"ref\" reference=\"eq:Lsb\"} we\n",
    "generate several toy datasets and for each of them we compute the test\n",
    "statistics $-2\\ln(Q)$. The blue and brown pdfs in\n",
    "Fig. [1.28](#fig:Q115){reference-type=\"ref\" reference=\"fig:Q115\"} are\n",
    "the normalized histograms of the test statistics under the two\n",
    "hypothesis. To have a good estimation of the means, the histograms have\n",
    "to be well populated which means tossing a large number of toy\n",
    "experiments. This procedure is unfortunately very computing expensive.\n",
    "We will see later, when discussing the test statistics used at the LHC,\n",
    "how this problem can be mitigated. We can see from\n",
    "Fig. [1.28](#fig:Q115){reference-type=\"ref\" reference=\"fig:Q115\"} that\n",
    "the pdf for the background only hypothesis clusters at large $-2\\ln(Q)$\n",
    "values, while the signal+background at low values. The observed is\n",
    "somewhere in the middle and we will see in the next section how to use\n",
    "this value to set a limit on the Higgs boson production. Note that the\n",
    "values of the test statistic is a function of the test mass $m_H$, in\n",
    "this plot the chosen value is $m_H=115$ GeV. Scanning the value of the\n",
    "test mass we obtain Fig. [1.30](#fig:bigLEP){reference-type=\"ref\"\n",
    "reference=\"fig:bigLEP\"}-top. Here the median of the signal+background\n",
    "and the background-only hypotheses together with the observed value are\n",
    "plotted as curves (dashed brown and blue, and continuous black\n",
    "respectively) as a function of the test mass. The green(yellow) band\n",
    "covers the 68%(95%) area around the median of the background only\n",
    "hypothesis. These bands are a very common way to convey the statistical\n",
    "uncertainty of the background: presence of signal would appear in this\n",
    "plot as a deviation of the observed (black curve) from the expected for\n",
    "background only (dashed-blue); in absence of signal, the observed would\n",
    "be contained within the uncertainty bands. In\n",
    "Fig. [1.30](#fig:bigLEP){reference-type=\"ref\"\n",
    "reference=\"fig:bigLEP\"}-bottom the same curves are plotted separately\n",
    "for each of the four LEP experiments (ALEPH, DELPHI, L3, OPAL). We see\n",
    "that for all experiments but ALEPH the observed fluctuates around the\n",
    "expected background only curve. ALEPH shows a signal-like fluctuation\n",
    "around 115 GeV.\\\n",
    "To better characterize data fluctuations we can compute the already\n",
    "encountered $p$-value as the integral $$\\label{eq:pValueLEP}\n",
    "\\int_{-\\infty}^{-2\\ln(Q_{obs})=q_{obs}}pdf(q|bkg)dq.$$ The smaller the\n",
    "$p$-value (the further out in the tail the $q_{0,obs}$ lies), the\n",
    "poorest the agreement with the background only hypothesis.\n",
    "Conventionally if the $p$-value is below $p = 2.87 \\cdot 10^{-7}$,\n",
    "corresponding to a 5$\\sigma$ gaussian probability tail, we talk about\n",
    "\"discovery\". Fig. [1.31](#fig:pValues){reference-type=\"ref\"\n",
    "reference=\"fig:pValues\"} shows the $p$-value for the four LEP\n",
    "experiments combined and the separately for each of them. The ALEPH\n",
    "fluctuation observed in Fig. [1.30](#fig:bigLEP){reference-type=\"ref\"\n",
    "reference=\"fig:bigLEP\"} corresponds to a $p$-value of $\\sim3~10^{-3}$\n",
    "(i.e. $\\sim3\\sigma$), all other experiments are compatible with the\n",
    "background only hypothesis at that mass. The combined $p$-value shows a\n",
    "$\\sim2\\sigma$ fluctuation around 95 GeV and a smaller one at around 115\n",
    "GeV.\\\n",
    "\n",
    "![[\\[fig:Q115\\]]{#fig:Q115 label=\"fig:Q115\"}Distribution of the test\n",
    "statistic $-2\\ln(Q)$ for the signal+background hypothesis in brown and\n",
    "background only hypothesis in blue. The value of the test statistic\n",
    "computed on data (observed) is represented by the vertical black\n",
    "line.](Section9Bilder/Q115.png){#fig:Q115 width=\"40%\"}\n",
    "\n",
    "![[\\[fig:bigLEP\\]]{#fig:bigLEP label=\"fig:bigLEP\"}(top) Test statistics\n",
    "as a function of the test mass for the combined LEP experiments. The\n",
    "insert is Fig. [1.28](#fig:Q115){reference-type=\"ref\"\n",
    "reference=\"fig:Q115\"} rotated and placed at the test mass of 115 GeV.\n",
    "(bottom)Test statistics as a function of the test mass for the single\n",
    "LEP experiments.](Section9Bilder/bigLEP.png \"fig:\"){#fig:bigLEP\n",
    "width=\"60%\"} ![[\\[fig:bigLEP\\]]{#fig:bigLEP label=\"fig:bigLEP\"}(top)\n",
    "Test statistics as a function of the test mass for the combined LEP\n",
    "experiments. The insert is Fig. [1.28](#fig:Q115){reference-type=\"ref\"\n",
    "reference=\"fig:Q115\"} rotated and placed at the test mass of 115 GeV.\n",
    "(bottom)Test statistics as a function of the test mass for the single\n",
    "LEP experiments.](Section9Bilder/ADLO.png \"fig:\"){#fig:bigLEP\n",
    "width=\"50%\"}\n",
    "\n",
    "![[\\[fig:pValues\\]]{#fig:pValues label=\"fig:pValues\"}p-values as a\n",
    "function of the test mass for the combined (left) and single (right) LEP\n",
    "experiments.](Section9Bilder/pValues.png){#fig:pValues width=\"100%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuisance parameters\n",
    "\n",
    "So far we have not considered any uncertainty on the signal and\n",
    "background models. *Systematic* uncertainties can arise from several\n",
    "sources and can affect both the signal (e.g. energy scale affecting the\n",
    "position of the signal, energy resolution affecting its width, etc\\...)\n",
    "and the background (the shape could come from some control region or\n",
    "sidebands, etc\\...). To include this uncertainties, we add more\n",
    "parameters to the model. These parameters are called *nuisance\n",
    "parameters*[^4]: $\\vec{\\nu}$. For instance we can add an uncertainty\n",
    "$\\delta m$ on the mass position of the signal $m_0$. The effect of these\n",
    "uncertainties is to widen the test statistics pdfs. The reason for this\n",
    "is rather intuitive, we are reducing the information in the model by\n",
    "including uncertainties on the parameters, and so the separation power\n",
    "between signal+background and background-only is reduced. The $p$-values\n",
    "will become a function also of these extra parameters, e.g.:\n",
    "$$\\int_{-\\infty}^{q_{obs}}\\mbox{pdf}(q|m_0)dq \\qquad\\to\\qquad\\int_{-\\infty}^{q_{obs}}\\mbox{pdf}(q|m_0, \\delta m)dq.$$\n",
    "Ideally we would like any statement we make based on the pdf of the test\n",
    "statistics to be valid for any value of the nuisances. This turns out to\n",
    "be very restrictive when the values of the nuisance are disfavored by\n",
    "the data. At LEP a *\"hybrid frequentist-bayesian\"* procedure was\n",
    "followed to take into account nuisance parameters. Suppose you have a\n",
    "prior $\\pi(\\nu)$ describing the degree of belief on where the nuisance\n",
    "parameter $\\nu$ lies. From the example above, the uncertainty on the\n",
    "mass scale could be constrained by calibration measurements to be\n",
    "gaussian distributed. Using the prior we can marginalize the likelihood:\n",
    "$$L_{\\mbox{marginalized}} (x) = \\int L(x|\\nu) \\pi(\\nu) d\\nu$$ and then\n",
    "proceed using this new likelihood to perform any frequentist test\n",
    "($p$-value, intervals, etc\\...). The marginalization is usually done\n",
    "with Monte Carlo techniques, sampling the distribution $\\pi(\\nu)$ and\n",
    "computing the likelihood at the sampled value $\\nu$.\\\n",
    "The Neyman-Pearson lemma says that the likelihood ratio $L_{s+b}/L_b$\n",
    "for simple hypotheses is the optimal test statistics. The inclusion of\n",
    "nuisance parameters in the model changes the problem from the test of a\n",
    "simple hypothesis to a composite one, so strictly speaking the\n",
    "Neyman-Pearson lemma is not applicable. Nevertheless, when the nuisance\n",
    "parameters are well constrained, we can effectively consider the\n",
    "hypothesis to be simple and the likelihood ratio to be close to optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue of sensitivity and the CLs procedure\n",
    "----------------------------------------------\n",
    "\n",
    "In absence of a clear signal observation (conventionally corresponding\n",
    "to a significance of $3\\sigma$) we can still use our data to provide\n",
    "useful information about the largest signal we can exclude[^5]. We have\n",
    "already encountered the concept of upper limit in\n",
    "Sec. [1.1](#sec:Neyman){reference-type=\"ref\" reference=\"sec:Neyman\"}.\n",
    "Let's apply it to the Standard Model Higgs boson search at LEP. The\n",
    "search results were presented in terms of lower limits on the mass. The\n",
    "procedure to set the limit is straightforward. Look again at\n",
    "Fig. [1.28](#fig:Q115){reference-type=\"ref\" reference=\"fig:Q115\"}. In\n",
    "the previous section we used the observed value of the test statistics\n",
    "to define the $p$-value $p_b$ on the pdf of the background only\n",
    "hypothesis (yellow area). This gave us the probability of the background\n",
    "to over-fluctuate to mimic a signal. Now we use the observed value of\n",
    "the test statistics to define the $p$-value $p_{s+b}$ on the pdf of the\n",
    "signal+background hypothesis (green area). This will give us the\n",
    "probability for the signal+background to under-fluctuate to mimic\n",
    "absence of signal. When $p_{s+b} = 0.05$ we can exclude the presence of\n",
    "signal at the 95% CL. To set a limit on the mass of the SM Higgs boson\n",
    "we scan the values of the test mass until we find $p_{s+b} = 0.05$[^6].\\\n",
    "In Fig. [1.32](#fig:Qxx){reference-type=\"ref\" reference=\"fig:Qxx\"} we\n",
    "show the pdf of the test statistics for different test mass values. The\n",
    "central plot is again Fig. [1.28](#fig:Q115){reference-type=\"ref\"\n",
    "reference=\"fig:Q115\"}, the left one is computed at 110 GeV and the right\n",
    "one at 120 GeV. The lower the mass the larger the separation power\n",
    "between the signal+background and the background only hypothesis and\n",
    "this translates, for a given dataset, to lower and lower values of\n",
    "$p_{s+b}$. We can read this as \"given the expected SM-Higgs and the\n",
    "SM-backgrounds, it's easier to exclude low mass values than high ones\".\n",
    "Going to high masses we see that the overlap between the two pdfs\n",
    "increases. The physical meaning of this is that we are not able anymore\n",
    "to separate the two hypothesis (in this case, we're are reaching the\n",
    "kinematic limit of LEP to produce SM Higgs bosons).\n",
    "\n",
    "![[\\[fig:Qxx\\]]{#fig:Qxx label=\"fig:Qxx\"}The pdf of the test statistics\n",
    "for different test mass values.](Section9Bilder/Qxx.png){#fig:Qxx\n",
    "width=\"100%\"}\n",
    "\n",
    "This situation is rather dangerous. Let's take a deeper look at the\n",
    "meaning of the $p$-values in\n",
    "Fig. [1.33](#fig:pValuefluctuations){reference-type=\"ref\"\n",
    "reference=\"fig:pValuefluctuations\"} to understand why. The left plot\n",
    "focuses on the background only hypothesis: the right tail of the\n",
    "distribution contains experiments where the background under-fluctuates\n",
    "(i.e. we are lacking events also for the background only hypothesis),\n",
    "while the left tail instead contains the experiments where the backgroud\n",
    "over-fluctuates mimicking a signal. The right plot instead focuses on\n",
    "the signal+background hypothesis: the left tail contains experiments\n",
    "where the signal+background over-fluctuates, while in the right tail\n",
    "there are the under-fluctuations that mimic absence of signal.\n",
    "\n",
    "![[\\[fig:pValuefluctuations\\]]{#fig:pValuefluctuations\n",
    "label=\"fig:pValuefluctuations\"}Signal and background under/over\n",
    "fluctuations.](Section9Bilder/pValuefluctuations.png){#fig:pValuefluctuations\n",
    "width=\"100%\"}\n",
    "\n",
    "Let's contrast this picture with\n",
    "Fig. [1.34](#fig:noSep){reference-type=\"ref\" reference=\"fig:noSep\"}. In\n",
    "this case the two pdfs largely overlaps providing very small separation\n",
    "power. Here is where the situation becomes dangerous. Suppose we are\n",
    "setting a 95% CL on a signal and the test mass used for\n",
    "Fig. [1.34](#fig:noSep){reference-type=\"ref\" reference=\"fig:noSep\"} is\n",
    "the one providing $p_{s+b} = 0.05$. In this case we can exclude the\n",
    "presence of a signal with such a mass at 95% CL, but we are also sitting\n",
    "on the right tail of the background. *The background itself has an\n",
    "under-fluctuation!* From the point of view of a signal search, it\n",
    "doesn't make any sense. We are excluding the signal+background and the\n",
    "background-only hypotheses at the same CL. A situation like that could\n",
    "arise when trying to exclude a Higgs boson of 1 TeV at LEP where the\n",
    "kinematic reach is just above 100 GeV. While that is kinematically\n",
    "obvious, it is worth noticing that the procedure for limits settings\n",
    "detailed so far does not prevent to incur in such kind of troubles.\\\n",
    "\n",
    "![[\\[fig:noSep\\]]{#fig:noSep label=\"fig:noSep\"} Poor separation\n",
    "power.](Section9Bilder/noSep.png){#fig:noSep width=\"50%\"}\n",
    "\n",
    "The root of the problem is that there is not enough information used in\n",
    "the limit setting procedure and we risk a \"spurious exclusion\". A way to\n",
    "overcome this issue is to include somehow the information coming from\n",
    "$p_b$. This is what the \"CLs\" procedure does. To illustrate the \"CLs\"\n",
    "procedure we will follow the notation of the original paper\n",
    "Ref. [@CLsRead]. The $p_{s+b}$ is renamed \"confidence level\"[^7]. The\n",
    "confidence level of the signal+background hypothesis is then the\n",
    "integral $CL_{s+b} = P(Q>Q_{obs}|s+b)$ calculated on the p.d.f. for\n",
    "signal+background hypothesis. Small values of $CL_{s+b}$ correspond to a\n",
    "poor compatibility with the signal+background hypothesis and so favor\n",
    "the background only hypothesis and viceversa. The $CL_s$ procedure[^8]\n",
    "\"corrects\" the $CL_{s+b}$ dividing it by $CL_b$, defined as\n",
    "$CL_b = 1 - P(Q<Q_{obs}|b)$: $$\\label{eq:CLs}\n",
    "CL_s = \\frac{CL_{s+b}}{CL_b} = \\frac{p_{s+b}}{1-p_b}$$ Remember that\n",
    "despite the misleading name, this is not a confidence level ! It's not\n",
    "even a $p$-value, it's a ratio of p-values. Nevertheless we will say\n",
    "that a signal is excluded at the confidence level CL if $1-CL_s\\ge CL$.\\\n",
    "The false exclusion rate is now reduced:\n",
    "\n",
    "-   in case of a clear separation between the two hypothesis $p_b\\to 0$\n",
    "    and $CL_s \\to CL_{s+b}$, so we recover the standard p-value\n",
    "    definition;\n",
    "\n",
    "-   in case of poor separation $p_b \\to 1$ and $CL_s \\to 1$, preventing\n",
    "    spurious exclusions.\n",
    "\n",
    "The results of the CLs procedure applied to the Higgs search at LEP are\n",
    "shown in Fig. [1.35](#fig:LEPCLs){reference-type=\"ref\"\n",
    "reference=\"fig:LEPCLs\"}. This is the famous plot excluding Higgs masses\n",
    "below 114.4 GeV [@MarumiLEP].\n",
    "\n",
    "![[\\[fig:LEPCLs\\]]{#fig:LEPCLs label=\"fig:LEPCLs\"} LEP exclusion plot\n",
    "for the Standard Model Higgs\n",
    "search.](Section9Bilder/LEPCLs.png){#fig:LEPCLs width=\"50%\"}\n",
    "\n",
    "The CLs will deviate from the standard $p$-value the smaller the\n",
    "separation power of the test. The price to pay for this is that the\n",
    "limits obtained with the $CL_s$ procedure will by construction\n",
    "\"over-cover\" resulting in conservative limits (you exclude less phase\n",
    "space). This is clearly not a desirable feature for a frequentist-based\n",
    "approach, but because \"it works\" it has been adopted as the standard way\n",
    "to set the limits at colliders. Opponents to this rather arbitrary\n",
    "procedure advocates the use of a Bayesian approach, which on the other\n",
    "hand raises the usual issues about setting a prior on the parameter\n",
    "under test.\\\n",
    "For simplicity the CLs procedure has been detailed here using the LEP\n",
    "test statistics $L_{s+b}/L_b$, but it can be applied precisely in the\n",
    "same way to the LHC test statistics that we will encounter in the next\n",
    "section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LHC test statistics: $-2\\ln(\\lambda(\\mu))$\n",
    "------------------------------------------\n",
    "\n",
    "In this section we describe the LHC test statistics and review the large\n",
    "sample approximations (Wald's theorem and asymptotic formulas). We will\n",
    "develop the main concepts step by step using the discovery test\n",
    "statistic $q_0$. Then we will develop the test statistic for upper\n",
    "limits and give some examples. *In this section we will follow closely\n",
    "the paper in Ref.* [@asymptotic]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile likelihood ratio\n",
    "\n",
    "Throughout this section we will use a concrete example to keep a uniform\n",
    "notation and help visualizing the results. For this \"prototype\n",
    "experiment\", let's assume that the data are represented by a histogram\n",
    "$\\textbf{n} = (n_1,\\dots, n_N)$ in only one variable $x$. For example x\n",
    "could be the candidate invariant mass (the generalization to several\n",
    "variables is trivial). The expected number of events in each bin of the\n",
    "histogram depends on our expectations of the signal and background:\n",
    "$$E[n_i]=\\mu s_i + b_i$$ where:\n",
    "\n",
    "-   $s_i$ is the number of signal events expected in bin $i$:\n",
    "    $s_i = s_{tot} \\int_{bin_i} f_s(x|\\theta_s) dx$. The distribution\n",
    "    $f_s$ is the p.d.f. of the variable $x$ for the signal, $\\theta_s$\n",
    "    are the parameters characterising the shape of the signal and\n",
    "    $s_{tot}$ is the total mean number of expected signal events;\n",
    "\n",
    "-   $b_i$ is the number of background events expected in bin $i$:\n",
    "    $b_i = b_{tot} \\int_{bin_i} f_b(x|\\theta_b)dx$. The distribution\n",
    "    $f_b$ is the p.d.f. of the variable $x$ for the background,\n",
    "    $\\theta_b$ are the parameters characterising the shape of the\n",
    "    background and $b_{tot}$ is the total mean number of expected\n",
    "    background events.\n",
    "\n",
    "-   the parameter $\\mu$ is the *signal strength* that we have already\n",
    "    encountered and which allows to go in a continuous way from the\n",
    "    background only hypothesis $\\mu=0$ to the nominal signal+background\n",
    "    hypothesis $\\mu=1$\n",
    "\n",
    "We group all parameters, but the signal strength $\\mu$ our parameter of\n",
    "interest, in a vector $\\vec{\\theta}=(\\theta_s,\\theta_b,s_{tot},b_{tot})$\n",
    "of nuisance parameters.\\\n",
    "\\\n",
    "**Example** You want to extract the fraction of signal events in a data\n",
    "sample $D$. The statistical model used is:\n",
    "$$L(D|f_{sig}) = f_{sig}~\\mbox{Gauss}(m;m_0,\\sigma) + (1-f_{sig})~e^{-\\alpha m}$$\n",
    "where the observable $m$ is the invariant mass of the candidates,\n",
    "$f_{sig}$ is the fraction of signal events, $m_0$ is the position of the\n",
    "resonance, $\\sigma$ is the width of the resonance and $\\alpha$ is the\n",
    "slope of the background. The parameter of interest is $f_{sig}$, all the\n",
    "other parameters of the model are\n",
    "$\\vec{\\theta}=\\{m_0,~\\sigma,~\\alpha\\}$.\\\n",
    "\\\n",
    "Typically in a measurement, together with the main dataset, we use\n",
    "several other samples to help constraining the parameters of the model\n",
    "(e.g. the background in the signal region can be constrained by the\n",
    "measurement of the number of events in a control sample). These\n",
    "constraints are usually collected in auxiliary histograms\n",
    "$\\textbf{m} = (m_1, \\ldots, m_M)$: $$E[m_i] = u_i(\\theta)$$ where the\n",
    "$u_i$ are quantities that depend on $\\theta$ and model e.g. the shape of\n",
    "the background. With this we can build the complete likelihood used to\n",
    "model the data:\n",
    "$$L(\\mu,\\theta) = \\prod_{j=1}^{N} \\frac{(\\mu s_j + b_j)^{n_j}}{n_j!} e^{-(\\mu s_j + b_j)} \\prod_{k=1}^{M} \\frac{u_k^{m_k}}{m_k!} e^{-u_k}.$$\\\n",
    "The test statistic developed at the LHC is based on a **profile\n",
    "likelihood ratio** defined by: $$\\label{eq:LR}\n",
    "\\lambda(\\mu) = \\frac{L(\\mu,\\hat{\\hat{\\theta}})}{L(\\hat{\\mu},\\hat{\\theta})}$$\n",
    "where:\n",
    "\n",
    "-   $\\mu$ is the value we are testing\n",
    "\n",
    "-   $\\hat{\\hat{\\theta}}$ is the best fit of the nuisance parameters once\n",
    "    we fixed the $\\mu$ we want to test (i.e. conditional to the test\n",
    "    value $\\mu$ in the likelihood). We say in this case that the\n",
    "    parameters $\\theta$ are *profiled*. The value of\n",
    "    $\\hat{\\hat{\\theta}}$ is a function of $\\mu$.\n",
    "\n",
    "-   $\\hat{\\mu}$ and $\\hat{\\theta}$ are the best fit values of $\\mu$ and\n",
    "    $\\theta$ (the parameter of interest and the nuisances) when both are\n",
    "    left floating in the likelihood. In other words $\\hat{\\mu}$ and\n",
    "    $\\hat{\\theta}$ are the values that maximize the likelihood.\n",
    "\n",
    "The denominator of this ratio is just a likelihood function, the\n",
    "numerator is called \"profile likelihood\" and the ratio is called\n",
    "\"profile likelihood ratio\". The fitted value $\\hat{\\mu}$ is allowed to\n",
    "take any positive or negative (unphysical) values (provided that\n",
    "$\\mu s_i + b_i$ in the Poisson remains positive) even in the case where\n",
    "the search targets a positive signal. This assumption, rather arbitrary\n",
    "at this point, will be needed in the following to model $\\hat{\\mu}$ as a\n",
    "gaussian distributed variable, that will allow write the test statistics\n",
    "in an analytical closed form.\\\n",
    "The values taken by the profile likelihood ratio $\\lambda(\\mu)$ are in\n",
    "the interval $[0,1]$. The ratio will get to unity the closer the test\n",
    "value of $\\mu$ is to the value of $\\hat{\\mu}$ preferred by the data. On\n",
    "the contrary it will approach zero for test values of $\\mu$ very\n",
    "different from $\\hat{\\mu}$.\\\n",
    "As already noticed for the LEP test statistics, the inclusion of\n",
    "nuisance parameters changes the hypothesis under test from simple to\n",
    "composite. Generally, however, the nuisance parameters are well\n",
    "constrained and the likelihood ratio test is close to optimal. The\n",
    "inclusion of the nuisance parameters should give enough flexibility to\n",
    "the likelihood to be able to model the \"true\" unknown values of the\n",
    "parameters. Ideally, when making statements based on a test statistics\n",
    "(e.g. when setting a limit on $\\mu$ at $1-\\alpha$ level) we would like\n",
    "it to be correct for all values of the nuisances. In general it is not\n",
    "possible to cover all values of the nuisances and as consequence the\n",
    "coverage is not guaranteed. Nevertheless the choice of a profile\n",
    "likelihood ratio allows to have the correct coverage at least in a\n",
    "\"trajectory\" given by $(\\mu, \\hat{\\hat{\\theta}})$ [@Cranmer].\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery test statistics\n",
    "\n",
    "From the definition in Eq. [\\[eq:LR\\]](#eq:LR){reference-type=\"ref\"\n",
    "reference=\"eq:LR\"} we can build several test statistics. Instead of\n",
    "listing all of them, let's learn how to use one and come back later to\n",
    "some of the other cases. We will start from the **discovery test\n",
    "statistics** for a positive signal; this is the typical case for\n",
    "searches at the LHC. We want to test for $\\mu=0$ which correspond to the\n",
    "background only hypothesis. Rejecting the background only hypothesis\n",
    "corresponds to acknowledge the presence of something else in data which\n",
    "is not described correctly: a signal. The discovery test statistics for\n",
    "positive signals is defined as: $$\\label{eq:discovery}\n",
    "        q_0 = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                -2 \\ln \\lambda(0) & \\mbox{if} & \\hat{\\mu} \\ge 0 \\\\\n",
    "                0                 & \\mbox{if} & \\hat{\\mu} < 0 \n",
    "            \\end{array}\\right.$$ where $\\lambda(0)$ is the is the\n",
    "profile likelihood ratio for $\\mu = 0$ as defined in\n",
    "Eq.[\\[eq:LR\\]](#eq:LR){reference-type=\"ref\" reference=\"eq:LR\"}. With\n",
    "this definition, large values of $q_0$ correspond to increasing\n",
    "incompatibility between the data and the background only hypothesis.\n",
    "Remember that $\\mu$ is the value you are testing, in this case $\\mu=0$,\n",
    "while $\\hat{\\mu}$ is the value you fit from data (the so called \"best\n",
    "fit\"). The idea to have different definitions for positive and negative\n",
    "values of $\\hat{\\mu}$ has a simple explanation. If the measured value\n",
    "$\\hat{\\mu}$ is negative, it means that we are observing fewer events\n",
    "than what we would expect from the background only hypothesis. In\n",
    "absence of signal, that should happen 50% of the times: for large enough\n",
    "statistics allowing for a gaussian description, 50% of the times the\n",
    "background fluctuates up, 50% it fluctuates down. Under-fluctuations of\n",
    "the background are of no interest for the search of an excess of events\n",
    "in data. To discover a new signal we are only interested in upper\n",
    "fluctuations of data with little compatibility with the background only\n",
    "model, i.e. $\\hat{\\mu} \\ge 0$.\\\n",
    "To quantify the level of disagreement between the background only\n",
    "hypothesis and the observed value of the test statistics, we can compute\n",
    "the $p$-value $$p=\\int_{q_{0,obs}}^\\infty f(q_0|0) dq_0$$ $f(q_0|0)$\n",
    "denotes the pdf of the statistic $q_0$ under assumption of the\n",
    "background-only ($\\mu = 0$) hypothesis (see\n",
    "Fig. [1.36](#fig:MeasuredPValue){reference-type=\"ref\"\n",
    "reference=\"fig:MeasuredPValue\"}). Note that the extremes of the\n",
    "$p$-value integral are different from the LEP case in\n",
    "Eq. [\\[eq:pValueLEP\\]](#eq:pValueLEP){reference-type=\"ref\"\n",
    "reference=\"eq:pValueLEP\"} because of the different test statistics\n",
    "definition.\n",
    "\n",
    "![[\\[fig:MeasuredPValue\\]]{#fig:MeasuredPValue\n",
    "label=\"fig:MeasuredPValue\"}Example of a measured p-value when having the\n",
    "observed test statistics $q_{0,obs}$. Good compatibility with $H_{0}$\n",
    "corresponds to a $q_{0,obs}$ which is on the left side (i.e. has a large\n",
    "p-value), whereas a $q_{0,obs}$ on the far right indicates bad\n",
    "compatibility.](Section9Bilder/MeasuredPValue.pdf){#fig:MeasuredPValue\n",
    "width=\"40%\"}\n",
    "\n",
    "\\\n",
    "In order to compute the integral we need to know $f(q_0|0)$. The brute\n",
    "force way to build the p.d.f. for the test statistics $q_0$ is through\n",
    "toy experiments. Each toy experiment is created by generating random\n",
    "data on the background only model; to be representative of the\n",
    "luminosity in the data set we are analysing, the number of events in\n",
    "each toy has to match the one of the measured data sample. For each toy\n",
    "we then compute the test statistics and fill a histogram. The number of\n",
    "toy experiments to be generated depends on the significance we are\n",
    "trying to estimate. Because the $p$-value is computed by integrating the\n",
    "histogram above $q_{0,obs}$, we will need to properly populate the tail\n",
    "of the distribution above $q_{0,obs}$. To quantify a deviation\n",
    "corresponding to a discovery ($2.87 \\cdot 10^{-7}$) we will need to\n",
    "generate a number of toy experiments significantly larger than\n",
    "$1/(2.87 \\cdot 10^{-7})$\\\n",
    "\\\n",
    "**Example** Suppose you want to estimate the $p$-value for the measured\n",
    "test statistics $q_{0,obs}$ where the signal appears in the distribution\n",
    "of the invariant mass as gaussian bump on an exponentially falling\n",
    "background. The procedure is depicted in\n",
    "Fig. [1.37](#fig:toys){reference-type=\"ref\" reference=\"fig:toys\"}. From\n",
    "the exponential distribution describing the background we generate\n",
    "random events following e.g. the \"hit or miss\" method shown in\n",
    "Sec. [\\[sec:MC\\]](#sec:MC){reference-type=\"ref\" reference=\"sec:MC\"}. The\n",
    "number of events to be generated has to be same as the one\n",
    "experimentally collected (i.e. representing the same integrated\n",
    "luminosity). We repeat the data generation a large number of times, we\n",
    "compute the test statistics for each \"toy experiment\" and fill a\n",
    "histogram with those values. The observed $p$-value is simply the\n",
    "integral of the histogram from $q_{0,obs}$ to infinity.\n",
    "\n",
    "![[\\[fig:toys\\]]{#fig:toys label=\"fig:toys\"}Cartoon showing how to\n",
    "extract the $p$-value from a toy study. The pdf $f(q_0|0)$ is\n",
    "approximated by the histogram normalized to unit\n",
    "area.](Section9Bilder/toys.png){#fig:toys width=\"100%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Formulas\n",
    "\n",
    "So far we have seen how to build the test statistics $f(q_0|0)$ tossing\n",
    "toy experiments and how CPU expensive that is. In recent years Cowan at\n",
    "al. in Ref. [@asymptotic] have used results proved by Wilks and Wald in\n",
    "the early '40s to overcome this problem and find an analytic formula to\n",
    "describe the generic pdf $f(q_\\mu|\\mu')$.\\\n",
    "The Wald's theorem basically states that in the limit of a sufficiently\n",
    "large data sample we can approximate the test statistics\n",
    "$-2\\ln\\lambda(\\mu)$ (see Eq. [\\[eq:LR\\]](#eq:LR){reference-type=\"ref\"\n",
    "reference=\"eq:LR\"}) as: $$\\label{eq:wald}\n",
    "-2\\ln\\lambda(\\mu) = \\frac{(\\mu-\\hat{\\mu})^2}{\\sigma^2} + o\\left( \\frac{1}{\\sqrt{N}}\\right)$$\n",
    "where $\\mu$ is the value we are testing, $\\hat{\\mu}$ is the estimator of\n",
    "$\\mu$ and the width $\\sigma$ can be extracted from the second derivative\n",
    "of the likelihood (Fisher information) as\n",
    "$$V^{-1}_{ij} = -E\\left[ \\frac{\\partial^2 \\ln L}{\\partial \\theta_i \\partial \\theta_j} \\right]$$\n",
    "or using the Asimov dataset that we will describe in the next section.\\\n",
    "The importance of Wald's theorem is that, for large enough statistics,\n",
    "the estimator $\\hat{\\mu}$ is gaussian distributed around $\\mu'$ (the\n",
    "true value of the parameter $\\mu$ - unknown in data, only nature knows\n",
    "it, or known in a Monte Carlo sample, you decided its value) and that\n",
    "all the parameters of the gaussian distribution can be computed. The\n",
    "\"large enough\" sample limitation is needed to be able to neglect the\n",
    "term $o\\left( \\frac{1}{\\sqrt{N}}\\right)$, but we will see later that the\n",
    "approximation is valid for relatively low number of events.\\\n",
    "Neglecting the term $o(1\\sqrt{N})$ the test statistics\n",
    "$t_{\\mu} = -2\\ln \\lambda({\\mu})$ is distributed as a \"non-central\n",
    "$\\chi^2$\" distribution for one degree of freedom.\n",
    "$$f(t_\\mu|\\Lambda) = \\frac{1}{2\\sqrt{t_\\mu}}\\frac{1}{\\sqrt{2\\pi}}\\left[ \\exp \\left( -\\frac{1}{2} (\\sqrt{t_\\mu} + \\sqrt{\\Lambda})^2 \\right) + \\exp\\left( -\\frac{1}{2} (\\sqrt{t_\\mu} - \\sqrt{\\Lambda})^2 \\right) \\right]$$\n",
    "with the non-centrality parameter $\\Lambda$\n",
    "$$\\Lambda = \\frac{(\\mu-\\mu')^2}{\\sigma^2}$$ The Wilks' theorem is a\n",
    "special case of the Wald's theorem for $\\mu'=\\mu$, $\\Lambda = 0$. In\n",
    "that case $-2\\ln\\lambda(\\mu)$ approaches a $\\chi^2$ distribution for one\n",
    "degree of freedom.\\\n",
    "\\\n",
    "We can now apply these results to the discovery test statistics. In the\n",
    "approximation of large test statistics\n",
    "[\\[eq:wald\\]](#eq:wald){reference-type=\"ref\" reference=\"eq:wald\"}:\n",
    "$$\\label{eq:waldDiscovery}\n",
    "        q_0 = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                -2 \\ln \\lambda(0) & \\mbox{if} & \\hat{\\mu} \\ge 0 \\\\\n",
    "                0                 & \\mbox{if} & \\hat{\\mu} < 0 \n",
    "            \\end{array}\\right.\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "        q_0 = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                \\hat{\\mu}^2/\\sigma^2 & \\mbox{if} & \\hat{\\mu} \\ge 0 \\\\\n",
    "                0                    & \\mbox{if} & \\hat{\\mu} < 0 \n",
    "            \\end{array}\\right.$$ where the estimator $\\hat{\\mu}$ is\n",
    "gaussian distributed around the mean $\\mu'$. The pdf for the test\n",
    "statistics for a generic $\\mu'$ becomes:\n",
    "$$\\label{eq:asymptoticDiscoveryGen}\n",
    "f(q_0|\\mu') = \\left( 1-\\Phi\\left(\\frac{\\mu'}{\\sigma} \\right)\\right)\\delta(q_0) + \\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{q_0}} \\exp\\left[-\\frac{1}{2}\\left(\\sqrt{q_0} - \\frac{\\mu'}{\\sigma} \\right)^2\\right]$$\n",
    "The special case where $\\mu'=0$ , i.e. in the hypothesis of background\n",
    "only, this equation simplifies to $$\\label{eq:asymptoticDiscovery}\n",
    "f(q_0|0) = \\frac{1}{2}\\delta(q_0) + \\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{q_0}} e^{-\\frac{q_0}{2}}$$\n",
    "Compare this formula to the bottom left plot in\n",
    "Fig. [1.37](#fig:toys){reference-type=\"ref\" reference=\"fig:toys\"}. The\n",
    "delta function describes the first bin of the histogram; this comes from\n",
    "our choice to set the test statistics to zero when $\\hat{\\mu}<0$. The\n",
    "coefficient 1/2 of the delta function reflects the 50% probability of\n",
    "the background to fluctuated below zero. The exponentially falling part\n",
    "instead describes the tail of distribution for $\\hat{\\mu}>0$.\\\n",
    "To get to the $p$-value using toys, we had to integrate the histogram\n",
    "above the observed value of the test statistics. In the approximation of\n",
    "large sample, using Wald's theorem, this corresponds to the integral of\n",
    "Eq. [\\[eq:asymptoticDiscovery\\]](#eq:asymptoticDiscovery){reference-type=\"ref\"\n",
    "reference=\"eq:asymptoticDiscovery\"} above the observed value of the test\n",
    "statistics. The simple form of the pdf gives an even simpler expression\n",
    "for the cumulative of\n",
    "Eq. [\\[eq:asymptoticDiscoveryGen\\]](#eq:asymptoticDiscoveryGen){reference-type=\"ref\"\n",
    "reference=\"eq:asymptoticDiscoveryGen\"}\n",
    "$$F(q_0|\\mu') = \\Phi\\left( \\sqrt{q_0} - \\frac{\\mu'}{\\sigma} \\right)$$\n",
    "which, for $\\mu'=0$ as in\n",
    "Eq. [\\[eq:asymptoticDiscovery\\]](#eq:asymptoticDiscovery){reference-type=\"ref\"\n",
    "reference=\"eq:asymptoticDiscovery\"}, becomes:\n",
    "$$F(q_0|0) = \\Phi\\left( \\sqrt{q_0} \\right).$$ The $p$-value can them be\n",
    "simply computed as $$p_0 = 1-F(q_0|0)$$ obtaining for the significance\n",
    "$$Z_0 = \\Phi^{-1}(1-p_0) = \\sqrt{q_0}.$$ The signal significance is\n",
    "simply the square root of the observed test statistics! To fully\n",
    "appreciate this result, think about the evaluation of the significance\n",
    "for a $5\\sigma$ signal: using toys you need to produce $o(10^8)$ data\n",
    "samples to populate the high tail of $f(q_0|0)$ to compute the integral\n",
    "above $q_0^{obs}$ (and then convert it to a significance); with the\n",
    "asymptotic to get you just take the square root of $q_0^{obs}$: if\n",
    "$q_0^{obs} = 25$ you have a $5\\sigma$ deviation!\\\n",
    "\\\n",
    "**Example** Fig. [1.38](#fig:muHff){reference-type=\"ref\"\n",
    "reference=\"fig:muHff\"} shows the scan of the test statistics as a\n",
    "function of $\\mu$. The minimum is obtained when $\\hat{\\mu} = \\mu$ and it\n",
    "is zero by construction of the likelihood ratio. The intercept at\n",
    "$\\mu=0$ i.e. $q_0^{obs} = -2\\ln(\\lambda(0))$ is the square of the\n",
    "significance. This means that we can read off the vertical axis the\n",
    "significance of the signal as $\\sqrt{14.25} = 3.8\\sigma$.\\\n",
    "\\\n",
    "\n",
    "![[\\[fig:muHff\\]]{#fig:muHff label=\"fig:muHff\"}Observation of the Higgs\n",
    "coupling to fermions.](Section9Bilder/muHff.png){#fig:muHff width=\"50%\"}\n",
    "\n",
    "**Example** Fig. [1.39](#fig:pvalueHgg){reference-type=\"ref\"\n",
    "reference=\"fig:pvalueHgg\"} shows results of the $H\\to\\gamma\\gamma$\n",
    "search at CMS using the $p$-value computed at each test mass hypothesis.\n",
    "The black continuous line is the observed $p$-value, that is computed on\n",
    "the asymptotic pdf for $f(q_0|0)$ using the measured $q_0^{obs}$, the\n",
    "dashed black line represent the expected $p$-value for the Standard\n",
    "Model signal ($\\mu=1$). The other curves represent the observe and\n",
    "expected $p$-values for different datasets collected by CMS at the LHC\n",
    "with different centre of mass energies ($\\sqrt{s}=7$ TeV and\n",
    "$\\sqrt{s}=8$ TeV).\\\n",
    "\\\n",
    "\n",
    "![[\\[fig:pvalueHgg\\]]{#fig:pvalueHgg label=\"fig:pvalueHgg\"}Expected\n",
    "(dashed) and observed (continuous) $p$-values for the CMS\n",
    "$H\\to\\gamma\\gamma$ search. The red curves are computed on the first data\n",
    "collected in 2011, the blue ones for the data collected in 2012 and the\n",
    "black curves on the two data set\n",
    "combined.](Section9Bilder/Hggpvalue.png){#fig:pvalueHgg width=\"60%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asimov dataset\n",
    "\n",
    "There are cases where you want to have the estimation of the *expected*\n",
    "significance of a signal. Typically this happens during the design phase\n",
    "of an experiment or, after a measurement, when you want to compare the\n",
    "observed and expected significances. To do this you need to have access\n",
    "to two pdfs: $f(q_0|0)$, the distribution of the test statistics in the\n",
    "background only hypothesis and $f(q_0|1)$, the distribution of the test\n",
    "statistics in the signal+background hypothesis. In the latter case\n",
    "$\\mu=1$ indicates the expected value of the signal strength. A sketch of\n",
    "these two functions is shown in\n",
    "Fig. [1.40](#fig:expectedSignificance){reference-type=\"ref\"\n",
    "reference=\"fig:expectedSignificance\"}.\n",
    "\n",
    "![[\\[fig:expectedSignificance\\]]{#fig:expectedSignificance\n",
    "label=\"fig:expectedSignificance\"}Discovery statistics distribution under\n",
    "the background only $f(q_0|0)$ and signal + background $f(q_0|1)$.\n",
    "](Section9Bilder/expectedSignificance.png){#fig:expectedSignificance\n",
    "width=\"50%\"}\n",
    "\n",
    "We have already encountered above the pdf $f(q_0|0)$: the most probable\n",
    "value of the test statistic is zero and the large tail corresponds to\n",
    "signal-like fluctuation of the background only hypothesis. The pdf\n",
    "$f(q_0|1)$ instead clusters at high values of $q_0$. We can understand\n",
    "this from the definition of the likelihood ratio\n",
    "$\\lambda(0) = L(0, \\hat{\\hat{\\theta}})/L(\\hat{\\mu},\\hat{\\theta})$. Here\n",
    "$\\hat{\\mu} = 1$ by construction, so the ratio $\\lambda(0)$ will cluster\n",
    "around small values of the test statistics and consequently\n",
    "$-2\\ln(\\lambda(0))$ will cluster at large ones.\\\n",
    "To compute the expected significance of a signal, we compute the\n",
    "$p$-value as the integral from the median of the $f(q_0|1)$ distribution\n",
    "to infinity. We use the median as the \"most representative\" value for\n",
    "the expected signal+background.\\\n",
    "To compute the median value of the test statistics we first need the pdf\n",
    "from toys or the asymptotic formulas. Can we produce a single dataset\n",
    "such that if we compute the test statistics on it we get the median\n",
    "value of the pdf ? This is the idea behind the \"Asimov[^9]\" dataset.\n",
    "This can be thought as \"the perfect average\" of the experiments outcome.\n",
    "To understand how to build this dataset, we can use the prototype\n",
    "analysis:\n",
    "$$L(\\mu,\\theta) = \\prod_{j=1}^{N} \\frac{(\\mu s_j + b_j)^{n_j}}{n_j!} e^{-(\\mu s_j + b_j)} \\prod_{k=1}^{M} \\frac{u_k^{m_k}}{m_k!} e^{-u_k}$$\n",
    "From here we can find the ML estimator for the parameters as\n",
    "$$\\frac{\\partial \\ln L}{\\partial \\theta_j} = \\sum_{i=1}^{N} \\left( \\frac{n_i}{\\nu_i}-1  \\right) \\frac{\\partial \\nu_i}{\\partial \\theta_i} + \\sum_{i=1}^{M}\\left( \\frac{m_i}{u_i} -1 \\right) \\frac{\\partial u_i}{\\partial \\theta_j} = 0$$\n",
    "and define the Asimov dataset bin by bin as: $$\\begin{aligned}\n",
    "n_{i,A} &=& E[n_i] = \\nu_i = \\mu' s_i(\\theta) + b_i(\\theta)\\\\\n",
    "m_{i,A} &=& E[m_i] = u_i(\\theta).\\end{aligned}$$ Each bin of the Asimov\n",
    "dataset has by construction the number of entries equal to the expected\n",
    "value, with the correct statistical uncertainty. The only difference\n",
    "with respect to a toy data set is that there are no statistical\n",
    "fluctuation associated to the entries per bin.\\\n",
    "On the Asimov dataset we can compute the Asimov likelihood and use it to\n",
    "compute the likelihood ratio:\n",
    "$$\\lambda_A(\\mu) = \\frac{L_A(\\mu, \\hat{\\hat{\\theta}})}{L_A(\\hat{\\mu},\\hat{\\theta})} \\sim \\frac{L_A(\\mu, \\hat{\\hat{\\theta}})}{L_A(\\mu',\\theta)}$$\n",
    "where by construction of the Asimov, $\\hat{\\mu}=\\mu'$ and the nuisance\n",
    "parameters $\\hat{\\theta}=\\theta$.\\\n",
    "The Asimov can also be used to obtain the width $\\sigma$ for the\n",
    "approximate formula in Eq. [\\[eq:wald\\]](#eq:wald){reference-type=\"ref\"\n",
    "reference=\"eq:wald\"}:\n",
    "$$q_\\mu = -2\\ln\\lambda(\\mu) = \\frac{(\\mu-\\hat{\\mu})^2}{\\sigma^2} + o\\left( \\frac{1}{\\sqrt{N}}\\right)  \\qquad\\Rightarrow\\qquad q_{\\mu,A}  = -2\\ln\\lambda_A(\\mu) \\sim \\frac{(\\mu -\\mu')^2}{\\sigma^2}.$$\n",
    "From here we can extract\n",
    "$$\\sigma_A^2 = \\frac{(\\mu - \\mu')^2}{q_{\\mu,A}}$$ Finally, it's easy to\n",
    "verify that the test statistic computed on the Asimov dataset coincides\n",
    "with the median of the distribution $f(q_\\mu|\\mu')$:\n",
    "$$\\mbox{med}[q_0] = q_0(\\mbox{med}[\\hat{\\mu}]) = \\frac{\\mu'}{\\sigma} = -2\\ln \\lambda_A(0)$$\n",
    "where the first equality comes from the fact that the median of the\n",
    "$q_0$ is the value of $q_0$ computed at the median value of $\\hat{\\mu}$,\n",
    "the second equality comes by construction of the Asimov dataset\n",
    "$\\mbox{med}[\\hat{\\mu}] = \\mu'$, $q_0(\\mu') = (0-\\mu')^2/\\sigma^2$ and\n",
    "the last one comes from the Wald's theorem in\n",
    "Eq. [\\[eq:waldDiscovery\\]](#eq:waldDiscovery){reference-type=\"ref\"\n",
    "reference=\"eq:waldDiscovery\"}. The expected significance\n",
    "$Z_0 = \\sqrt{q_0}$ computed on the Asimov dataset is then simply\n",
    "$\\mbox{med}[Z_0] = \\sqrt{-2\\ln\\lambda_A(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper limits test statistic\n",
    "\n",
    "To set an upper limit we can define the following test statistics:\n",
    "$$q_\\mu = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                -2 \\ln \\lambda(\\mu) & \\mbox{if} & \\hat{\\mu} \\le \\mu \\\\\n",
    "                0                 & \\mbox{if} & \\hat{\\mu} > \\mu \n",
    "            \\end{array}\\right.$$ Here we are using the same profile\n",
    "likelihood ratio as for the discovery test statistics, but this time we\n",
    "test for a generic $\\mu$. Notice that that is not the only difference in\n",
    "the definition. The $\\le$ and $>$ signs are swapped with respect to\n",
    "Eq. [\\[eq:discovery\\]](#eq:discovery){reference-type=\"ref\"\n",
    "reference=\"eq:discovery\"}. The reason for this is that, given that we\n",
    "are only considering cases where signal is associated to an excess of\n",
    "events above the background, we can only exclude an hypothesised value\n",
    "of $\\mu$ if the observed $\\hat{\\mu}$ fluctuates below that. Vice versa,\n",
    "we cannot exclude a value of $\\mu$ if the observed $\\hat{\\mu}$ is\n",
    "measured above the tested value $\\mu$ and so we set the test statistics\n",
    "to zero.\\\n",
    "As we have already done for the discovery test statistics we can define\n",
    "a $p$-value as: $$p_\\mu = \\int_{q_{\\mu,obs}}^\\infty f(q_\\mu|\\mu)dq_\\mu$$\n",
    "How do we set a upper limit with this test statistics? Suppose you want\n",
    "to set an upper limit at 95% CL. You need to scan the values of $\\mu$\n",
    "until you find the largest value of $\\mu$ such that the $p$-value is\n",
    "equal to 0.05. Practically you will also need to guess what is the range\n",
    "of $\\mu$ values to scan and what step to use in the scan.\\\n",
    " To set the expected upper limit on the signal strength of a signal you\n",
    "need first to get the pdf for $f(q_\\mu|\\mu')$. Analogous to what we have\n",
    "seen with the discovery test statistics the distributions for $\\mu'=\\mu$\n",
    "and $\\mu'\\neq\\mu$ are as shown in\n",
    "Fig. [1.41](#fig:expectedUpperLimit){reference-type=\"ref\"\n",
    "reference=\"fig:expectedUpperLimit\"}. In particular we will need the pdf\n",
    "$f(q_\\mu|0)$ describing the test statistics in absence of signal, from\n",
    "which we extract the median value, and then scan the values of $\\mu$\n",
    "(same procedure used for the observed) until you find the largest value\n",
    "of $\\mu$ such that the $p$-value is equal to the desired $\\alpha$=1-CL.\n",
    "\n",
    "![[\\[fig:expectedUpperLimit\\]]{#fig:expectedUpperLimit\n",
    "label=\"fig:expectedUpperLimit\"} Sketch of the pdf for $f(q_\\mu|\\mu')$.\n",
    "](Section9Bilder/expectedUpperLimit.png){#fig:expectedUpperLimit\n",
    "width=\"50%\"}\n",
    "\n",
    "\\\n",
    "Very often when displaying the expected upper limits we also show the\n",
    "1$\\sigma$ and 2$\\sigma$ uncertainty bands around the expected median. To\n",
    "do this, from the median value (i.e. 50% quantile) of the distribution\n",
    "giving the desired $\\alpha$ = 1-CL, i.e. med$[q_\\mu|0]$, we can quote\n",
    "the \"median$\\pm 1 \\sigma$\" (16%, 84% quantiles), and the\n",
    "\"median$\\pm 2 \\sigma$\" (5%, 95% quantiles), as shown in\n",
    "Fig. [1.42](#fig:expectedBands){reference-type=\"ref\"\n",
    "reference=\"fig:expectedBands\"}.\n",
    "\n",
    "![[\\[fig:expectedBands\\]]{#fig:expectedBands\n",
    "label=\"fig:expectedBands\"}Median (50% quantile) and median $-1\\sigma$\n",
    "(15.87% quantile).](Section9Bilder/expectedBands.png){#fig:expectedBands\n",
    "width=\"110%\"}\n",
    "\n",
    "The observed limits together with the expected one and its respective\n",
    "uncertainty bands are shown as a function of the test mass in\n",
    "Fig. [1.43](#fig:exclusionLimit){reference-type=\"ref\"\n",
    "reference=\"fig:exclusionLimit\"}.\n",
    "\n",
    "![[\\[fig:exclusionLimit\\]]{#fig:exclusionLimit\n",
    "label=\"fig:exclusionLimit\"} Describe the red line as SM\n",
    "limit.](Section9Bilder/exclusionLimit.png){#fig:exclusionLimit\n",
    "width=\"80%\"}\n",
    "\n",
    "Using Wald's theorem the asymptotic formula for the test statistic\n",
    "becomes: $$q_\\mu = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                -2 \\ln \\lambda(\\mu) & \\mbox{if} & \\hat{\\mu} \\le \\mu \\\\\n",
    "                0                 & \\mbox{if} & \\hat{\\mu} > \\mu \n",
    "            \\end{array}\\right.\n",
    "         \\qquad \\Rightarrow \\qquad\n",
    "        q_\\mu = \\left\\{\n",
    "            \\begin{array}{rll}\n",
    "                \\frac{(\\mu-\\hat{\\mu})^2}{\\sigma^2}& \\mbox{if} & \\hat{\\mu} \\le \\mu \\\\\n",
    "                0                 & \\mbox{if} & \\hat{\\mu} > \\mu \n",
    "            \\end{array}\\right.$$ where $\\hat{\\mu}$ is, as for the\n",
    "discovery test statistics, gaussian distributed around $\\mu'$ with\n",
    "standard deviation $\\sigma$. From this expression, it is possible to\n",
    "compute the closed form expression for $f(q_\\mu|\\mu')$:\n",
    "$$f(q_\\mu|\\mu') = \\Phi\\left(\\frac{\\mu' - \\mu}{\\sigma} \\right)\\delta(q_\\mu) + \\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{q_\\mu}} \\exp\\left[-\\frac{1}{2}\\left(\\sqrt{q_\\mu} - \\frac{\\mu-\\mu'}{\\sigma} \\right)^2\\right]$$\n",
    "which, for the special case where $\\mu = \\mu'$, becomes:\n",
    "$$f(q_\\mu|\\mu) = \\frac{1}{2}\\delta(q_\\mu) + \\frac{1}{2}\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{q_\\mu}} e^{-\\frac{q_\\mu}{2}}$$\n",
    "Using the cumulative distribution\n",
    "$F(q_\\mu|\\mu') = \\Phi\\left(\\sqrt{q_\\mu}-\\frac{\\mu - \\mu'}{\\sigma}\\right)$\n",
    "we get $p_\\mu = 1-F(q_\\mu|\\mu') = 1 - \\Phi(\\sqrt{q_\\mu})$, which, as for\n",
    "the discovery test statistic, gives:\n",
    "$$Z_\\mu = \\Phi^{-1}(1-p_\\mu)=\\sqrt{q_\\mu}$$ The upper limit at\n",
    "$(1-\\alpha)$ CL on $\\mu$ is the largest value of $\\mu$ such that\n",
    "$p_\\mu\\leq \\alpha$. With the asymptotic formulas we just need to set\n",
    "$p_\\mu = \\alpha$ and solve\n",
    "$\\mu_{up} = \\hat{\\mu} + \\sigma\\Phi^{-1}(1-\\alpha)$.\\\n",
    "\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining measurements\n",
    "----------------------\n",
    "\n",
    "As we have already seen in\n",
    "Sec. [\\[sec:likelihood\\]](#sec:likelihood){reference-type=\"ref\"\n",
    "reference=\"sec:likelihood\"}, the results of different measurements of a\n",
    "given parameter $\\mu$ can easily be combined by multiplying their\n",
    "likelihoods: $$L(\\mu,\\vec{\\theta}) = \\prod_i L_i(\\mu, \\vec{\\theta}_i)$$\n",
    "where the subscript $i$ stands for the different experiments (e.g.\n",
    "ATLAS, CMS) or different processes (e.g. $H\\to \\gamma\\gamma$, $H\\to ZZ$,\n",
    "etc\\...).\\\n",
    "Whenever different measurements are combined we need to pay attention at\n",
    "the possible correlations between the nuisances among the different\n",
    "experiments/channels. Typically the nuisances are chosen to be:\n",
    "\n",
    "-   uncorrelated: we use different parameters in the likelihoods of the\n",
    "    different experiments to describe the nuisance\n",
    "\n",
    "-   fully correlated: we use the same parameter in all the likelihoods\n",
    "    to describe the nuisance\n",
    "\n",
    "-   fully anticorrelated: we use again one parameter but we flip its\n",
    "    sign.\n",
    "\n",
    "To compute the expected significance for the combination the easiest\n",
    "approach is to use the Asimov dataset:\n",
    "$$\\lambda_A(\\mu) = \\prod_i \\lambda_{A,i}(\\mu) \\qquad \\mbox{where}\\qquad \\lambda_{A,i}(\\mu) = \\frac{L_{A,i}(\\mu, \\hat{\\hat{\\theta}})}{L_{A,i}(\\hat{\\mu},\\hat{\\theta})} = \\frac{L_{A,i}(\\mu, \\hat{\\hat{\\theta}})}{L_{A,i}(\\hat{\\mu'},\\hat{\\theta})}.$$\n",
    "where the last equality comes from the properties of the Asimov dataset\n",
    "($\\hat{\\mu}$ converges by construction to $\\mu'$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovery significance: $S/\\sqrt{B}$\n",
    "------------------------------------\n",
    "\n",
    "Consider a counting experiment where you observe $n$ events, the\n",
    "expected number of background events is $b$ and the expected number of\n",
    "events in case of signal is $s+b$. To simplify let's first consider the\n",
    "case of large statistics such that the we can approximate the Poisson\n",
    "distribution with a Gaussian $G(x|\\mu, \\sigma)$ with $\\mu = s+b$ and\n",
    "$\\sigma = \\sqrt{s+b}$ (the fact that the n is a discrete variable, wile\n",
    "x is continuous is irrelevant in this context).\\\n",
    "The significance to reject the background hypothesis can be quantified\n",
    "as the $p$-value associated to the observation of $x$ events\n",
    "Prob$(x>x_{obs} | s = 0)$ which, for a gaussian, is simply\n",
    "$p_0 = 1-\\Phi((x_{obs}-b)/\\sqrt{b})$. The significance is then\n",
    "$Z_0 = \\Phi^{-1}(1-p_0) = (x_{obs} - b)/\\sqrt(b)$.\\\n",
    "The median significance to for a signal $s\\neq0$ can be computed in the\n",
    "same way replacing $x_{obs}$ with the median signal plus background i.e.\n",
    "$s+b$:\n",
    "$$\\mbox{median}[Z_0 | s+b] = \\frac{s+b-b}{\\sqrt{b}} = \\frac{s}{\\sqrt{b}}$$\n",
    "giving the famous formula for the the signal significance. This is the\n",
    "typical quantity we try to maximize when optimizing a selection in a\n",
    "search.\\\n",
    "This formula is only valid in the limit of large statistics. For the\n",
    "most general case we need to go back to the Poisson distribution:\n",
    "$$L(n|s+b) = \\mbox{Poisson}(n|s+b) = \\frac{(s+b)^n e^{-(s+b)}}{n!}$$ or\n",
    "taking the logarithm $$\\ln L(n|s+b) = n\\ln(s+b) -(s+b) -\\ln n!$$\n",
    "Recalling that the ML estimator for $s$ is simply $\\hat{s} = n-b$, we\n",
    "can build the test statistic: $$q_0 = \\left\\{\n",
    "            \\begin{array}{lll}\n",
    "                -2 \\ln \\lambda(0)   = -2\\ln \\frac{L(n|0+\\hat{b})}{L(n|\\hat{s}+\\hat{b})}  = 2( n \\ln \\frac{n}{b} + b -n) & \\mbox{if} & n > b \\\\\n",
    "                0                 & \\mbox{if} & n \\leq b \n",
    "            \\end{array}\\right.$$ Now we can use Wald's theorem and\n",
    "rewrite the significance to reject the background only hypothesis as:\n",
    "$$Z_0 \\sim \\left\\{\n",
    "            \\begin{array}{lll}\n",
    "                \\sqrt{q_0} = \\sqrt{2( n \\ln \\frac{n}{b} + b -n)} & \\mbox{if} & n > b \\\\\n",
    "                0                 & \\mbox{if} & n \\leq b \n",
    "            \\end{array}\\right.$$ The median significance from the\n",
    "expected signal plus background $s+b$ is:\n",
    "$$\\mbox{median}[Z_0 | s+b] = \\sqrt{2( (s+b) \\ln (s/b +1) -s)}.$$ This\n",
    "formula can be considered as a generalization of $s/\\sqrt{b}$. If we\n",
    "expand this result to the second order in the limit of $s << b$ we get\n",
    "back to $s/\\sqrt{b}$. This is an important condition to keep in mind\n",
    "when using $s/\\sqrt{b}$. If $s$ and $b$ are both large, then the\n",
    "$p$-value goes to zero! The $p$-value is the probability to observe a\n",
    "fluctuation as large or larger than the one observed, if both $s$ and\n",
    "$b$ are large that probability is vanishingly small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from the search of the Higgs at the LHC\n",
    "------------------------------------------------\n",
    "\n",
    "In this section we will walk through some results from search for the\n",
    "Higgs boson at the LHC, taking the $H\\to \\gamma\\gamma$ as the\n",
    "conceptually easy example of a search of a \"bump\" on top of a falling\n",
    "background. All plots are taken from the public CMS results [@CMShiggs]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best fit signal strength\n",
    "\n",
    "The plot in Fig. [1.44](#fig:bestfitmuhat){reference-type=\"ref\"\n",
    "reference=\"fig:bestfitmuhat\"} shows the best fit of the signal strength\n",
    "modifier $\\hat{\\mu}$ as a function of the test mass $m_H$. The value\n",
    "obtained fluctuates around $\\hat{\\mu}\\sim 0$ everywhere in but in the\n",
    "region around 125 GeV where the excess was observed in the exclusion\n",
    "limits (Fig. [1.43](#fig:exclusionLimit){reference-type=\"ref\"\n",
    "reference=\"fig:exclusionLimit\"}) and in the $p$-value plot in\n",
    "Fig. [1.39](#fig:pvalueHgg){reference-type=\"ref\"\n",
    "reference=\"fig:pvalueHgg\"}. In that region, the signal strength steeply\n",
    "raise to a value compatible with the Standard Model expectation of\n",
    "$\\mu=1$. The green band represent the uncertainty on $\\hat{\\mu}$ defined\n",
    "on the likelihood ratio as\n",
    "$-2\\ln \\lambda(\\mu) = -2 \\ln L(\\mu) / L(\\hat{\\mu}) <1$ which is\n",
    "equivalent to the familiar 68% uncertainty band on a fitted parameter in\n",
    "a maximum likelihood fit $\\ln L(\\mu) > \\ln L(\\hat{\\mu}) - 1/2$.\n",
    "\n",
    "![[\\[fig:bestfitmuhat\\]]{#fig:bestfitmuhat label=\"fig:bestfitmuhat\"}\n",
    "Best fit of the signal strength $\\hat{\\mu}$ as a function of the test\n",
    "mass $m_H$.](Section9Bilder/bestfitmuhat.png){#fig:bestfitmuhat\n",
    "width=\"60%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting other parameters\n",
    "\n",
    "Up to now we have considered only the case where the parameter of\n",
    "interested in our likelihood was the signal strength. Given that all\n",
    "parameters in the likelihood are treated in the same way, we can use the\n",
    "same techniques to extract information about any other parameter. To\n",
    "study the properties of the Higgs boson, we rewrite the signal \"$s$\" as\n",
    "a function of the parameter we are interested in, let's call it \"$a$\",\n",
    "and plug $s(a)$ in the likelihood. Then all we need to do is to rewrite\n",
    "the test statistics as:\n",
    "$$q(a) = -2 \\ln \\frac{L(\\mbox{data}|s(a)+b, \\hat{\\theta}_a)}{L(\\mbox{data}|s(\\hat{a})+b, \\hat{\\theta})}.$$\n",
    "As in the previous definition of the test statistics, the profile\n",
    "likelihood at the numerator is maximized fixing $a$ and floating\n",
    "$\\hat{\\theta}_a$, the value of the nuisance parameters once $a$ is fixed\n",
    "(i.e. the nuisances are profiled as before), and, at the denominator,\n",
    "the likelihood is maximized against both $a$ and $\\theta$. As before the\n",
    "68% and 95% CL interval on $a$ is evaluated from $q(a) = 1~(4)$ with all\n",
    "other unconstrained parameters treated as nuisance parameters.\\\n",
    "The same idea can be used to scan simultaneously two parameters of\n",
    "interest $a$ and $b$. In this case the 68% and 95% CL interval becomes a\n",
    "2D region such that $q(a,b) = 2.3~(6)$. It is important to remember that\n",
    "the boundaries of the 2D confidence regions projected on either\n",
    "parameter axis are not necessarily identical to the 1D confidence\n",
    "interval for that parameter, because of the possible correlations\n",
    "between the two (I'm integrating on all the other variables I'm not\n",
    "considering). Examples of 1D and 2D parameter scans are shown in\n",
    "Fig. [1.45](#fig:scans1D2D){reference-type=\"ref\"\n",
    "reference=\"fig:scans1D2D\"}. All the properties of the Higgs boson are\n",
    "extracted from data using this procedure.\n",
    "\n",
    "![[\\[fig:scans1D2D\\]]{#fig:scans1D2D label=\"fig:scans1D2D\"} Examples of\n",
    "parameters scans in 1D (left: $m_H$) and 2D (right:\n",
    "$(\\mu,m_H)$).](Section9Bilder/scans1D2D.png){#fig:scans1D2D width=\"90%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian approach to upper limits\n",
    "---------------------------------\n",
    "\n",
    "The standard prescription to present the LHC results is based on the\n",
    "frequentist paradigma. Nevertheless it is interesting to see how to\n",
    "extract the same results using the bayesian statistics.[^10] Using Bayes\n",
    "theorem we can write the posterior for the signal strength as:\n",
    "$$f(\\mu) = f(\\mu|\\mbox{data}) = \\int\\frac{1}{N}L(\\mbox{data}|\\mu,\\vec{\\theta})\\pi(\\vec{\\theta})d\\vec{\\theta}$$\n",
    "where $\\pi$ is the prior function describing the nuisance parameters and\n",
    "N is a normalization factor to have $\\int f(\\mu)d\\mu$ =1\\\n",
    "The prior presents the usual issues, and it is usually chosen to be flat\n",
    "in the parameter of interest $\\mu$ (assume \"total ignorance\"). To find\n",
    "the upper limit on $\\mu$ at the $1-\\alpha$ CL (credible level) we need\n",
    "to solve numerically for $\\mu_{1-\\alpha}$ in\n",
    "$$\\int_0^{\\mu_{1-\\alpha}} f(\\mu) d\\mu = 1-\\alpha$$ see\n",
    "Fig. [1.46](#fig:bayesHiggs){reference-type=\"ref\"\n",
    "reference=\"fig:bayesHiggs\"}. In general $\\mu$ is a function of the test\n",
    "mass $\\mu = \\mu(m_H)$ and to obtain the exclusion plot as in\n",
    "Fig. [1.43](#fig:exclusionLimit){reference-type=\"ref\"\n",
    "reference=\"fig:exclusionLimit\"}, we will need to compute this integral\n",
    "for each value of the test mass.\n",
    "\n",
    "![[\\[fig:bayesHiggs\\]]{#fig:bayesHiggs label=\"fig:bayesHiggs\"}\n",
    "](Section9Bilder/bayesHiggs.png){#fig:bayesHiggs width=\"60%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look-Elsewhere Effect {#sec:LEE}\n",
    "---------------------\n",
    "\n",
    "Before formalizing the Look-Elsewhere Effect (LEE) for a HEP search,\n",
    "let's consider this easy example:\\\n",
    "\\\n",
    "**Example** In a city the average number of accidents per day is\n",
    "$7\\pm 1$. When looking at the number of accidents with full moon, the\n",
    "number is 10 which is $3\\sigma$ from the average. Is this sufficient to\n",
    "claim that full moon has an influences on the number of incidents? To\n",
    "answer this question we need to consider the fact that this result was\n",
    "obtained looking at the statistics of this one city. To have a better\n",
    "understanding of the phenomenon we would need to verify it on a larger\n",
    "number of cities. If we repeat the observation on 100 cities, then you\n",
    "would expect that at least one shows a $3\\sigma$ deviation. The question\n",
    "becomes how many cities above 3$\\sigma$ should I observe to convince\n",
    "myself of a supernatural influence of the full moon on the drivers\n",
    "capabilities ?\\\n",
    "\\\n",
    "Back to HEP: consider the search for a resonance in an invariant mass\n",
    "distribution. In case you know that the resonance is expected at\n",
    "$m = m_0$ (e.g. because you have a theoretical prediction about its\n",
    "position) you can build the discovery test statistics for this fixed\n",
    "mass hypothesis:\n",
    "$$t_{\\mbox{local}} = -2 \\ln \\frac{L(0)}{L(\\hat{\\mu}, m_0)}.$$ Notice\n",
    "that there is no reference to the mass position $m_0$ at the numerator\n",
    "because we're considering the case where there is no signal (and so no\n",
    "need to worry about its position). To measure the level of compatibility\n",
    "of your data with the background only hypothesis you can compute the\n",
    "$p$-value:\n",
    "$$p_{\\mbox{local}}=\\int_{t_{\\mbox{local}}}^\\infty f(t_{\\mbox{local}} | 0) dt_{\\mbox{local}}$$\n",
    "This is what generally goes under the name of \"local\" $p$-value (often\n",
    "indicated with $p_0$).\\\n",
    "If instead you don't know where the peak will appear (a much more\n",
    "frequent situation) the $p$-value we are interested in is the one which\n",
    "tells the probability to observe a fluctuation *anywhere* in the\n",
    "experimentally accessible mass range. The goal is to take care of the\n",
    "trivial fact that with a large enough dataset and a large enough number\n",
    "of bins, we are bound to find a deviation from the background only\n",
    "hypothesis somewhere because of statistical fluctuations. To take this\n",
    "into account the position of the resonance $m_0$ is replaced by an\n",
    "adjustable parameter in the likelihood:\n",
    "$$t_{\\mbox{global}} = -2 \\ln \\frac{L(0)}{L(\\hat{\\mu}, \\hat{m})}$$ the\n",
    "denominator is allowed to fit the strength parameter ($\\hat{\\mu}$)\n",
    "anywhere ($\\hat{m}$). The corresponding $p$-value is:\n",
    "$$p_{\\mbox{global}}=\\int_{t_{\\mbox{global}}}^\\infty f(t_{\\mbox{global}} | 0) dt_{\\mbox{global}}.$$\n",
    "In order to compute this integral, as usual, we need to find the p.d.f.\n",
    "$f(t_{\\mbox{global}} | 0)$. To do this we can proceed brute force\n",
    "tossing toy experiments. This is a particularly computing intensive\n",
    "approach, since we need to scan both the signal strength and the bump\n",
    "position. To overcome this issue it would be tempting to use the\n",
    "asymptotic formulas based on Wald's theorem we discussed in the previous\n",
    "sections. Unfortunately the Wald's theorem only works under some\n",
    "\"regularity conditions\" which require to have the same parameter of\n",
    "interest appearing both in the null and the alternative hypotheses. In\n",
    "our case the alternative hypothesis (denominator) has both the position\n",
    "$m_0$ of the excess and its signal strength $\\mu$, while the null\n",
    "hypothesis (numerator) has only the signal strength $\\mu = 0$ because\n",
    "the position of the possible excess is not defined in absence of a\n",
    "signal.\\\n",
    "A solution to this problem has been developed in Ref. [@LEE]. The main\n",
    "idea behind the method is to first compute the $p_{local}$ as if the\n",
    "position of the mass was known and then apply a correction factor to\n",
    "bring it to $_{global}$:\n",
    "$$p_{global} \\sim p_{local}+ \\langle N(t_{local}) \\rangle$$ The factor\n",
    "$\\langle N(t_{local}) \\rangle$ is the mean number of times the\n",
    "statistics $-2\\ln L$ cross the $t_{local}$ threshold from below (slang:\n",
    "\"up-crossings\"). To understand what this is, let's take the example of\n",
    "the original paper in Fig. [1.47](#fig:LEE){reference-type=\"ref\"\n",
    "reference=\"fig:LEE\"}.\n",
    "\n",
    "![[\\[fig:LEE\\]]{#fig:LEE label=\"fig:LEE\"} (top) An example\n",
    "pseudo-experiment with background only. The solid line shows the best\n",
    "signal fit, while the dotted line shows the background fit. (bottom) The\n",
    "likelihood ratio test statistic $t_{global}$. The dotted line marks the\n",
    "reference level $t_{local}$ with the up-crossings marked by the dark\n",
    "dots. [@LEE]](Section9Bilder/LEE.png){#fig:LEE width=\"60%\"}\n",
    "\n",
    "The top part of the figure shows a mass spectrum generated on the\n",
    "background only hypothesis. Around 25 there is a hint of an excess,\n",
    "while around 50 and 70 there is an hint of a deficit. The test\n",
    "statistics will be larger the larger the discrepancy of the data from\n",
    "the background only model (the denominator will get the value preferred\n",
    "by the data, while the numerator will adjust the background shape\n",
    "forcing the signal strength to zero). This behavior is clearly visible\n",
    "in the bottom part of the figure, where the three hints of discrepancy\n",
    "manifests themselves as bumps in the test statistics. The procedure\n",
    "would now consist in generating toy experiments and for each of them\n",
    "count how often the $t_{global}$ test statistics up-cross the value of\n",
    "$t_{local}$ we observe in data and average this number on the total\n",
    "number of toy-experiments. Up to this point the procedure suffers from\n",
    "the same limitation as the brute force approach to generate\n",
    "toy-experiments to populate the p.d.f. for $t_{global}$: if we have a\n",
    "large fluctuation in data (remember we're thinking about a discovery)\n",
    "then $t_{local}$ is going to be very high and only very few toys will\n",
    "up-cross that high threshold (hence the need of generating a large\n",
    "number of toy experiments). The idea of the paper is that one can\n",
    "compute $\\langle N(t_{local}) \\rangle$ for a small value of $t_{local}$\n",
    "and then estimate what $\\langle N(t_{local}) \\rangle$ would be at any\n",
    "other value of $t_{local}$ by:\n",
    "$$\\langle N(t_{local}) \\rangle = \\langle N(t_{local_0}) \\rangle e^{-\\frac{(t_{local} - t_{local_0})}{2}}$$\n",
    "The advantage of this formula is that you can compute\n",
    "$\\langle N(t_{local}) \\rangle$ with a very small number of\n",
    "toy-experiments and then propagate the result.\\\n",
    "The example treated in this section refers to the most common case of\n",
    "the search of a resonance appearing as an excess in a mass distribution.\n",
    "The same ideas obviously can be applied to any other search where\n",
    "instead of a bump in a mass spectrum the deviation from the background\n",
    "only hypothesis appears in another variable or in a more general case in\n",
    "a set of variables (e.g. search for an excess where you don't know\n",
    "neither the position nor the width)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "-   R. Barlow [@Barlow], \" A guide to the use of statistical methods in\n",
    "    the physical sciences\". Ch. 7\n",
    "\n",
    "-   Gary J. Feldman and Robert D. Cousins, \"Unified approach to the\n",
    "    classical statistical analysis of small signals.\" *Phys. Rev. D*,\n",
    "    57:3873--3889, Apr 1998\n",
    "\n",
    "-   A. L. Read, \"Modified frequentist analysis in search results (CLs\n",
    "    method)\", CERN Yellow Report 2000-005\n",
    "\n",
    "-   Cowan, Cranmer, Gross, Vitells, \"Asymptotic formulae for\n",
    "    likelihood-based tests of new physics\", EPJC 71 (2011) 1554,\n",
    "    physics/1007.1727\n",
    "\n",
    "-   Gross and Vitells, \"Trial factors for the look elsewhere effect in\n",
    "    high energy physics\", EPJC 70:525-530,2010, physics/1005.1891\n",
    "\n",
    "[^3]: Multiplying several probabilities (positive numbers smaller than\n",
    "    1) will quickly reach the machine numerical precision.\n",
    "\n",
    "[^4]: The CMS $H\\to\\gamma\\gamma$ analysis has o(50) nuisance parameters\n",
    "\n",
    "[^5]: The larger the expected signal the easier is to exclude it.\n",
    "\n",
    "[^6]: Here we assuming that the searched for signal is the Standard\n",
    "    Model Higgs boson, i.e. its production cross section/couplings/etc.\n",
    "    are the ones predicted by the Standard Model.\n",
    "\n",
    "[^7]: This typically generate some confusion, the CL is a parameter set\n",
    "    by hand and not a function of the data!\n",
    "\n",
    "[^8]: The idea stemmed from the frequentist approach of Zech to the\n",
    "    problem of setting limits for a counting experiment in presence of\n",
    "    background [@Zech].\n",
    "\n",
    "[^9]: The name comes from the short story \"Franchise\" from I. Asimov,\n",
    "    where in the far future of 2008 the U.S.A. elections were to be\n",
    "    replaced by the choice of a single citizen chosen by a computer\n",
    "    which would represent the perfect average of the whole population\n",
    "\n",
    "[^10]: The initial Higgs results were indeed verified using both\n",
    "    approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
